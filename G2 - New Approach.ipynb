{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliomi import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slice_indexes(sequence_repo, subject, percentile, interval):\n",
    "    \n",
    "    roi = sequence_repo.get_roi(subject)\n",
    "\n",
    "    ((rmin, rmax), (cmin, cmax), (zmin, zmax)) = get_bounding_box(roi)\n",
    "\n",
    "    width = rmax - rmin\n",
    "    height = cmax - cmin\n",
    "    z_height = zmax - zmin\n",
    "\n",
    "    roi_sizes = get_roi_size(roi, 2)\n",
    "    \n",
    "    return {'r-axis': (rmin, rmax), 'c-axis': (cmin, cmax), 'z-axis': (zmin, zmax), 'sizes': roi_sizes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_list_task(item):\n",
    "    sequence_repo, subject, percentile, interval = item\n",
    "    print(\"Working on:\", subject)\n",
    "    result = get_slice_indexes(sequence_repo, subject, percentile, interval)\n",
    "    print(subject, \":\", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "sequence_repo = SequenceRepoGliomi(\"/data/RMN/dataset-gliomi\")\n",
    "\n",
    "items = []\n",
    "for file in glob.glob(\"/data/RMN/dataset-gliomi/scaled/*\"):\n",
    "    subject = os.path.basename(file)\n",
    "    items.append([sequence_repo, subject, 0, 10])\n",
    "    \n",
    "print(\"Starting...\")\n",
    "    \n",
    "processing_pool = Pool(int(multiprocessing.cpu_count()))\n",
    "res = processing_pool.map(get_index_list_task, items)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = {}\n",
    "for i, item in enumerate(items):\n",
    "    sequence_repo, subject, percentile, interval = item\n",
    "    final[subject] = res[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/data/RMN/dataset-gliomi-cnn/roi-metadata.pickle\", \"wb\") as file:\n",
    "    pickle.dump(final, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lh /data/RMN/dataset-gliomi-cnn/roi-metadata.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/RMN/dataset-gliomi-cnn/roi-metadata.pickle\", \"rb\") as file:\n",
    "    print(pickle.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequence from roi bounding box\n",
    "\n",
    "\"\"\"\n",
    "Get the part of sequence masked and related to its bounding box\n",
    "\"\"\"\n",
    "def mask_and_crop(sequence, affine, bbox, full_brain=False):\n",
    "    \n",
    "    ((rmin, rmax), (cmin, cmax), (zmin, zmax)) = bbox\n",
    "    \n",
    "    if full_brain:\n",
    "        rmin, rmax = 0, sequence.shape[0]\n",
    "        cmin, cmax = 0, sequence.shape[1]\n",
    "\n",
    "    delta_r = rmax-rmin\n",
    "    delta_c = cmax-cmin\n",
    "    delta_z = zmax-zmin\n",
    "    \n",
    "    data = sequence.get_fdata()[rmin:(rmin+delta_r),cmin:(cmin+delta_c),zmin:(zmin+delta_z)]\n",
    "\n",
    "    return data, affine\n",
    "\n",
    "\"\"\"\n",
    "Extract mask from a sequence and resize to a cube a a given side\n",
    "\"\"\"\n",
    "def mask_crop_resize(sequence, affine, bbox, side, full_brain=False):\n",
    "\n",
    "    data, affine = mask_and_crop(sequence, affine, bbox, full_brain)\n",
    "    \n",
    "    ((rmin, rmax), (cmin, cmax), (zmin, zmax)) = bbox\n",
    "\n",
    "    if full_brain:\n",
    "        rmin, rmax = 0, sequence.shape[0]\n",
    "        cmin, cmax = 0, sequence.shape[1]\n",
    "\n",
    "    (dim1, dim2, dim3) = (rmax - rmin), (cmax - cmin), (zmax - zmin)\n",
    "\n",
    "    scale_affine = np.array([[float(side) / dim1, 0, 0, 0], \n",
    "                             [0, float(side) / dim2, 0, 0], \n",
    "                             [0, 0, float(zmax - zmin) / dim3, 0], \n",
    "                             [0, 0, 0, 1]])\n",
    "    \n",
    "    resampled_roi = nb.Nifti1Image(\n",
    "        data,\n",
    "        affine=scale_affine)\n",
    "\n",
    "    img = nilearn.image.resample_img(\n",
    "        resampled_roi, \n",
    "        target_affine=np.eye(4),\n",
    "        target_shape=(side, side, zmax - zmin), \n",
    "        interpolation='nearest')\n",
    "    \n",
    "    return img.get_fdata(), img.affine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "who = \"ALESSANDRINI_GLAUCO\" # \"ANGELONI_GIUSEPPINA\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = sequence_repo.get_roi(who) #, \"FLAIR\")\n",
    "sequence = sequence_repo.get_sequence(who, \"T1\")\n",
    "\n",
    "sequence = nb.Nifti1Image(\n",
    "        sequence.get_fdata() * roi.get_fdata(),\n",
    "        affine=sequence.affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = final[who]['r-axis'], final[who]['c-axis'], final[who]['z-axis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = (bbox[0], bbox[1], (0, sequence.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, a = mask_crop_resize(sequence, sequence.affine, bbox, 224, full_brain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.shape, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = int(i.shape[2] / 4) + 1\n",
    "\n",
    "fig = plt.figure(figsize=(40., rows * 10.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(rows, 4),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, img in zip(grid, [i[:,:,j] for j in range(i.shape[2])]):\n",
    "    ax.imshow(img)\n",
    "    # draw_text_in_dataset(ax, label)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = sequence.get_fdata()\n",
    "\n",
    "np.min(v), np.max(v), np.mean(v), np.std(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "sns.distplot(v[v > 0].flatten());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "def plot_distribution_values_in_roi(ax, subject, sequence_name, side, title):\n",
    "    \n",
    "    roi = sequence_repo.get_roi(subject)\n",
    "    \n",
    "    sequence = sequence_repo.get_sequence(subject, sequence_name)\n",
    "\n",
    "    sequence = nb.Nifti1Image(\n",
    "            sequence.get_fdata() * roi.get_fdata(),\n",
    "            affine=sequence.affine)\n",
    "    \n",
    "    # bbox = final[subject]['r-axis'], final[subject]['c-axis'], final[subject]['z-axis']\n",
    "    \n",
    "    # i, _ = mask_crop_resize(sequence, sequence.affine, bbox, side, full_brain=False)\n",
    "    \n",
    "    v = sequence.get_fdata()\n",
    "    \n",
    "    v = v[v > 0]\n",
    "    \n",
    "    _max = np.max(v)\n",
    "    _min = np.min(v)\n",
    "    v = (v - _min) / (_max - _min)\n",
    "    \n",
    "    print(subject, np.histogram(v))\n",
    "\n",
    "    sns.distplot(v, ax=ax, label=title, kde_kws={\"label\": f\"{subject}-{title}\"});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/data/RMN/dataset-gliomi-cnn/dataset-survivor.csv\")\n",
    "subjects = np.array(df.iloc[:,1])\n",
    "labels = np.array(df.iloc[:,2])\n",
    "for subject, label in zip(subjects, labels):\n",
    "    print(subject, \",\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import seaborn\n",
    "\n",
    "a4_dims = (11.7, 8.27)\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize=a4_dims)\n",
    "\n",
    "plot_distribution_values_in_roi(ax, \"BIANCHI_GIOVANNI\", \"FLAIR\", 224, f\"0\")\n",
    "plot_distribution_values_in_roi(ax, \"BIAVATI_S\", \"FLAIR\", 224, f\"0\")\n",
    "plot_distribution_values_in_roi(ax, \"BIANCHI_ORAZIO\", \"FLAIR\", 224, f\"0\")\n",
    "\n",
    "plot_distribution_values_in_roi(ax, \"BOEZI_MARIO\", \"FLAIR\", 224, f\"1\")\n",
    "plot_distribution_values_in_roi(ax, \"CAMPLESE_CANDEROLA\", \"FLAIR\", 224, f\"1\")\n",
    "plot_distribution_values_in_roi(ax, \"CACACE_PAOLO\", \"FLAIR\", 224, f\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on 130 tasks\n",
      "Working on ALESSANDRINI_GLAUCO\n",
      "Working on BAGNOLI_VINCENZO\n",
      "Working on BATTISTA_DOMENICA\n",
      "Working on ASSANTO_MARIA\n",
      "Working on BARONTINI_MARIA_GIOVANNA\n",
      "Working on BERNOLA_TERESA\n",
      "Working on ANGELONI_GIUSEPPINA\n",
      "Working on BERGNACH_SILVANO\n",
      "Working on BEVILACQUA_RITA\n",
      "Working on BERTUZZI_LUISA\n",
      "Working on BIANCHI_GIOVANNI\n",
      "Working on BIANCHI_ORAZIO\n",
      "Working on BIAVATI_S\n",
      "Working on BOVE_A\n",
      "Working on BOEZI_MARIO\n",
      "Working on CACACE_PAOLO\n",
      "Working on CALDARONI_ANNA\n",
      "Working on COLAZZO_LUIGI_GIUSEPPE\n",
      "Working on COSTANZI_P\n",
      "Working on CAMACCI_FILIBERTO\n",
      "Working on CARZEDDA_PAOLO\n",
      "Working on DEL_BOVE_PIERINA\n",
      "Working on DI_MARCO_L\n",
      "Working on DI_LORENZO_TOMMASO\n",
      "Working on DIASPRO_G\n",
      "Working on CIMPUREANU_N\n",
      "Working on CATALANI_F\n",
      "Working on DI_MASO_SIMONE\n",
      "Working on CARULLI_L\n",
      "Working on CAMPLESE_CANDEROLA\n",
      "Working on COLAFRANCESCO_ROCCO\n",
      "Working on DI_CARLATONIO_MAURIZIO\n",
      "Working on DE_SANTIS_GIORGO\n",
      "Working on CAPEZZONE\n",
      "Working on CRESCENZI_ARMANDO\n",
      "Working on DOBRISAN_DORINA\n",
      "Working on D_ANGELI_ANNUNZIATA\n",
      "Working on DI_MASSA_SERGIO\n",
      "Working on DARIDA\n",
      "Working on DE_PAOLI_R\n",
      "Working on D_ANGELO_RENATO\n",
      "Working on EMERY_R_C\n",
      "Working on FABIANI_ANNA\n",
      "Working on FEDERICO_FRANCESCO\n",
      "Working on FERRAZZA_RITA\n",
      "Working on FERRI_M_B\n",
      "Working on FILIPPONI_QUINTINO\n",
      "Working on FIUCCI_A\n",
      "Working on FLORIO_FRANCESCO_PAOLO\n",
      "Working on FRATINI_RITA\n",
      "Working on GATTAMORTA_NATALINA\n",
      "Working on GEGGI_GIULIO\n",
      "Working on GENNARI_CRISTIANO\n",
      "Working on GIANFELICI_LUISA\n",
      "Working on GIOIA_COSMO_DAMIANO\n",
      "Working on GIORDANO_STEFANIA\n",
      "Working on INCITI_DONATA\n",
      "Working on IONTA_LUCIANA\n",
      "Working on ISONI_FRANCESCO\n",
      "Working on LABELLA_ADRIANA\n",
      "Working on LANDONE_ANNUNZIATA\n",
      "Working on LIBERATI_G_L\n",
      "Working on LIOCE_CARMELA\n",
      "Working on LONGO_ROSALIA\n",
      "Working on LO_BELLO_MARIO\n",
      "Working on LUPI_GIANCARLO\n",
      "Working on MAIOLINI_SANTA\n",
      "Working on MARAGNO_CLARA\n",
      "Working on MARCOLINI\n",
      "Working on MARIANI_BERNARDO\n",
      "Working on MAROCCHI_CORRADO\n",
      "Working on MARTINEZ\n",
      "Working on MASCI_ADA\n",
      "Working on MEDICI_GIOVANNA\n",
      "Working on MICHELI_MICHELE\n",
      "Working on MONACELLI_LAURA\n",
      "Working on MOSCARDINI_GIACINTO\n",
      "Working on MOVIA_A\n",
      "Working on MUSAT_DORINA\n",
      "Working on NERONE_GIANLUCA\n",
      "Working on NERVEGNA_G\n",
      "Working on ORLANDI_PAOLO\n",
      "Working on PAGANNONE_GIANNI\n",
      "Working on PAGLIAROLI_LUCIA\n",
      "Working on PAGNOTTA\n",
      "Working on PALMA\n",
      "Working on PALMIERI\n",
      "Working on PANETTI\n",
      "Working on PASSARI\n",
      "Working on PIERI\n",
      "Working on PIERINI_CATERINA\n",
      "Working on PINEDA_MARIA_ASSUNTA\n",
      "Working on PISTOIA_CARLO\n",
      "Working on PODAGROSI_TERESA\n",
      "Working on PODDA_ANTONINO\n",
      "Working on POMPEI_F\n",
      "Working on PRINCIPI_ANNA_MARIA\n",
      "Working on PROIETTI_GIOVANNI\n",
      "Working on PROIETTI_MARIA\n",
      "Working on QUACQUARELLI_A\n",
      "Working on QUATTROCIOCCHI_EVELINA\n",
      "Working on RICCI_ALESSANDRO\n",
      "Working on ROMITO_ORAZIO\n",
      "Working on ROSARI_NANDO\n",
      "Working on RUSCITO_ELISABETTA\n",
      "Working on RUSNAC_NINA\n",
      "Working on RUSSO_IDA\n",
      "Working on SALA_CLARA\n",
      "Working on SALTARELLI_DOMENICO\n",
      "Working on SANTINI_ERMANNO\n",
      "Working on SCARAMUZZA_F\n",
      "Working on SOLOVIY_VOLODYMYR\n",
      "Working on STAN_FLORENTINA\n",
      "Working on STEFANINI_CLORINDA\n",
      "Working on STERPA_GIUSEPPE\n",
      "Working on SYKULA_GRAZYNA_BARBARA\n",
      "Working on TAVERNESE_G\n",
      "Working on TAVOLUCCI_MARIA_RITA\n",
      "Working on TEMPESTINI_MARISA\n",
      "Working on TEMPORIN_PATRIZIA\n",
      "Working on TEOFILI_STEFANO\n",
      "Working on TIBERI_GIUSEPPE\n",
      "Working on TOMAO_ANGELO\n",
      "Working on TOMEO_VINCENZO\n",
      "Working on TROSCIA_M\n",
      "Working on VERONESI_ROCCO\n",
      "Working on VITULANO_RITA\n",
      "Working on ZANATTA_CARLO\n",
      "Working on ZANGARI_ALDO\n",
      "Working on ZEPPA_ONORIO\n",
      "[([(array([  221,  8721, 31528, 32102,  5993,   528,    11,    14,    13,\n",
      "           8]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  171, 26534, 37820,  4727,  2677,  2475,  2575,  1731,   403,\n",
      "          26]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   45,   159,   487, 12526, 12971, 18499, 22060, 11535,   766,\n",
      "          91]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  181,  2958, 11374, 12278,  7385,  3772,    99,    43,    11,\n",
      "          13]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  733, 10268, 12755,  6299,  6610,   840,   379,   157,    54,\n",
      "          19]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 604, 2993, 6189, 9761, 9043, 8025, 1435,   34,   21,    9]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([  258,  4681, 11179, 13851, 18158, 26241, 23141, 12492,  4092,\n",
      "         264]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  563, 21186, 72281,  7580,  3704,  3973,  3430,  1474,   151,\n",
      "          15]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 4378, 15330, 14577, 14699, 13877, 15584, 15403, 13984,  6125,\n",
      "         400]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    4,     5,     7,    68,   631, 16869, 87040, 72421, 12476,\n",
      "         415]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    42,   9045, 107617,  56126,  12809,   2974,    949,    301,\n",
      "           62,     11]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   11,   122,   852,  5475, 51375, 47916, 40745, 31235, 10889,\n",
      "        1316]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    5,    10,    35,   481,  5375, 17246, 17064,  6205,   753,\n",
      "          61]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  192,  1213,  5671, 28597,  5198,  2540,  2429,  1156,   206,\n",
      "          33]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([7157, 6705, 6681, 6739, 6076, 5253, 4682, 3488,  410,   44]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    5,    24,   157,   794,  5524, 12308, 23378, 27829, 14381,\n",
      "        1352]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   37,   810, 13952, 51673, 15811,  1639,  1029,   648,   136,\n",
      "          17]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    9,    62,   283,   770,  2111, 19224, 16897, 16937, 21475,\n",
      "        7984]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   30,     0,     0,     0,     0,     4,  2301, 40968, 55462,\n",
      "        6981]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  155, 20925, 57381, 15765,  7777,  3048,   573,    90,    24,\n",
      "           8]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([21480, 17257, 15737, 13573, 11549,  9025,  6995,  6382,  3537,\n",
      "         211]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([ 145,  644, 1676, 1982, 2578, 3492, 3512, 2055,  215,    7]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 919, 3467, 7875, 2322, 1197,  430,   56,   25,   11,    4]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   6,    8, 3854, 4828, 3757, 2458, 1114,  230,   42,    9]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   40,   420,  3210, 16039, 27479, 44139, 42540, 17070,   104,\n",
      "          13]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  426, 48431, 88653,  5901,  3168,  2851,  1353,   227,    30,\n",
      "          14]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    3, 21076, 50152, 31589, 25856, 15521,  3182,  2246,  1331,\n",
      "          98]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   5,   21,  155, 1730, 5244, 5987, 5485, 2101,  454,   40]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  13,  267, 1888, 7277, 3552, 3402, 3427, 1254,  130,   12]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([8912, 5159, 2361, 1733, 1127,  532,  518,  517,  293,   70]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  498,  7705, 23390, 31474, 29136,  7371,   154,    30,    20,\n",
      "           6]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1065, 23741, 56682, 13863,  2937,  1115,   256,    77,    35,\n",
      "          13]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([20748, 14429, 13074, 12099, 12971, 12680, 11538,  2056,    94,\n",
      "          95]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    7,   124,   830,  3648, 11454, 22983, 21897,  9220,  1512,\n",
      "          62]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  573, 15841, 41093,  6317,  4433,  2411,   822,   202,    39,\n",
      "           6]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    2,     3, 10358, 12896, 12668, 11315,  9625,  9287,  5173,\n",
      "         410]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   250,  19738, 275777, 201641,  24217,   7132,   1535,    151,\n",
      "           70,     30]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   138,   3397,  71728, 257768, 112685,  61669,  18657,   4019,\n",
      "          442,     38]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([360253, 141725,  24621,   2064,    751,    466,    297,    196,\n",
      "          119,     49]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   28,   255,  2202, 25657, 53642, 45470, 21907,  5808,   147,\n",
      "           9]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1846, 90545, 51470,  4397,  3987,  2435,   417,    23,     3,\n",
      "           2]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   24, 12994, 31226, 25310, 25177, 20495, 17020, 14054,  8427,\n",
      "         398]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    55,    419,   1332,   4844,  35262, 149701, 120345,  49959,\n",
      "         9327,    234]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   619,  98717, 227896,  36571,   6223,   1195,    208,     37,\n",
      "            8,      4]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([111690, 110330,  78549,  42629,  19291,   6680,   1777,    447,\n",
      "           72,     13]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    6,    55,  1523, 10566, 24534, 35884, 23318,  8211,   386,\n",
      "          52]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  278, 17141, 58412, 18947,  4644,  2602,  1423,   813,   251,\n",
      "          24]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([30639, 18798, 12471, 12515, 13579, 11933,  2734,   769,   772,\n",
      "         325]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([ 8982, 49686,  7235,   518,     1,     1,     0,     1,     1,\n",
      "           2]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  603, 29156, 26400,  2523,  2161,  2366,  1953,  1018,   241,\n",
      "           6]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  137,   287,   266,  8110, 15000, 15960, 17006,  9348,   280,\n",
      "          33]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    9,    71,   879,  5777, 28370, 60370, 63392, 37722,  4944,\n",
      "          11]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   447,  25600, 104484,  40330,  18765,   7783,   2712,   1074,\n",
      "          303,     47]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   14,  3414, 52056, 43315, 35179, 28661, 19923, 13500,  4496,\n",
      "         987]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  845, 13236, 18960, 30839, 24103,  1192,   264,   182,    93,\n",
      "          16]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 2466, 24330, 36646,  9078,  5110,  4515,  4017,  2803,   700,\n",
      "          65]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    1,    10, 15525, 39468, 24240,  8984,  1453,    27,    13,\n",
      "           9]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   49,   753,  2756,  9057, 23992, 44223, 44588, 23790,  9755,\n",
      "         621]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  244, 24662, 94457, 26421,  4025,  4491,  3231,  1617,   380,\n",
      "          56]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   14,   449,  1790, 30815, 30738, 25888, 24059, 23377, 21051,\n",
      "        1403]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   13,    43,   103,   767,  5877, 14632, 13238,  3401,   311,\n",
      "          15]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   29,  8826, 18351,  3241,  3651,  2549,   973,   503,   237,\n",
      "          40]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   52,   200,   695,  1736, 14008, 15577,  4480,  1287,   351,\n",
      "          14]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([10476, 58579, 15437,  1239,     6,     0,     2,     0,     0,\n",
      "           1]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1961, 48902, 10330, 11143,  8029,  4080,  1075,   170,    38,\n",
      "          12]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    6,  5144,  9194,  7475,  6626,  6617,  7589, 13832, 23820,\n",
      "        5437]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([ 12625,  66004, 129178, 110348, 118346, 101155,  42905,   7606,\n",
      "          174,     13]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    63,   5364, 125002, 289264,  69961,  65393,  27209,   5166,\n",
      "          880,     52]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    84,    823, 146047, 152748, 124638,  98980,  50227,  13057,\n",
      "         1730,     20]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  1712, 156083, 333385, 156454,  17084,   1923,    353,    169,\n",
      "          122,     62]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    33,   1648, 113447, 418021, 102396,  19241,   8636,   3410,\n",
      "          489,     26]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 68729, 332976, 162115,  64200,  22756,   9915,   4479,   1757,\n",
      "          360,     60]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   85,  1526, 36650, 91398, 72684, 20864,   253,    19,     5,\n",
      "           5]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   169,  10689, 103833,  75352,  14165,  11688,   5865,   1500,\n",
      "          214,     14]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([31606, 30065, 33681, 39170, 24269, 19718, 22361, 17866,  4611,\n",
      "         142]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  196, 19293, 47937, 31752,  1119,     1,     0,     0,     1,\n",
      "           1]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  555, 17036, 27290, 37697, 12742,  3640,  1030,   229,    63,\n",
      "          18]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 8118, 13489, 14643, 14962, 14972, 13159, 10473,  6814,  3116,\n",
      "         554]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   164, 173484, 610898, 163405,  21026,   3148,    535,    125,\n",
      "           54,     13]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    32,   2917, 209319, 628232, 113680,   9770,   5996,   2528,\n",
      "          364,     14]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    19,     40,     91,   2125, 288725, 363200, 214050,  91564,\n",
      "        12540,    498]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   77,  1693, 23444, 82014, 91174, 32859,  8511,   367,    31,\n",
      "           9]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   245,  27131, 118201,  69094,  16778,   6199,   1859,    490,\n",
      "          158,     24]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   51,    58,  1003, 60797, 59406, 44904, 38699, 26922,  7988,\n",
      "         351]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   11,    67,   566,  3421, 10349, 13801,  5538,   629,    37,\n",
      "           4]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    8,   439, 16397, 12961,  1766,  1062,  1104,   549,   120,\n",
      "          17]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   13,  8681, 11693,  8091,  3664,  1580,   593,    95,     8,\n",
      "           5]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    2,     6,   121,   299,  2909, 15654, 45461, 29030,  3621,\n",
      "          17]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   46,  3451, 31357, 30691, 13590,  9756,  5624,  2099,   466,\n",
      "          40]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   46,   168,  2802, 22058, 17482, 17210, 16792, 14407,  5778,\n",
      "         377]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   702,  45112, 443822, 940042, 196631,   4247,    255,     93,\n",
      "           23,      2]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  1331, 504227, 807460, 101639,  65728,  58084,  48265,  31341,\n",
      "        11942,    912]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([268714, 320957, 315511, 284484, 248229, 151795,  39412,   1801,\n",
      "           21,      5]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   3,    3,   18,   54,  380, 1059, 1864, 2082,  781,  102]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  81,  926, 1062,  847,  975, 1164,  932,  298,   49,   12]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  16,  280, 2574, 1657,  792,  469,  316,  158,   70,   14]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  2756, 105563, 232341, 294252, 377630, 212099,  16851,    201,\n",
      "            6,      7]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  4170, 244735, 813701,  61615,  36728,  37037,  29869,  12050,\n",
      "         1704,     97]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([107026, 152147, 184503, 131329, 130383, 181541, 204469, 121411,\n",
      "        27903,    994]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   55,     0,     0,    95,  3330,  8693, 10024,  6312,  3510,\n",
      "        1176]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  258,  5768, 15403,  8709,  1230,   622,   702,   409,    88,\n",
      "           6]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([3988, 3469, 3433, 3523, 3262, 3315, 3751, 4606, 3543,  305]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   12,    55,  7058, 36466, 34579, 14598,  3580,   289,    61,\n",
      "          28]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  545, 26222, 48923,  8970,  3950,  3693,  2827,  1209,   343,\n",
      "          44]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    2,     3,    65,  1316,  7337, 28341, 40686, 18729,   139,\n",
      "         108]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   21,   636,  4056, 25401, 48101, 40616, 24106,  1609,   164,\n",
      "          33]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  354,  9836, 50956, 40478, 13534, 10720, 11089,  6144,  1475,\n",
      "         157]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   16,    55, 16263, 37171, 38288, 26109, 15084,  8878,  2805,\n",
      "          74]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  132,  2864, 12137, 26421, 47483, 43618, 42662, 28989,  9736,\n",
      "         618]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 3404, 72365, 93365, 15352, 13091, 10560,  4563,  1538,   390,\n",
      "          32]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  466, 25683, 43744, 35002, 29722, 26543, 23252, 19625,  9687,\n",
      "         936]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    65,   6004, 100030, 322542, 169644,  22885,    414,     17,\n",
      "           11,      7]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 20129, 465274,  93776,  18325,  13798,   7030,   2299,    786,\n",
      "          163,     39]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 95046,  84910,  79091,  88884, 105107,  92246,  56207,  17747,\n",
      "         2267,    114]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    6,     9,    47,   518,  2308, 11417, 11847,  6469,  4130,\n",
      "         853]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  353,  4047, 14294,  4067,  5437,  5742,  2733,   777,   136,\n",
      "          18]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([3251, 7658, 8004, 7109, 4871, 2775, 1888, 1290,  663,   95]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   19,   535,  5902, 18507, 41376, 62562, 36728,  7332,  1104,\n",
      "         113]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1556, 38330, 88546, 16780, 14789,  9495,  3684,   874,   119,\n",
      "           5]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   59, 50631, 43062, 29162, 23755, 19512,  6825,   999,   152,\n",
      "          21]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([ 1218,  4489, 10679,  3356,    60,    22,     7,     5,     1,\n",
      "           2]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  27,  963, 3963, 9906, 2128, 1208,  920,  551,  155,   18]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   2, 3304, 3090, 3306, 3613, 2604, 1286, 1098, 1143,  393]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   29,   478,  6827, 56832, 16785,  8528,  4831,  2582,  1336,\n",
      "         195]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  123,   637,  5398, 25818, 28585, 33090,  4301,   342,   100,\n",
      "          29]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1396, 56516, 16357,  7210,  4670,  4097,  3894,  3144,  1036,\n",
      "         103]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    2,    18,   451,  8986, 33043, 40933, 17048,  1984,    27,\n",
      "           4]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  696, 27423, 46025, 11925,  6610,  5360,  2822,  1159,   405,\n",
      "          71]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([18755, 16112, 13635, 11131, 10255,  9743,  8711,  8252,  5340,\n",
      "         562]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([  185,  8605, 24273, 25277, 13825,  6480,   570,     5,     1,\n",
      "           6]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   16,    53,  9264, 43931, 20265,  3620,  1449,   485,   122,\n",
      "          22]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([13348, 11611,  8922,  7469,  7112,  7867,  8976,  8585,  4951,\n",
      "         386]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([ 2091, 33213, 37981,  5588,  2587,  1841,  1311,   650,   362,\n",
      "          88]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   58,  7287, 43097, 25532,  4199,  3206,  1711,   536,    82,\n",
      "           4]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([15143, 35376, 25333,  4380,  1678,  1394,  1138,   819,   382,\n",
      "          69]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  301,  4957,  5675, 13805, 21022, 20213, 12715,  2885,   419,\n",
      "           4]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1893,  3960, 12223, 27887, 11424,  9478,  9974,  4650,   482,\n",
      "          25]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([15893, 17511, 20824, 14751,  5801,  3362,  2341,  1085,   379,\n",
      "          49]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([  569,  9748, 21139, 22695, 22500, 20273, 11416,  5387,  1365,\n",
      "         115]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1174, 33799, 43090, 18819,  8100,  4863,  2861,  1735,   678,\n",
      "          88]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([22222, 30601, 25233, 18367, 10920,  4580,  1672,   903,   595,\n",
      "         114]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   790,  21092, 396680, 457714,  22442,    726,    195,     61,\n",
      "           16,      4]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  1530,  10068, 151724, 599040,  73652,  45705,  15454,   2040,\n",
      "          464,     43]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([252521, 180440, 152671, 136791,  88491,  43466,  22658,  14732,\n",
      "         7289,    661]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    7,   155,  1551,  6116, 16609, 19168, 15613,  8296,  2989,\n",
      "         332]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1146, 26334, 30047,  5372,  2160,  2096,  1604,  1337,   670,\n",
      "          70]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   39,   248,  6239, 11637, 10341, 10105,  9998, 11735,  9839,\n",
      "         655]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   26,  2923, 21463, 33912, 19497,  5426,   742,     4,     3,\n",
      "           1]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   11,  1034, 36925, 36035,  3458,  2558,  2547,  1232,   189,\n",
      "           8]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([10473, 10337, 10651, 10206, 10416, 11061, 10123,  8054,  2493,\n",
      "         183]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    3,    21,    33,    89,   504, 11959, 49182, 49668, 22058,\n",
      "        1545]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  111, 15895, 65505, 21314, 15924, 12308,  3177,   645,   160,\n",
      "          23]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   48,    75,   138,   789,  6483, 35456, 39851, 26496, 22184,\n",
      "        3542]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  107,  8197, 23795, 31104, 23316, 14871, 11235,  7601,  4097,\n",
      "         696]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  155, 20532, 34109, 31109, 20007, 13125,  4963,   919,    88,\n",
      "          12]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   13,  1344, 17313, 24441, 24403, 42269, 13383,  1431,   375,\n",
      "          47]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   66,  1375, 10902, 26148, 33579, 27709, 19452, 10009,  2116,\n",
      "          84]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  447, 41687, 71773,  7502,  4151,  3081,  1604,   930,   247,\n",
      "          18]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([17586, 16389, 13485, 12569, 12874, 14199, 17063, 18937,  8092,\n",
      "         246]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([ 1059, 13599, 37011, 14406,  1539,    34,    17,    15,     8,\n",
      "          12]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  279, 37279, 28436,  1349,   263,    61,    15,     2,     5,\n",
      "          11]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([11653, 11028,  9709,  9120, 12712,  9326,  3322,   674,   147,\n",
      "           9]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  283,  2799,  9119, 16201, 26017, 31907, 26100, 10811,  3756,\n",
      "         297]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 3227, 69746, 32881,  7496,  6557,  5117,  1961,   288,    14,\n",
      "           3]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 8197, 18047, 16189, 15514, 15930, 17697, 18760, 12688,  4039,\n",
      "         229]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    2,     4,     0,     9,   817, 21018, 49414, 30221, 12514,\n",
      "        1220]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  311, 24531, 65129, 11904,  3974,  3965,  3016,  1873,   488,\n",
      "          28]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    6,     8,    62,  2021, 10199, 31901, 35045, 28132,  7671,\n",
      "         174]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   27,   233,  2722,  7685, 14452, 21435, 19750, 10961,  3306,\n",
      "         320]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 6397, 49355,  8260,  4085,  4880,  4840,  2227,   525,   231,\n",
      "          91]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   15,    45,  4896, 17327, 19584, 10898,  8829, 11290,  7377,\n",
      "         630]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   3,    6,    8,   12,   56,  993, 5001, 6132, 3934,  817]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    8,    55,  1825, 10930,  3436,   532,   128,    33,    14,\n",
      "           1]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   1,    5,    8,   15,   58,  633, 4256, 8420, 3286,  280]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   91,  1598,  7481, 19947, 36798, 37050, 25165, 12956,  3657,\n",
      "         232]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 2075, 15069, 81904, 35173,  7458,  2346,   631,   215,    91,\n",
      "          13]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   34,   172, 41248, 39174, 24862, 13645, 10023,  9009,  6123,\n",
      "         685]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    37,    140,   1847,  39584, 101238,  24691,    314,     52,\n",
      "           23,      7]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  239, 12798, 87619, 39062, 11160,  9817,  5182,  1603,   400,\n",
      "          53]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   14,  1251, 35001, 31655, 30761, 32443, 27757,  7360,  1549,\n",
      "         142]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    61,      0,      0,      0,      8,   4221,  29467, 116261,\n",
      "        49237,   1799]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  1738,  33061, 120319,  23653,   9303,   9026,   3394,    501,\n",
      "           49,     10]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   13,     0,     0,     0,     0, 29478, 48972, 51016, 50683,\n",
      "       20892]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   531,   3899,  45839, 179532, 260460, 271550, 186167,  70356,\n",
      "        10627,     77]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   569,  34938, 367727, 479685, 113058,  28535,   3788,    600,\n",
      "          129,      9]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    15,  67280, 254854, 212156, 128328,  83026, 108079, 134121,\n",
      "        40591,    588]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    4,    11,   134,  1284,  3519,  8287, 13621, 10971,  8331,\n",
      "        2032]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  747,  2854, 19427, 11295,  5117,  4745,  2978,   902,   117,\n",
      "          12]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    7,     7,    51,   930, 10558, 21354,  9273,  4079,  1724,\n",
      "         211]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    2,     0,   147,  2644, 11634, 19892, 22887, 10717,  3832,\n",
      "         534]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  113,  3209, 40965, 22600,  2075,  1606,  1126,   482,    98,\n",
      "          15]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    6, 12044, 10545,  8020,  7227,  7357,  8468, 10350,  7525,\n",
      "         747]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    9,    30,  1935, 15577, 59135, 76287, 31575,  2904,   281,\n",
      "          16]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  2556, 104335,  60569,   8154,   5588,   4383,   1856,    260,\n",
      "           43,      5]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   40,   136,   223, 22308, 35488, 34736, 36429, 38288, 19509,\n",
      "         592]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    7,    15,    30,   183,   866,  4771, 14987, 21058, 15725,\n",
      "         892]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  174,  3941, 24923, 26236,  1231,   742,   649,   455,   164,\n",
      "          19]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    2,     3,    11,    21,   108,   991, 29372, 19726,  7292,\n",
      "        1008]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   59,   713,  4556, 11378, 12171, 12648,  8435,  2083,   369,\n",
      "          12]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  400, 12510, 30766,  6268,  1124,   698,   474,   166,    17,\n",
      "           1]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  95, 4687, 7966, 6202, 5548, 5270, 5820, 7076, 7775, 1985]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([  326,  8307, 14756, 18519, 16605, 16597,  8887,  1062,   155,\n",
      "          20]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1015, 18694, 37517, 19427,  5499,  1949,   814,   252,    58,\n",
      "           9]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([21086, 29516, 19493,  8239,  4447,  2093,   232,    67,    41,\n",
      "          20]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   87,  2893, 10933, 16222, 17456, 15249, 11386,  2258,    74,\n",
      "           9]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1711, 16594, 40707, 13062,  3541,   735,   143,    48,    21,\n",
      "           5]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   11,  6642, 12659, 11703, 11387, 12039,  9683,  6702,  4720,\n",
      "        1021]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   10,   524,  4609, 12469, 16697, 16019, 11393,  6621,  2840,\n",
      "         196]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  179, 11504, 30756,  8470,  3924,  4047,  5602,  4994,  1690,\n",
      "         212]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 6574,  7043, 10542,  8906,  7856,  7611,  8388,  9334,  4988,\n",
      "         136]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    8,    15,    54,   256,  5805, 57622, 73276, 44679,   708,\n",
      "           6]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  110,  8706, 69354, 71547, 16263, 11659,  3814,   894,    74,\n",
      "           8]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    2, 16491, 37879, 39153, 35020, 30384, 19240,  3915,   298,\n",
      "          47]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  254,  8071, 49278, 33724,  2975,   258,    49,     7,     0,\n",
      "           1]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  513, 27723, 47260,  9128,  5231,  2587,  1405,   647,   111,\n",
      "          12]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([32247, 17050, 12060, 10904, 11079,  9586,  1170,   299,   174,\n",
      "          48]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    5,   173,  4464, 74000, 95454,  8368,     7,     0,     0,\n",
      "           2]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   281,  70293, 103716,   3918,   2417,   1268,    437,    123,\n",
      "           15,      5]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([27550, 27900, 29076, 27713, 25142, 20544, 13334,  8154,  2956,\n",
      "         104]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   5,   12,   80,  468, 1886, 4879, 5114, 2817, 1166,  128]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  62, 6016, 8026, 1178,  547,  264,  179,  171,   96,   16]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([3482, 2375, 2707, 2932, 2420, 1299,  781,  431,   91,   37]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    3,    27,   196,  1323,  4645,  8164, 10182, 10558,  4431,\n",
      "         212]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    3,   196,  9492, 27270,  1961,   252,   173,   173,   181,\n",
      "          40]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([7181, 7875, 8102, 8136, 4779, 1943,  805,  483,  333,  104]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([  157,  2162,  7978, 19059, 23747, 26026, 21298,  7391,  1515,\n",
      "         125]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  277, 16652, 62467, 13097,  7532,  6387,  2451,   530,    59,\n",
      "           6]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 5352, 16880, 17754, 18849, 18403, 15728, 10834,  5078,   572,\n",
      "           8]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([ 134, 1131, 2188, 2076, 4275, 7670, 8090, 4260,  959,   75]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1327, 13066, 13104,   718,   787,   710,   537,   385,   190,\n",
      "          34]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([4143, 3618, 3220, 3098, 3077, 3473, 3596, 3768, 2580,  285]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   116,   6677,  50378, 104596, 106168,  46911,   5670,    188,\n",
      "           31,     14]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    28,   1436, 149496, 123797,  26980,  13940,   4155,    812,\n",
      "           92,     13]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([69322, 62885, 50177, 43074, 39512, 30639, 16244,  6926,  1832,\n",
      "         138]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   6,   16,  266, 2043, 5090, 3909, 2147, 1344,  712,  123]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 820, 9117, 3919,  429,  267,  306,  355,  284,  114,   45]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   5,    8,   55,  212, 1029, 5536, 5662, 1900,  964,  285]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([  300,  7436, 13676, 16370, 19961, 15474,  5518,  1269,   245,\n",
      "          57]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 5115, 34337, 19473,  6222,  6336,  5493,  2712,   562,    52,\n",
      "           4]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([12483, 13274, 12081, 12082, 12625,  9372,  5169,  2349,   760,\n",
      "         111]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    22,   6465, 228870, 452780, 494527,  67751,    168,     10,\n",
      "            0,      1]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 12007, 417973, 529217, 203244,  54732,  22946,   7800,   2211,\n",
      "          435,     29]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([356573, 322583, 223907, 128192,  76059,  57938,  45094,  29465,\n",
      "        10072,    711]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([ 134, 1345, 3657, 6077, 5439, 2322,  200,   15,    5,    6]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 918, 8514, 8243,  905,  281,  181,   85,   34,   23,   16]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([4865, 3914, 3256, 2546, 1924, 1428,  684,  395,  178,   10]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   16,   603, 13322, 48706, 42762,  5481,    39,     7,     7,\n",
      "           5]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    9,   302, 22671, 70994, 14574,  1580,   562,   211,    42,\n",
      "           3]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    2,     2,    15,    52,   270, 29835, 38674, 31930,  9745,\n",
      "         423]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([ 469, 2776, 2485, 3004, 2973, 2185, 1030,  330,  229,   50]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 672, 3424, 4121, 4991, 1394,  551,  221,  108,   43,    6]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([4309, 7944, 2063,  627,  226,  142,  100,   81,   29,   10]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  104,  1187,  5990, 17150, 38075, 55876, 41276, 27541,  6400,\n",
      "         241]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 3115, 21175, 93164, 35876, 21662, 11775,  5108,  1616,   305,\n",
      "          44]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   30,   485, 17504, 65019, 47586, 38586, 20194,  3963,   446,\n",
      "          27]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([  321,  4433, 15716, 35351, 35591, 27357, 16751,  8360,  2726,\n",
      "         163]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  285, 40690, 75263, 13216,  5910,  5722,  3891,  1457,   295,\n",
      "          40]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    3,  2453, 31524, 29934, 29270, 26751, 21152,  5344,   320,\n",
      "          18]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  862, 11109, 60825, 77031, 25029,  1779,    17,    12,     7,\n",
      "           6]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   247,  32791, 109524,  19107,   5290,   4956,   3034,   1379,\n",
      "          322,     27]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([27215, 25026, 23473, 21670, 20964, 19820, 15508, 13199,  8585,\n",
      "        1217]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   23,     0,     0,     0,    15,  3056, 40553, 74000, 31625,\n",
      "        1232]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  226, 68687, 61870,  5119,  5804,  5842,  2335,   510,    95,\n",
      "          16]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([15208, 16386, 19476, 18897, 15439, 15189, 17744, 18066, 12762,\n",
      "        1337]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([  139,  1421,  6128, 13936, 26193, 29649, 24223, 18547, 11002,\n",
      "        1380]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  207, 13206, 85746, 16668,  8592,  4950,  2468,   710,    68,\n",
      "           3]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   73,  7242, 20292, 24884, 19578, 13545, 15026, 16186, 11759,\n",
      "        4033]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([ 3375, 19484, 15415,  9290,  2870,  1970,  1459,   638,    51,\n",
      "          12]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   38, 10074, 15565,  9661,  5465,  4955,  4922,  2923,   868,\n",
      "          93]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   25, 12239, 23145, 12344,  5057,  1128,   356,   206,    52,\n",
      "          12]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   14,  1181, 14419, 44771, 74028, 32192,  3995,   668,   341,\n",
      "          59]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  102,  1615, 34102, 48395, 60521, 19441,  5743,  1538,   192,\n",
      "          19]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   30, 34161, 43366, 34150, 27131, 18802, 10783,  3065,   152,\n",
      "          28]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    5,    25,   235,  6847, 39657, 62088, 43535,  7669,    67,\n",
      "           3]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1087, 24752, 83680, 32653,  9426,  5022,  2497,   837,   152,\n",
      "          25]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   15,    63,   234, 45980, 43927, 30427, 21025, 13211,  4580,\n",
      "         669]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   1,    8,   38,  276, 1251, 3452, 3999, 3837, 2561,  158]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 108, 4053, 7297, 2297, 1208,  464,  120,   26,    5,    3]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   2,    6, 5403, 5748, 2857, 1075,  318,  102,   44,   26]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([ 6570, 28139, 19041,    70,     8,     0,     0,     6,     4,\n",
      "           8]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    2,   265,  3481, 16533, 16670,  9492,  5811,  1470,   113,\n",
      "           9]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    9,   177,  1545, 14098, 21667,  7871,  3773,  2471,  1685,\n",
      "         550]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    5,   384,  9061, 38945, 27226,  1921,    48,    17,     8,\n",
      "           6]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 2951, 18587, 37338, 12230,  4269,  1423,   564,   204,    48,\n",
      "           7]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([14274, 13107, 12553, 11779,  9823,  8930,  5321,  1471,   330,\n",
      "          33]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    14,    402,  16812,  86454, 105266, 112674,  33768,  10786,\n",
      "         4199,    115]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([     3,      5,    171,  10058, 133514, 145810,  59953,  17630,\n",
      "         2912,    434]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([147474,  67061,  51348,  42293,  28449,  15901,  10262,   4553,\n",
      "         2506,    643]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    8,    75,  1675,  5740, 13083, 24707, 13879,  2196,   176,\n",
      "           4]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  281, 21672, 25959,  3155,  2856,  3588,  2973,   938,   111,\n",
      "          10]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    5,   818,  8595, 10406, 12361, 14275, 10202,  3951,   773,\n",
      "         157]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([  19,  405, 2809, 6247, 7543, 6272, 4633, 2885,  988,   44]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  254,  2353,  3543, 13099,  7126,  2636,  1734,   877,   210,\n",
      "          13]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([9723, 6865, 4884, 3831, 2879, 1936, 1202,  408,   89,   28]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([  292, 15663, 67540, 32804,  4248,  1185,   705,   289,    77,\n",
      "          12]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1772, 25395, 37022,  4907,  6406, 18702, 20455,  5676,  2306,\n",
      "         174]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  136, 52028, 41398, 19902,  8916,   348,    50,    20,    11,\n",
      "           6]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   49,   235,   937,  5393, 25664, 34176, 15859,  2915,    89,\n",
      "           6]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   12,   178,  2579, 26355, 30475, 10153,  5798,  6687,  2947,\n",
      "         139]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   39,   222,  6465, 10935, 12422, 18314, 22653, 12363,  1842,\n",
      "          68]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   44,  1162,  9655, 24555, 24901, 16122,  8670,  2066,   347,\n",
      "          44]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1753, 38105, 29445,  5490,  3393,  3942,  3329,  1507,   541,\n",
      "          61]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([11159, 10992, 12490, 14342, 14461, 11524,  8202,  3407,   891,\n",
      "          98]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  495,  6809, 14055, 12173,  6186,  3339,   336,    29,    14,\n",
      "           7]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  196,  4072, 19624, 13271,  2525,  1486,  1366,   699,   176,\n",
      "          28]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([4365, 4870, 5413, 5484, 4614, 5376, 5760, 4732, 2568,  261]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   12,    77,   725,  7451, 24244, 28552, 14453,  2270,   145,\n",
      "           2]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  325, 11584, 26328, 14635, 12553,  9110,  2860,   456,    73,\n",
      "           7]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 8412, 10182, 12772, 14823, 13900,  8565,  5039,  2839,  1210,\n",
      "         189]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  311,  3582, 11717, 28230, 41119, 33409, 20872,  9217,  2807,\n",
      "         250]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1258, 64586, 47649, 10354, 11407,  9810,  4735,  1364,   326,\n",
      "          25]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   39,   535,  4478, 23074, 37416, 39461, 24435, 14029,  7016,\n",
      "        1031]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([  97,  879, 5173, 6801, 5231, 4441, 2829, 1543, 1125,  481]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   49,   353,  6043, 11889,  3635,  2871,  2271,  1152,   301,\n",
      "          36]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([5102, 4802, 6375, 4316, 1835, 1460, 1220, 1231, 1434,  825]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([     2,     53,   1196,  30639, 238622, 233430,  39762,   3198,\n",
      "          218,     26]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 10917, 276687, 124708,  44625,  45223,  32530,  10123,   2065,\n",
      "          248,     20]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   111,  39055, 113351, 103791, 100409, 105241,  77382,   7679,\n",
      "          120,      7]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([  173,  1970, 12816, 35032, 28044, 19285, 18832, 13101,  4397,\n",
      "         138]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 5784, 55115, 51509, 16142,  3137,  1191,   516,   278,   104,\n",
      "          12]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  143,     0,     0,    84,  7712, 28446, 27548, 28029, 33671,\n",
      "        8155]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   44,   441,  3182, 11373, 22111, 25384, 23125, 20289,  8506,\n",
      "         815]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  148, 14509, 90292,  6517,  1296,  1034,   773,   531,   154,\n",
      "          16]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([20824, 17320, 15897, 14472, 13778, 11839,  9673,  7379,  3737,\n",
      "         351]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  190,  1616,  5896, 18712, 37009, 42945, 24434,  8022,   889,\n",
      "          42]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  915, 47616, 76621,  8178,  3824,  1475,   685,   288,   116,\n",
      "          37]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   16,   173,  1043, 18529, 24913, 25450, 22788, 21117, 22048,\n",
      "        3678]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   20,   263,  6281, 31697, 49526, 44416, 28838, 14752,  5475,\n",
      "         465]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 3313, 65356, 82552, 15225,  5497,  4786,  2622,  1592,   722,\n",
      "          68]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([21811, 23947, 24704, 22171, 18974, 21020, 22804, 20720,  5442,\n",
      "         140]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    22,    159,   4137,  88346, 192315,  85481,   8136,    161,\n",
      "           25,      5]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  1849, 154305, 121792,  15088,  21960,  28838,  25498,   8088,\n",
      "         1275,     84]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([66268, 77568, 73887, 58944, 36921, 28332, 21847, 12312,  2498,\n",
      "         210]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([ 1571,  8527,  6076,  7480,  8604, 10822, 13555, 18244,  4191,\n",
      "         128]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 3087,  8879,  6749, 18428, 33170,  6853,  1739,   240,    44,\n",
      "           9]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   15, 21475, 17992, 18194, 11628,  4252,  2220,  1538,  1501,\n",
      "         383]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   10,    68,  1109,  7716, 29783, 51184, 50602, 31347,  8961,\n",
      "         563]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   411,  35198, 124550,  17039,   1984,   1096,    524,    271,\n",
      "          186,     84]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   13,   149, 35232, 41129, 31640, 27298, 21593, 15038,  8601,\n",
      "         650]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([ 3483, 18027, 14932,   465,   319,   238,    79,    30,    14,\n",
      "           8]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  128,  1442, 11760, 16537,  4422,  2098,   788,   335,    69,\n",
      "          16]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([19350, 11765,  5157,   921,   270,    80,    37,     7,     6,\n",
      "           2]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   23,   925, 14797, 48330, 67772, 66704, 41924, 22569,  4262,\n",
      "         148]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  2072,  21620, 122262,  78562,  24279,  12569,   5092,    929,\n",
      "           58,     11]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([44082, 56421, 48471, 27301, 21221, 21928, 22045, 19022,  6663,\n",
      "         300]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([  151,  1849,  4434,  6080, 17906, 25621, 19936, 15318,  7400,\n",
      "         862]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  333, 26450, 59344, 10450,   556,   533,   598,   713,   509,\n",
      "          71]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([11528, 10729, 10805, 10428, 11137, 13426, 16787, 10800,  3759,\n",
      "         158]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([    5,    20,    43,   211,  3997, 27323, 41327, 49084, 41206,\n",
      "        1053]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   42,   294, 14576, 93932, 32072, 15114,  5965,  1887,   277,\n",
      "         110]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([     8,     21,     52,    156,   1820,   3297, 112779,  40582,\n",
      "         5177,    377]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  612, 11180, 37154, 60449, 40759,  9928,   247,    39,    14,\n",
      "           2]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 2372, 56283, 82525,  7516,  5993,  4177,  1262,   230,    23,\n",
      "           3]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([31439, 25671, 21976, 18417, 15250, 13515, 13630, 12482,  7252,\n",
      "         752]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   10,   103,  1126,  5624, 13045, 17149, 12075,  5348,   857,\n",
      "          17]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  331, 12198, 30943,  5373,  3059,  1905,  1040,   384,   106,\n",
      "          15]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([11575, 12176, 11689,  8862,  6024,  3357,  1288,   309,    63,\n",
      "          11]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([  167,  2167,  7238, 13358, 21930, 28764, 24112, 11149,  2969,\n",
      "         215]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1115, 27927, 59712, 12373,  3237,  3116,  2576,  1443,   482,\n",
      "          88]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   14,    37,   205,  1346, 19097, 19656, 19893, 22090, 23092,\n",
      "        6639]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    7,    68,   784,  4561, 16460, 42463, 46746, 33341, 11867,\n",
      "        1111]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   17,  2047, 65771, 66477, 12175,  5499,  3189,  1725,   458,\n",
      "          50]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([22788, 20066, 21014, 16349, 13892, 14142, 17237, 21809,  9185,\n",
      "         926]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    25,    205,   4314,  67911, 207152,  98226,   2488,    380,\n",
      "          150,     38]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   371,  60090, 230667,  48656,  12519,  11313,  10038,   5565,\n",
      "         1533,    137]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   53,   394, 14382, 69575, 72135, 63727, 63514, 66016, 29104,\n",
      "        1989]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   18,   317, 14605, 68704, 59329, 13348,   478,     5,     4,\n",
      "           1]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  156, 26996, 83144, 22753,  8094,  6956,  4752,  2750,  1056,\n",
      "         152]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([27717, 23446, 22163, 19425, 17331, 17151, 14638, 12431,  2463,\n",
      "          44]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   10,   104,   375,  1192,  6447, 17239,  6248,  1801,   541,\n",
      "         144]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  194,  1273, 13000, 10077,  5890,  2477,   819,   305,    54,\n",
      "          12]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([13906, 12841,  5259,  1002,   378,   234,   174,   133,   130,\n",
      "          44]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   17,    56,   236,  2021, 18950, 79961, 45985,  1090,   103,\n",
      "          15]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([   72,   441,  4275, 47715, 84172,  7306,  2527,  1458,   439,\n",
      "          29]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([    3,    17, 13820, 68193, 42093, 15159,  6240,  2303,   548,\n",
      "          58]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([   9,   42,  486, 2463, 4237, 5530, 2913, 1043,  356,   63]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 212, 7616, 4765, 2142, 1474,  493,  208,  169,   52,   11]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  70, 1807, 4602, 4062, 2881, 2022, 1143,  422,  109,   24]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([ 1025, 11126, 25195, 16330,  7811,  1874,    61,    22,    10,\n",
      "           4]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 5822, 38106, 17425,   882,   380,   314,   270,   159,    77,\n",
      "          23]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([8503, 7279, 7085, 7615, 8085, 7269, 7296, 7208, 2930,  188]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    8,    58,  1752, 10142, 30687, 52699, 43525, 16585,  3338,\n",
      "         194]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  741, 55068, 80332,  7451,  3387,  4241,  3897,  2655,  1100,\n",
      "         116]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  281,  6861, 18270, 21769, 22618, 23522, 24979, 28131, 12139,\n",
      "         418]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 1.0), ([(array([    2,    20,   231,  1945,  6737, 13181, 26545, 29672, 10351,\n",
      "         364]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  221,  2833, 45487, 30934,  2796,  2391,  2142,  1554,   623,\n",
      "          67]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([17690, 13604, 11789, 11428,  9578,  7890,  7790,  6090,  2535,\n",
      "         654]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0), ([(array([   24,   370, 36655, 76116, 40005, 11107,  6329,  1936,   232,\n",
      "          23]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([ 1930, 38765, 91165, 24911, 10478,  3709,  1248,   438,   138,\n",
      "          15]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])), (array([  168, 11080, 66973, 50489, 30306,  9602,  2934,  1041,   193,\n",
      "          11]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))], 0.0)]\n",
      "CPU times: user 3.17 s, sys: 8.78 s, total: 11.9 s\n",
      "Wall time: 5min 28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-117:\n",
      "Process ForkPoolWorker-109:\n",
      "Process ForkPoolWorker-114:\n",
      "Process ForkPoolWorker-93:\n",
      "Process ForkPoolWorker-113:\n",
      "Process ForkPoolWorker-90:\n",
      "Process ForkPoolWorker-86:\n",
      "Process ForkPoolWorker-106:\n",
      "Process ForkPoolWorker-104:\n",
      "Process ForkPoolWorker-119:\n",
      "Process ForkPoolWorker-112:\n",
      "Process ForkPoolWorker-116:\n",
      "Process ForkPoolWorker-89:\n",
      "Process ForkPoolWorker-102:\n",
      "Process ForkPoolWorker-96:\n",
      "Process ForkPoolWorker-111:\n",
      "Process ForkPoolWorker-88:\n",
      "Process ForkPoolWorker-97:\n",
      "Process ForkPoolWorker-95:\n",
      "Process ForkPoolWorker-85:\n",
      "Process ForkPoolWorker-87:\n",
      "Process ForkPoolWorker-98:\n",
      "Process ForkPoolWorker-107:\n",
      "Process ForkPoolWorker-105:\n",
      "Process ForkPoolWorker-82:\n",
      "Process ForkPoolWorker-103:\n",
      "Process ForkPoolWorker-81:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-92:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-83:\n",
      "Process ForkPoolWorker-94:\n",
      "Process ForkPoolWorker-108:\n",
      "Process ForkPoolWorker-101:\n",
      "Process ForkPoolWorker-118:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-91:\n",
      "Process ForkPoolWorker-110:\n",
      "Process ForkPoolWorker-100:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-120:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-115:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-99:\n",
      "Process ForkPoolWorker-84:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gliomi import *\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "sequence_repo = SequenceRepoGliomi(\"/data/RMN/dataset-gliomi\")\n",
    "\n",
    "# Load data\n",
    "\n",
    "def get_histogram(item):\n",
    "    \n",
    "    sequence_repo, subject, sequence_names, label = item\n",
    "\n",
    "    print(\"Working on\", subject)\n",
    "\n",
    "    roi = sequence_repo.get_roi(subject)\n",
    "    \n",
    "    histograms = []\n",
    "    \n",
    "    for sequence_name in sequence_names:\n",
    "    \n",
    "        sequence = sequence_repo.get_sequence(subject, sequence_name)\n",
    "\n",
    "        sequence = nb.Nifti1Image(\n",
    "                sequence.get_fdata() * roi.get_fdata(),\n",
    "                affine=sequence.affine)\n",
    "\n",
    "        values = sequence.get_fdata()\n",
    "\n",
    "        values = values[values > 0]\n",
    "\n",
    "        _max = np.max(values)\n",
    "        _min = np.min(values)\n",
    "        \n",
    "        values = (values - _min) / (_max - _min)\n",
    "    \n",
    "        histograms.append(np.histogram(values))\n",
    "        \n",
    "    return histograms, label\n",
    "\n",
    "df = pd.read_csv(\"/data/RMN/dataset-gliomi-cnn/dataset-survivor.csv\")\n",
    "subjects = np.array(df.iloc[:,1])\n",
    "labels = np.array(df.iloc[:,2])\n",
    "\n",
    "tasks = []\n",
    "\n",
    "sequence_names = [\"T1\", \"MPRAGE\", \"FLAIR\"]\n",
    "\n",
    "real_names = []\n",
    "for subject, label in zip(subjects, labels):\n",
    "    if subject == \"RE_ATTILIO\":\n",
    "        continue\n",
    "    if not sequence_repo.has(subject, \"FLAIR\"):\n",
    "        continue\n",
    "    if not sequence_repo.has(subject, \"MPRAGE\"):\n",
    "        continue\n",
    "    if not sequence_repo.has(subject, \"T1\"):\n",
    "        continue\n",
    "    tasks.append((sequence_repo, subject, sequence_names, label))\n",
    "    real_names.append(subject)\n",
    "    \n",
    "print(\"Starting on\", len(tasks), \"tasks\")\n",
    "    \n",
    "processing_pool = Pool(int(multiprocessing.cpu_count()))\n",
    "dataset = processing_pool.map(get_histogram, tasks)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(array([  221,  8721, 31528, 32102,  5993,   528,    11,    14,    13,\n",
       "               8]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  171, 26534, 37820,  4727,  2677,  2475,  2575,  1731,   403,\n",
       "              26]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   45,   159,   487, 12526, 12971, 18499, 22060, 11535,   766,\n",
       "              91]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  181,  2958, 11374, 12278,  7385,  3772,    99,    43,    11,\n",
       "              13]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  733, 10268, 12755,  6299,  6610,   840,   379,   157,    54,\n",
       "              19]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 604, 2993, 6189, 9761, 9043, 8025, 1435,   34,   21,    9]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([  258,  4681, 11179, 13851, 18158, 26241, 23141, 12492,  4092,\n",
       "             264]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  563, 21186, 72281,  7580,  3704,  3973,  3430,  1474,   151,\n",
       "              15]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 4378, 15330, 14577, 14699, 13877, 15584, 15403, 13984,  6125,\n",
       "             400]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    4,     5,     7,    68,   631, 16869, 87040, 72421, 12476,\n",
       "             415]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    42,   9045, 107617,  56126,  12809,   2974,    949,    301,\n",
       "               62,     11]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   11,   122,   852,  5475, 51375, 47916, 40745, 31235, 10889,\n",
       "            1316]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    5,    10,    35,   481,  5375, 17246, 17064,  6205,   753,\n",
       "              61]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  192,  1213,  5671, 28597,  5198,  2540,  2429,  1156,   206,\n",
       "              33]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([7157, 6705, 6681, 6739, 6076, 5253, 4682, 3488,  410,   44]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    5,    24,   157,   794,  5524, 12308, 23378, 27829, 14381,\n",
       "            1352]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   37,   810, 13952, 51673, 15811,  1639,  1029,   648,   136,\n",
       "              17]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    9,    62,   283,   770,  2111, 19224, 16897, 16937, 21475,\n",
       "            7984]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   30,     0,     0,     0,     0,     4,  2301, 40968, 55462,\n",
       "            6981]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  155, 20925, 57381, 15765,  7777,  3048,   573,    90,    24,\n",
       "               8]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([21480, 17257, 15737, 13573, 11549,  9025,  6995,  6382,  3537,\n",
       "             211]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([ 145,  644, 1676, 1982, 2578, 3492, 3512, 2055,  215,    7]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 919, 3467, 7875, 2322, 1197,  430,   56,   25,   11,    4]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   6,    8, 3854, 4828, 3757, 2458, 1114,  230,   42,    9]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   40,   420,  3210, 16039, 27479, 44139, 42540, 17070,   104,\n",
       "              13]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  426, 48431, 88653,  5901,  3168,  2851,  1353,   227,    30,\n",
       "              14]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    3, 21076, 50152, 31589, 25856, 15521,  3182,  2246,  1331,\n",
       "              98]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   5,   21,  155, 1730, 5244, 5987, 5485, 2101,  454,   40]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  13,  267, 1888, 7277, 3552, 3402, 3427, 1254,  130,   12]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([8912, 5159, 2361, 1733, 1127,  532,  518,  517,  293,   70]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  498,  7705, 23390, 31474, 29136,  7371,   154,    30,    20,\n",
       "               6]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1065, 23741, 56682, 13863,  2937,  1115,   256,    77,    35,\n",
       "              13]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([20748, 14429, 13074, 12099, 12971, 12680, 11538,  2056,    94,\n",
       "              95]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    7,   124,   830,  3648, 11454, 22983, 21897,  9220,  1512,\n",
       "              62]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  573, 15841, 41093,  6317,  4433,  2411,   822,   202,    39,\n",
       "               6]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    2,     3, 10358, 12896, 12668, 11315,  9625,  9287,  5173,\n",
       "             410]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   250,  19738, 275777, 201641,  24217,   7132,   1535,    151,\n",
       "               70,     30]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   138,   3397,  71728, 257768, 112685,  61669,  18657,   4019,\n",
       "              442,     38]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([360253, 141725,  24621,   2064,    751,    466,    297,    196,\n",
       "              119,     49]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   28,   255,  2202, 25657, 53642, 45470, 21907,  5808,   147,\n",
       "               9]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1846, 90545, 51470,  4397,  3987,  2435,   417,    23,     3,\n",
       "               2]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   24, 12994, 31226, 25310, 25177, 20495, 17020, 14054,  8427,\n",
       "             398]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    55,    419,   1332,   4844,  35262, 149701, 120345,  49959,\n",
       "             9327,    234]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   619,  98717, 227896,  36571,   6223,   1195,    208,     37,\n",
       "                8,      4]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([111690, 110330,  78549,  42629,  19291,   6680,   1777,    447,\n",
       "               72,     13]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    6,    55,  1523, 10566, 24534, 35884, 23318,  8211,   386,\n",
       "              52]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  278, 17141, 58412, 18947,  4644,  2602,  1423,   813,   251,\n",
       "              24]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([30639, 18798, 12471, 12515, 13579, 11933,  2734,   769,   772,\n",
       "             325]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([ 8982, 49686,  7235,   518,     1,     1,     0,     1,     1,\n",
       "               2]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  603, 29156, 26400,  2523,  2161,  2366,  1953,  1018,   241,\n",
       "               6]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  137,   287,   266,  8110, 15000, 15960, 17006,  9348,   280,\n",
       "              33]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    9,    71,   879,  5777, 28370, 60370, 63392, 37722,  4944,\n",
       "              11]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   447,  25600, 104484,  40330,  18765,   7783,   2712,   1074,\n",
       "              303,     47]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   14,  3414, 52056, 43315, 35179, 28661, 19923, 13500,  4496,\n",
       "             987]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  845, 13236, 18960, 30839, 24103,  1192,   264,   182,    93,\n",
       "              16]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 2466, 24330, 36646,  9078,  5110,  4515,  4017,  2803,   700,\n",
       "              65]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    1,    10, 15525, 39468, 24240,  8984,  1453,    27,    13,\n",
       "               9]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   49,   753,  2756,  9057, 23992, 44223, 44588, 23790,  9755,\n",
       "             621]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  244, 24662, 94457, 26421,  4025,  4491,  3231,  1617,   380,\n",
       "              56]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   14,   449,  1790, 30815, 30738, 25888, 24059, 23377, 21051,\n",
       "            1403]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   13,    43,   103,   767,  5877, 14632, 13238,  3401,   311,\n",
       "              15]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   29,  8826, 18351,  3241,  3651,  2549,   973,   503,   237,\n",
       "              40]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   52,   200,   695,  1736, 14008, 15577,  4480,  1287,   351,\n",
       "              14]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([10476, 58579, 15437,  1239,     6,     0,     2,     0,     0,\n",
       "               1]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1961, 48902, 10330, 11143,  8029,  4080,  1075,   170,    38,\n",
       "              12]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    6,  5144,  9194,  7475,  6626,  6617,  7589, 13832, 23820,\n",
       "            5437]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([ 12625,  66004, 129178, 110348, 118346, 101155,  42905,   7606,\n",
       "              174,     13]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    63,   5364, 125002, 289264,  69961,  65393,  27209,   5166,\n",
       "              880,     52]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    84,    823, 146047, 152748, 124638,  98980,  50227,  13057,\n",
       "             1730,     20]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  1712, 156083, 333385, 156454,  17084,   1923,    353,    169,\n",
       "              122,     62]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    33,   1648, 113447, 418021, 102396,  19241,   8636,   3410,\n",
       "              489,     26]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 68729, 332976, 162115,  64200,  22756,   9915,   4479,   1757,\n",
       "              360,     60]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   85,  1526, 36650, 91398, 72684, 20864,   253,    19,     5,\n",
       "               5]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   169,  10689, 103833,  75352,  14165,  11688,   5865,   1500,\n",
       "              214,     14]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([31606, 30065, 33681, 39170, 24269, 19718, 22361, 17866,  4611,\n",
       "             142]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  196, 19293, 47937, 31752,  1119,     1,     0,     0,     1,\n",
       "               1]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  555, 17036, 27290, 37697, 12742,  3640,  1030,   229,    63,\n",
       "              18]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 8118, 13489, 14643, 14962, 14972, 13159, 10473,  6814,  3116,\n",
       "             554]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   164, 173484, 610898, 163405,  21026,   3148,    535,    125,\n",
       "               54,     13]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    32,   2917, 209319, 628232, 113680,   9770,   5996,   2528,\n",
       "              364,     14]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    19,     40,     91,   2125, 288725, 363200, 214050,  91564,\n",
       "            12540,    498]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   77,  1693, 23444, 82014, 91174, 32859,  8511,   367,    31,\n",
       "               9]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   245,  27131, 118201,  69094,  16778,   6199,   1859,    490,\n",
       "              158,     24]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   51,    58,  1003, 60797, 59406, 44904, 38699, 26922,  7988,\n",
       "             351]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   11,    67,   566,  3421, 10349, 13801,  5538,   629,    37,\n",
       "               4]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    8,   439, 16397, 12961,  1766,  1062,  1104,   549,   120,\n",
       "              17]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   13,  8681, 11693,  8091,  3664,  1580,   593,    95,     8,\n",
       "               5]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    2,     6,   121,   299,  2909, 15654, 45461, 29030,  3621,\n",
       "              17]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   46,  3451, 31357, 30691, 13590,  9756,  5624,  2099,   466,\n",
       "              40]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   46,   168,  2802, 22058, 17482, 17210, 16792, 14407,  5778,\n",
       "             377]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   702,  45112, 443822, 940042, 196631,   4247,    255,     93,\n",
       "               23,      2]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  1331, 504227, 807460, 101639,  65728,  58084,  48265,  31341,\n",
       "            11942,    912]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([268714, 320957, 315511, 284484, 248229, 151795,  39412,   1801,\n",
       "               21,      5]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   3,    3,   18,   54,  380, 1059, 1864, 2082,  781,  102]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  81,  926, 1062,  847,  975, 1164,  932,  298,   49,   12]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  16,  280, 2574, 1657,  792,  469,  316,  158,   70,   14]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  2756, 105563, 232341, 294252, 377630, 212099,  16851,    201,\n",
       "                6,      7]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  4170, 244735, 813701,  61615,  36728,  37037,  29869,  12050,\n",
       "             1704,     97]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([107026, 152147, 184503, 131329, 130383, 181541, 204469, 121411,\n",
       "            27903,    994]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   55,     0,     0,    95,  3330,  8693, 10024,  6312,  3510,\n",
       "            1176]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  258,  5768, 15403,  8709,  1230,   622,   702,   409,    88,\n",
       "               6]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([3988, 3469, 3433, 3523, 3262, 3315, 3751, 4606, 3543,  305]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   12,    55,  7058, 36466, 34579, 14598,  3580,   289,    61,\n",
       "              28]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  545, 26222, 48923,  8970,  3950,  3693,  2827,  1209,   343,\n",
       "              44]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    2,     3,    65,  1316,  7337, 28341, 40686, 18729,   139,\n",
       "             108]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   21,   636,  4056, 25401, 48101, 40616, 24106,  1609,   164,\n",
       "              33]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  354,  9836, 50956, 40478, 13534, 10720, 11089,  6144,  1475,\n",
       "             157]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   16,    55, 16263, 37171, 38288, 26109, 15084,  8878,  2805,\n",
       "              74]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  132,  2864, 12137, 26421, 47483, 43618, 42662, 28989,  9736,\n",
       "             618]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 3404, 72365, 93365, 15352, 13091, 10560,  4563,  1538,   390,\n",
       "              32]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  466, 25683, 43744, 35002, 29722, 26543, 23252, 19625,  9687,\n",
       "             936]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    65,   6004, 100030, 322542, 169644,  22885,    414,     17,\n",
       "               11,      7]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 20129, 465274,  93776,  18325,  13798,   7030,   2299,    786,\n",
       "              163,     39]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 95046,  84910,  79091,  88884, 105107,  92246,  56207,  17747,\n",
       "             2267,    114]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    6,     9,    47,   518,  2308, 11417, 11847,  6469,  4130,\n",
       "             853]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  353,  4047, 14294,  4067,  5437,  5742,  2733,   777,   136,\n",
       "              18]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([3251, 7658, 8004, 7109, 4871, 2775, 1888, 1290,  663,   95]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   19,   535,  5902, 18507, 41376, 62562, 36728,  7332,  1104,\n",
       "             113]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1556, 38330, 88546, 16780, 14789,  9495,  3684,   874,   119,\n",
       "               5]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   59, 50631, 43062, 29162, 23755, 19512,  6825,   999,   152,\n",
       "              21]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([ 1218,  4489, 10679,  3356,    60,    22,     7,     5,     1,\n",
       "               2]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  27,  963, 3963, 9906, 2128, 1208,  920,  551,  155,   18]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   2, 3304, 3090, 3306, 3613, 2604, 1286, 1098, 1143,  393]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   29,   478,  6827, 56832, 16785,  8528,  4831,  2582,  1336,\n",
       "             195]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  123,   637,  5398, 25818, 28585, 33090,  4301,   342,   100,\n",
       "              29]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1396, 56516, 16357,  7210,  4670,  4097,  3894,  3144,  1036,\n",
       "             103]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    2,    18,   451,  8986, 33043, 40933, 17048,  1984,    27,\n",
       "               4]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  696, 27423, 46025, 11925,  6610,  5360,  2822,  1159,   405,\n",
       "              71]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([18755, 16112, 13635, 11131, 10255,  9743,  8711,  8252,  5340,\n",
       "             562]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([  185,  8605, 24273, 25277, 13825,  6480,   570,     5,     1,\n",
       "               6]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   16,    53,  9264, 43931, 20265,  3620,  1449,   485,   122,\n",
       "              22]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([13348, 11611,  8922,  7469,  7112,  7867,  8976,  8585,  4951,\n",
       "             386]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([ 2091, 33213, 37981,  5588,  2587,  1841,  1311,   650,   362,\n",
       "              88]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   58,  7287, 43097, 25532,  4199,  3206,  1711,   536,    82,\n",
       "               4]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([15143, 35376, 25333,  4380,  1678,  1394,  1138,   819,   382,\n",
       "              69]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  301,  4957,  5675, 13805, 21022, 20213, 12715,  2885,   419,\n",
       "               4]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1893,  3960, 12223, 27887, 11424,  9478,  9974,  4650,   482,\n",
       "              25]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([15893, 17511, 20824, 14751,  5801,  3362,  2341,  1085,   379,\n",
       "              49]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([  569,  9748, 21139, 22695, 22500, 20273, 11416,  5387,  1365,\n",
       "             115]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1174, 33799, 43090, 18819,  8100,  4863,  2861,  1735,   678,\n",
       "              88]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([22222, 30601, 25233, 18367, 10920,  4580,  1672,   903,   595,\n",
       "             114]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   790,  21092, 396680, 457714,  22442,    726,    195,     61,\n",
       "               16,      4]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  1530,  10068, 151724, 599040,  73652,  45705,  15454,   2040,\n",
       "              464,     43]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([252521, 180440, 152671, 136791,  88491,  43466,  22658,  14732,\n",
       "             7289,    661]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    7,   155,  1551,  6116, 16609, 19168, 15613,  8296,  2989,\n",
       "             332]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1146, 26334, 30047,  5372,  2160,  2096,  1604,  1337,   670,\n",
       "              70]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   39,   248,  6239, 11637, 10341, 10105,  9998, 11735,  9839,\n",
       "             655]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   26,  2923, 21463, 33912, 19497,  5426,   742,     4,     3,\n",
       "               1]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   11,  1034, 36925, 36035,  3458,  2558,  2547,  1232,   189,\n",
       "               8]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([10473, 10337, 10651, 10206, 10416, 11061, 10123,  8054,  2493,\n",
       "             183]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    3,    21,    33,    89,   504, 11959, 49182, 49668, 22058,\n",
       "            1545]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  111, 15895, 65505, 21314, 15924, 12308,  3177,   645,   160,\n",
       "              23]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   48,    75,   138,   789,  6483, 35456, 39851, 26496, 22184,\n",
       "            3542]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  107,  8197, 23795, 31104, 23316, 14871, 11235,  7601,  4097,\n",
       "             696]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  155, 20532, 34109, 31109, 20007, 13125,  4963,   919,    88,\n",
       "              12]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   13,  1344, 17313, 24441, 24403, 42269, 13383,  1431,   375,\n",
       "              47]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   66,  1375, 10902, 26148, 33579, 27709, 19452, 10009,  2116,\n",
       "              84]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  447, 41687, 71773,  7502,  4151,  3081,  1604,   930,   247,\n",
       "              18]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([17586, 16389, 13485, 12569, 12874, 14199, 17063, 18937,  8092,\n",
       "             246]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([ 1059, 13599, 37011, 14406,  1539,    34,    17,    15,     8,\n",
       "              12]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  279, 37279, 28436,  1349,   263,    61,    15,     2,     5,\n",
       "              11]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([11653, 11028,  9709,  9120, 12712,  9326,  3322,   674,   147,\n",
       "               9]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  283,  2799,  9119, 16201, 26017, 31907, 26100, 10811,  3756,\n",
       "             297]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 3227, 69746, 32881,  7496,  6557,  5117,  1961,   288,    14,\n",
       "               3]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 8197, 18047, 16189, 15514, 15930, 17697, 18760, 12688,  4039,\n",
       "             229]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    2,     4,     0,     9,   817, 21018, 49414, 30221, 12514,\n",
       "            1220]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  311, 24531, 65129, 11904,  3974,  3965,  3016,  1873,   488,\n",
       "              28]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    6,     8,    62,  2021, 10199, 31901, 35045, 28132,  7671,\n",
       "             174]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   27,   233,  2722,  7685, 14452, 21435, 19750, 10961,  3306,\n",
       "             320]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 6397, 49355,  8260,  4085,  4880,  4840,  2227,   525,   231,\n",
       "              91]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   15,    45,  4896, 17327, 19584, 10898,  8829, 11290,  7377,\n",
       "             630]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   3,    6,    8,   12,   56,  993, 5001, 6132, 3934,  817]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    8,    55,  1825, 10930,  3436,   532,   128,    33,    14,\n",
       "               1]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   1,    5,    8,   15,   58,  633, 4256, 8420, 3286,  280]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   91,  1598,  7481, 19947, 36798, 37050, 25165, 12956,  3657,\n",
       "             232]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 2075, 15069, 81904, 35173,  7458,  2346,   631,   215,    91,\n",
       "              13]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   34,   172, 41248, 39174, 24862, 13645, 10023,  9009,  6123,\n",
       "             685]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    37,    140,   1847,  39584, 101238,  24691,    314,     52,\n",
       "               23,      7]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  239, 12798, 87619, 39062, 11160,  9817,  5182,  1603,   400,\n",
       "              53]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   14,  1251, 35001, 31655, 30761, 32443, 27757,  7360,  1549,\n",
       "             142]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    61,      0,      0,      0,      8,   4221,  29467, 116261,\n",
       "            49237,   1799]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  1738,  33061, 120319,  23653,   9303,   9026,   3394,    501,\n",
       "               49,     10]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   13,     0,     0,     0,     0, 29478, 48972, 51016, 50683,\n",
       "           20892]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   531,   3899,  45839, 179532, 260460, 271550, 186167,  70356,\n",
       "            10627,     77]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   569,  34938, 367727, 479685, 113058,  28535,   3788,    600,\n",
       "              129,      9]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    15,  67280, 254854, 212156, 128328,  83026, 108079, 134121,\n",
       "            40591,    588]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    4,    11,   134,  1284,  3519,  8287, 13621, 10971,  8331,\n",
       "            2032]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  747,  2854, 19427, 11295,  5117,  4745,  2978,   902,   117,\n",
       "              12]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    7,     7,    51,   930, 10558, 21354,  9273,  4079,  1724,\n",
       "             211]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    2,     0,   147,  2644, 11634, 19892, 22887, 10717,  3832,\n",
       "             534]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  113,  3209, 40965, 22600,  2075,  1606,  1126,   482,    98,\n",
       "              15]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    6, 12044, 10545,  8020,  7227,  7357,  8468, 10350,  7525,\n",
       "             747]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    9,    30,  1935, 15577, 59135, 76287, 31575,  2904,   281,\n",
       "              16]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  2556, 104335,  60569,   8154,   5588,   4383,   1856,    260,\n",
       "               43,      5]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   40,   136,   223, 22308, 35488, 34736, 36429, 38288, 19509,\n",
       "             592]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    7,    15,    30,   183,   866,  4771, 14987, 21058, 15725,\n",
       "             892]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  174,  3941, 24923, 26236,  1231,   742,   649,   455,   164,\n",
       "              19]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    2,     3,    11,    21,   108,   991, 29372, 19726,  7292,\n",
       "            1008]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   59,   713,  4556, 11378, 12171, 12648,  8435,  2083,   369,\n",
       "              12]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  400, 12510, 30766,  6268,  1124,   698,   474,   166,    17,\n",
       "               1]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  95, 4687, 7966, 6202, 5548, 5270, 5820, 7076, 7775, 1985]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([  326,  8307, 14756, 18519, 16605, 16597,  8887,  1062,   155,\n",
       "              20]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1015, 18694, 37517, 19427,  5499,  1949,   814,   252,    58,\n",
       "               9]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([21086, 29516, 19493,  8239,  4447,  2093,   232,    67,    41,\n",
       "              20]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   87,  2893, 10933, 16222, 17456, 15249, 11386,  2258,    74,\n",
       "               9]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1711, 16594, 40707, 13062,  3541,   735,   143,    48,    21,\n",
       "               5]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   11,  6642, 12659, 11703, 11387, 12039,  9683,  6702,  4720,\n",
       "            1021]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   10,   524,  4609, 12469, 16697, 16019, 11393,  6621,  2840,\n",
       "             196]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  179, 11504, 30756,  8470,  3924,  4047,  5602,  4994,  1690,\n",
       "             212]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 6574,  7043, 10542,  8906,  7856,  7611,  8388,  9334,  4988,\n",
       "             136]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    8,    15,    54,   256,  5805, 57622, 73276, 44679,   708,\n",
       "               6]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  110,  8706, 69354, 71547, 16263, 11659,  3814,   894,    74,\n",
       "               8]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    2, 16491, 37879, 39153, 35020, 30384, 19240,  3915,   298,\n",
       "              47]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  254,  8071, 49278, 33724,  2975,   258,    49,     7,     0,\n",
       "               1]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  513, 27723, 47260,  9128,  5231,  2587,  1405,   647,   111,\n",
       "              12]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([32247, 17050, 12060, 10904, 11079,  9586,  1170,   299,   174,\n",
       "              48]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    5,   173,  4464, 74000, 95454,  8368,     7,     0,     0,\n",
       "               2]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   281,  70293, 103716,   3918,   2417,   1268,    437,    123,\n",
       "               15,      5]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([27550, 27900, 29076, 27713, 25142, 20544, 13334,  8154,  2956,\n",
       "             104]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   5,   12,   80,  468, 1886, 4879, 5114, 2817, 1166,  128]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  62, 6016, 8026, 1178,  547,  264,  179,  171,   96,   16]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([3482, 2375, 2707, 2932, 2420, 1299,  781,  431,   91,   37]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    3,    27,   196,  1323,  4645,  8164, 10182, 10558,  4431,\n",
       "             212]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    3,   196,  9492, 27270,  1961,   252,   173,   173,   181,\n",
       "              40]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([7181, 7875, 8102, 8136, 4779, 1943,  805,  483,  333,  104]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([  157,  2162,  7978, 19059, 23747, 26026, 21298,  7391,  1515,\n",
       "             125]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  277, 16652, 62467, 13097,  7532,  6387,  2451,   530,    59,\n",
       "               6]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 5352, 16880, 17754, 18849, 18403, 15728, 10834,  5078,   572,\n",
       "               8]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([ 134, 1131, 2188, 2076, 4275, 7670, 8090, 4260,  959,   75]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1327, 13066, 13104,   718,   787,   710,   537,   385,   190,\n",
       "              34]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([4143, 3618, 3220, 3098, 3077, 3473, 3596, 3768, 2580,  285]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   116,   6677,  50378, 104596, 106168,  46911,   5670,    188,\n",
       "               31,     14]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    28,   1436, 149496, 123797,  26980,  13940,   4155,    812,\n",
       "               92,     13]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([69322, 62885, 50177, 43074, 39512, 30639, 16244,  6926,  1832,\n",
       "             138]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   6,   16,  266, 2043, 5090, 3909, 2147, 1344,  712,  123]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 820, 9117, 3919,  429,  267,  306,  355,  284,  114,   45]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   5,    8,   55,  212, 1029, 5536, 5662, 1900,  964,  285]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([  300,  7436, 13676, 16370, 19961, 15474,  5518,  1269,   245,\n",
       "              57]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 5115, 34337, 19473,  6222,  6336,  5493,  2712,   562,    52,\n",
       "               4]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([12483, 13274, 12081, 12082, 12625,  9372,  5169,  2349,   760,\n",
       "             111]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    22,   6465, 228870, 452780, 494527,  67751,    168,     10,\n",
       "                0,      1]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 12007, 417973, 529217, 203244,  54732,  22946,   7800,   2211,\n",
       "              435,     29]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([356573, 322583, 223907, 128192,  76059,  57938,  45094,  29465,\n",
       "            10072,    711]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([ 134, 1345, 3657, 6077, 5439, 2322,  200,   15,    5,    6]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 918, 8514, 8243,  905,  281,  181,   85,   34,   23,   16]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([4865, 3914, 3256, 2546, 1924, 1428,  684,  395,  178,   10]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   16,   603, 13322, 48706, 42762,  5481,    39,     7,     7,\n",
       "               5]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    9,   302, 22671, 70994, 14574,  1580,   562,   211,    42,\n",
       "               3]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    2,     2,    15,    52,   270, 29835, 38674, 31930,  9745,\n",
       "             423]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([ 469, 2776, 2485, 3004, 2973, 2185, 1030,  330,  229,   50]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 672, 3424, 4121, 4991, 1394,  551,  221,  108,   43,    6]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([4309, 7944, 2063,  627,  226,  142,  100,   81,   29,   10]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  104,  1187,  5990, 17150, 38075, 55876, 41276, 27541,  6400,\n",
       "             241]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 3115, 21175, 93164, 35876, 21662, 11775,  5108,  1616,   305,\n",
       "              44]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   30,   485, 17504, 65019, 47586, 38586, 20194,  3963,   446,\n",
       "              27]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([  321,  4433, 15716, 35351, 35591, 27357, 16751,  8360,  2726,\n",
       "             163]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  285, 40690, 75263, 13216,  5910,  5722,  3891,  1457,   295,\n",
       "              40]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    3,  2453, 31524, 29934, 29270, 26751, 21152,  5344,   320,\n",
       "              18]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  862, 11109, 60825, 77031, 25029,  1779,    17,    12,     7,\n",
       "               6]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   247,  32791, 109524,  19107,   5290,   4956,   3034,   1379,\n",
       "              322,     27]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([27215, 25026, 23473, 21670, 20964, 19820, 15508, 13199,  8585,\n",
       "            1217]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   23,     0,     0,     0,    15,  3056, 40553, 74000, 31625,\n",
       "            1232]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  226, 68687, 61870,  5119,  5804,  5842,  2335,   510,    95,\n",
       "              16]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([15208, 16386, 19476, 18897, 15439, 15189, 17744, 18066, 12762,\n",
       "            1337]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([  139,  1421,  6128, 13936, 26193, 29649, 24223, 18547, 11002,\n",
       "            1380]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  207, 13206, 85746, 16668,  8592,  4950,  2468,   710,    68,\n",
       "               3]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   73,  7242, 20292, 24884, 19578, 13545, 15026, 16186, 11759,\n",
       "            4033]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([ 3375, 19484, 15415,  9290,  2870,  1970,  1459,   638,    51,\n",
       "              12]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   38, 10074, 15565,  9661,  5465,  4955,  4922,  2923,   868,\n",
       "              93]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   25, 12239, 23145, 12344,  5057,  1128,   356,   206,    52,\n",
       "              12]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   14,  1181, 14419, 44771, 74028, 32192,  3995,   668,   341,\n",
       "              59]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  102,  1615, 34102, 48395, 60521, 19441,  5743,  1538,   192,\n",
       "              19]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   30, 34161, 43366, 34150, 27131, 18802, 10783,  3065,   152,\n",
       "              28]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    5,    25,   235,  6847, 39657, 62088, 43535,  7669,    67,\n",
       "               3]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1087, 24752, 83680, 32653,  9426,  5022,  2497,   837,   152,\n",
       "              25]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   15,    63,   234, 45980, 43927, 30427, 21025, 13211,  4580,\n",
       "             669]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   1,    8,   38,  276, 1251, 3452, 3999, 3837, 2561,  158]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 108, 4053, 7297, 2297, 1208,  464,  120,   26,    5,    3]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   2,    6, 5403, 5748, 2857, 1075,  318,  102,   44,   26]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([ 6570, 28139, 19041,    70,     8,     0,     0,     6,     4,\n",
       "               8]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    2,   265,  3481, 16533, 16670,  9492,  5811,  1470,   113,\n",
       "               9]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    9,   177,  1545, 14098, 21667,  7871,  3773,  2471,  1685,\n",
       "             550]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    5,   384,  9061, 38945, 27226,  1921,    48,    17,     8,\n",
       "               6]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 2951, 18587, 37338, 12230,  4269,  1423,   564,   204,    48,\n",
       "               7]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([14274, 13107, 12553, 11779,  9823,  8930,  5321,  1471,   330,\n",
       "              33]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    14,    402,  16812,  86454, 105266, 112674,  33768,  10786,\n",
       "             4199,    115]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([     3,      5,    171,  10058, 133514, 145810,  59953,  17630,\n",
       "             2912,    434]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([147474,  67061,  51348,  42293,  28449,  15901,  10262,   4553,\n",
       "             2506,    643]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    8,    75,  1675,  5740, 13083, 24707, 13879,  2196,   176,\n",
       "               4]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  281, 21672, 25959,  3155,  2856,  3588,  2973,   938,   111,\n",
       "              10]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    5,   818,  8595, 10406, 12361, 14275, 10202,  3951,   773,\n",
       "             157]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([  19,  405, 2809, 6247, 7543, 6272, 4633, 2885,  988,   44]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  254,  2353,  3543, 13099,  7126,  2636,  1734,   877,   210,\n",
       "              13]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([9723, 6865, 4884, 3831, 2879, 1936, 1202,  408,   89,   28]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([  292, 15663, 67540, 32804,  4248,  1185,   705,   289,    77,\n",
       "              12]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1772, 25395, 37022,  4907,  6406, 18702, 20455,  5676,  2306,\n",
       "             174]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  136, 52028, 41398, 19902,  8916,   348,    50,    20,    11,\n",
       "               6]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   49,   235,   937,  5393, 25664, 34176, 15859,  2915,    89,\n",
       "               6]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   12,   178,  2579, 26355, 30475, 10153,  5798,  6687,  2947,\n",
       "             139]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   39,   222,  6465, 10935, 12422, 18314, 22653, 12363,  1842,\n",
       "              68]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   44,  1162,  9655, 24555, 24901, 16122,  8670,  2066,   347,\n",
       "              44]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1753, 38105, 29445,  5490,  3393,  3942,  3329,  1507,   541,\n",
       "              61]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([11159, 10992, 12490, 14342, 14461, 11524,  8202,  3407,   891,\n",
       "              98]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  495,  6809, 14055, 12173,  6186,  3339,   336,    29,    14,\n",
       "               7]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  196,  4072, 19624, 13271,  2525,  1486,  1366,   699,   176,\n",
       "              28]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([4365, 4870, 5413, 5484, 4614, 5376, 5760, 4732, 2568,  261]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   12,    77,   725,  7451, 24244, 28552, 14453,  2270,   145,\n",
       "               2]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  325, 11584, 26328, 14635, 12553,  9110,  2860,   456,    73,\n",
       "               7]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 8412, 10182, 12772, 14823, 13900,  8565,  5039,  2839,  1210,\n",
       "             189]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  311,  3582, 11717, 28230, 41119, 33409, 20872,  9217,  2807,\n",
       "             250]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1258, 64586, 47649, 10354, 11407,  9810,  4735,  1364,   326,\n",
       "              25]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   39,   535,  4478, 23074, 37416, 39461, 24435, 14029,  7016,\n",
       "            1031]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([  97,  879, 5173, 6801, 5231, 4441, 2829, 1543, 1125,  481]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   49,   353,  6043, 11889,  3635,  2871,  2271,  1152,   301,\n",
       "              36]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([5102, 4802, 6375, 4316, 1835, 1460, 1220, 1231, 1434,  825]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([     2,     53,   1196,  30639, 238622, 233430,  39762,   3198,\n",
       "              218,     26]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 10917, 276687, 124708,  44625,  45223,  32530,  10123,   2065,\n",
       "              248,     20]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   111,  39055, 113351, 103791, 100409, 105241,  77382,   7679,\n",
       "              120,      7]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([  173,  1970, 12816, 35032, 28044, 19285, 18832, 13101,  4397,\n",
       "             138]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 5784, 55115, 51509, 16142,  3137,  1191,   516,   278,   104,\n",
       "              12]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  143,     0,     0,    84,  7712, 28446, 27548, 28029, 33671,\n",
       "            8155]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   44,   441,  3182, 11373, 22111, 25384, 23125, 20289,  8506,\n",
       "             815]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  148, 14509, 90292,  6517,  1296,  1034,   773,   531,   154,\n",
       "              16]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([20824, 17320, 15897, 14472, 13778, 11839,  9673,  7379,  3737,\n",
       "             351]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  190,  1616,  5896, 18712, 37009, 42945, 24434,  8022,   889,\n",
       "              42]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  915, 47616, 76621,  8178,  3824,  1475,   685,   288,   116,\n",
       "              37]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   16,   173,  1043, 18529, 24913, 25450, 22788, 21117, 22048,\n",
       "            3678]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   20,   263,  6281, 31697, 49526, 44416, 28838, 14752,  5475,\n",
       "             465]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 3313, 65356, 82552, 15225,  5497,  4786,  2622,  1592,   722,\n",
       "              68]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([21811, 23947, 24704, 22171, 18974, 21020, 22804, 20720,  5442,\n",
       "             140]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    22,    159,   4137,  88346, 192315,  85481,   8136,    161,\n",
       "               25,      5]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  1849, 154305, 121792,  15088,  21960,  28838,  25498,   8088,\n",
       "             1275,     84]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([66268, 77568, 73887, 58944, 36921, 28332, 21847, 12312,  2498,\n",
       "             210]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([ 1571,  8527,  6076,  7480,  8604, 10822, 13555, 18244,  4191,\n",
       "             128]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 3087,  8879,  6749, 18428, 33170,  6853,  1739,   240,    44,\n",
       "               9]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   15, 21475, 17992, 18194, 11628,  4252,  2220,  1538,  1501,\n",
       "             383]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   10,    68,  1109,  7716, 29783, 51184, 50602, 31347,  8961,\n",
       "             563]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   411,  35198, 124550,  17039,   1984,   1096,    524,    271,\n",
       "              186,     84]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   13,   149, 35232, 41129, 31640, 27298, 21593, 15038,  8601,\n",
       "             650]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([ 3483, 18027, 14932,   465,   319,   238,    79,    30,    14,\n",
       "               8]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  128,  1442, 11760, 16537,  4422,  2098,   788,   335,    69,\n",
       "              16]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([19350, 11765,  5157,   921,   270,    80,    37,     7,     6,\n",
       "               2]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   23,   925, 14797, 48330, 67772, 66704, 41924, 22569,  4262,\n",
       "             148]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  2072,  21620, 122262,  78562,  24279,  12569,   5092,    929,\n",
       "               58,     11]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([44082, 56421, 48471, 27301, 21221, 21928, 22045, 19022,  6663,\n",
       "             300]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([  151,  1849,  4434,  6080, 17906, 25621, 19936, 15318,  7400,\n",
       "             862]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  333, 26450, 59344, 10450,   556,   533,   598,   713,   509,\n",
       "              71]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([11528, 10729, 10805, 10428, 11137, 13426, 16787, 10800,  3759,\n",
       "             158]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([    5,    20,    43,   211,  3997, 27323, 41327, 49084, 41206,\n",
       "            1053]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   42,   294, 14576, 93932, 32072, 15114,  5965,  1887,   277,\n",
       "             110]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([     8,     21,     52,    156,   1820,   3297, 112779,  40582,\n",
       "             5177,    377]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  612, 11180, 37154, 60449, 40759,  9928,   247,    39,    14,\n",
       "               2]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 2372, 56283, 82525,  7516,  5993,  4177,  1262,   230,    23,\n",
       "               3]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([31439, 25671, 21976, 18417, 15250, 13515, 13630, 12482,  7252,\n",
       "             752]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   10,   103,  1126,  5624, 13045, 17149, 12075,  5348,   857,\n",
       "              17]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  331, 12198, 30943,  5373,  3059,  1905,  1040,   384,   106,\n",
       "              15]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([11575, 12176, 11689,  8862,  6024,  3357,  1288,   309,    63,\n",
       "              11]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([  167,  2167,  7238, 13358, 21930, 28764, 24112, 11149,  2969,\n",
       "             215]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1115, 27927, 59712, 12373,  3237,  3116,  2576,  1443,   482,\n",
       "              88]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   14,    37,   205,  1346, 19097, 19656, 19893, 22090, 23092,\n",
       "            6639]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    7,    68,   784,  4561, 16460, 42463, 46746, 33341, 11867,\n",
       "            1111]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   17,  2047, 65771, 66477, 12175,  5499,  3189,  1725,   458,\n",
       "              50]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([22788, 20066, 21014, 16349, 13892, 14142, 17237, 21809,  9185,\n",
       "             926]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    25,    205,   4314,  67911, 207152,  98226,   2488,    380,\n",
       "              150,     38]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   371,  60090, 230667,  48656,  12519,  11313,  10038,   5565,\n",
       "             1533,    137]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   53,   394, 14382, 69575, 72135, 63727, 63514, 66016, 29104,\n",
       "            1989]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   18,   317, 14605, 68704, 59329, 13348,   478,     5,     4,\n",
       "               1]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  156, 26996, 83144, 22753,  8094,  6956,  4752,  2750,  1056,\n",
       "             152]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([27717, 23446, 22163, 19425, 17331, 17151, 14638, 12431,  2463,\n",
       "              44]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   10,   104,   375,  1192,  6447, 17239,  6248,  1801,   541,\n",
       "             144]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  194,  1273, 13000, 10077,  5890,  2477,   819,   305,    54,\n",
       "              12]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([13906, 12841,  5259,  1002,   378,   234,   174,   133,   130,\n",
       "              44]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   17,    56,   236,  2021, 18950, 79961, 45985,  1090,   103,\n",
       "              15]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([   72,   441,  4275, 47715, 84172,  7306,  2527,  1458,   439,\n",
       "              29]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([    3,    17, 13820, 68193, 42093, 15159,  6240,  2303,   548,\n",
       "              58]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([   9,   42,  486, 2463, 4237, 5530, 2913, 1043,  356,   63]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 212, 7616, 4765, 2142, 1474,  493,  208,  169,   52,   11]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  70, 1807, 4602, 4062, 2881, 2022, 1143,  422,  109,   24]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([ 1025, 11126, 25195, 16330,  7811,  1874,    61,    22,    10,\n",
       "               4]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 5822, 38106, 17425,   882,   380,   314,   270,   159,    77,\n",
       "              23]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([8503, 7279, 7085, 7615, 8085, 7269, 7296, 7208, 2930,  188]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    8,    58,  1752, 10142, 30687, 52699, 43525, 16585,  3338,\n",
       "             194]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  741, 55068, 80332,  7451,  3387,  4241,  3897,  2655,  1100,\n",
       "             116]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  281,  6861, 18270, 21769, 22618, 23522, 24979, 28131, 12139,\n",
       "             418]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  1.0),\n",
       " ([(array([    2,    20,   231,  1945,  6737, 13181, 26545, 29672, 10351,\n",
       "             364]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  221,  2833, 45487, 30934,  2796,  2391,  2142,  1554,   623,\n",
       "              67]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([17690, 13604, 11789, 11428,  9578,  7890,  7790,  6090,  2535,\n",
       "             654]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0),\n",
       " ([(array([   24,   370, 36655, 76116, 40005, 11107,  6329,  1936,   232,\n",
       "              23]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([ 1930, 38765, 91165, 24911, 10478,  3709,  1248,   438,   138,\n",
       "              15]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       "   (array([  168, 11080, 66973, 50489, 30306,  9602,  2934,  1041,   193,\n",
       "              11]),\n",
       "    array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))],\n",
       "  0.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  221,  8721, 31528, 32102,  5993,   528,    11,    14,    13,\n",
       "             8]),\n",
       "  array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       " (array([  171, 26534, 37820,  4727,  2677,  2475,  2575,  1731,   403,\n",
       "            26]),\n",
       "  array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])),\n",
       " (array([   45,   159,   487, 12526, 12971, 18499, 22060, 11535,   766,\n",
       "            91]),\n",
       "  array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms = []\n",
    "y = []\n",
    "for item in dataset:\n",
    "    h = []\n",
    "    for histogram in item[0]:\n",
    "        h.extend(histogram[0])\n",
    "    histograms.append(h)\n",
    "    y.append(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(histograms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.6153846153846154\n",
      "1 : 0.4230769230769231\n",
      "2 : 0.46153846153846156\n",
      "3 : 0.5384615384615384\n",
      "4 : 0.6153846153846154\n",
      "5 : 0.5\n",
      "6 : 0.5\n",
      "7 : 0.5\n",
      "8 : 0.5384615384615384\n",
      "9 : 0.6153846153846154\n",
      "10 : 0.46153846153846156\n",
      "11 : 0.6153846153846154\n",
      "12 : 0.38461538461538464\n",
      "13 : 0.5\n",
      "14 : 0.5769230769230769\n",
      "15 : 0.5769230769230769\n",
      "16 : 0.5769230769230769\n",
      "17 : 0.5769230769230769\n",
      "18 : 0.5769230769230769\n",
      "19 : 0.5384615384615384\n",
      "20 : 0.4230769230769231\n",
      "21 : 0.46153846153846156\n",
      "22 : 0.5384615384615384\n",
      "23 : 0.5\n",
      "24 : 0.5769230769230769\n",
      "25 : 0.38461538461538464\n",
      "26 : 0.5\n",
      "27 : 0.5\n",
      "28 : 0.5384615384615384\n",
      "29 : 0.4230769230769231\n",
      "30 : 0.7307692307692307\n",
      "31 : 0.5384615384615384\n",
      "32 : 0.5\n",
      "33 : 0.4230769230769231\n",
      "34 : 0.4230769230769231\n",
      "35 : 0.46153846153846156\n",
      "36 : 0.5\n",
      "37 : 0.5384615384615384\n",
      "38 : 0.5384615384615384\n",
      "39 : 0.38461538461538464\n",
      "40 : 0.5769230769230769\n",
      "41 : 0.5384615384615384\n",
      "42 : 0.5384615384615384\n",
      "43 : 0.5384615384615384\n",
      "44 : 0.6538461538461539\n",
      "45 : 0.5\n",
      "46 : 0.5384615384615384\n",
      "47 : 0.5384615384615384\n",
      "48 : 0.6153846153846154\n",
      "49 : 0.5769230769230769\n",
      "0.5238461538461539 0.07170369668533969\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scores = []\n",
    "\n",
    "for random_state in range(50):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(histograms, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "    clf = svm.SVC()\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    score = clf.score(X_test, y_test)\n",
    "    \n",
    "    scores.append(score)\n",
    "\n",
    "    print(random_state, \":\", score)\n",
    "    \n",
    "scores = np.array(scores)\n",
    "\n",
    "print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "h = np.array(histograms)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.7302965022981943, 10.193821336724634, 0.0, 1.0)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X), np.max(X), np.mean(X), np.std(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 104 samples, validate on 26 samples\n",
      "Epoch 1/1000\n",
      "104/104 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.4904 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 2/1000\n",
      "104/104 [==============================] - 0s 984us/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 3/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.4615 - val_loss: 0.6930 - val_accuracy: 0.5385\n",
      "Epoch 4/1000\n",
      "104/104 [==============================] - 0s 987us/step - loss: 0.6934 - accuracy: 0.4327 - val_loss: 0.6929 - val_accuracy: 0.5385\n",
      "Epoch 5/1000\n",
      "104/104 [==============================] - 0s 992us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6928 - val_accuracy: 0.5385\n",
      "Epoch 6/1000\n",
      "104/104 [==============================] - 0s 978us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6927 - val_accuracy: 0.5385\n",
      "Epoch 7/1000\n",
      "104/104 [==============================] - 0s 952us/step - loss: 0.6935 - accuracy: 0.5096 - val_loss: 0.6927 - val_accuracy: 0.5385\n",
      "Epoch 8/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6926 - val_accuracy: 0.5385\n",
      "Epoch 9/1000\n",
      "104/104 [==============================] - 0s 939us/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6925 - val_accuracy: 0.5385\n",
      "Epoch 10/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6935 - accuracy: 0.5096 - val_loss: 0.6925 - val_accuracy: 0.5385\n",
      "Epoch 11/1000\n",
      "104/104 [==============================] - 0s 977us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6925 - val_accuracy: 0.5385\n",
      "Epoch 12/1000\n",
      "104/104 [==============================] - 0s 957us/step - loss: 0.6935 - accuracy: 0.5096 - val_loss: 0.6924 - val_accuracy: 0.5385\n",
      "Epoch 13/1000\n",
      "104/104 [==============================] - 0s 877us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6924 - val_accuracy: 0.5385\n",
      "Epoch 14/1000\n",
      "104/104 [==============================] - 0s 803us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6924 - val_accuracy: 0.5385\n",
      "Epoch 15/1000\n",
      "104/104 [==============================] - 0s 968us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6923 - val_accuracy: 0.5385\n",
      "Epoch 16/1000\n",
      "104/104 [==============================] - 0s 833us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6923 - val_accuracy: 0.5385\n",
      "Epoch 17/1000\n",
      "104/104 [==============================] - 0s 953us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6923 - val_accuracy: 0.5385\n",
      "Epoch 18/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6923 - val_accuracy: 0.5385\n",
      "Epoch 19/1000\n",
      "104/104 [==============================] - 0s 897us/step - loss: 0.6935 - accuracy: 0.5096 - val_loss: 0.6922 - val_accuracy: 0.5385\n",
      "Epoch 20/1000\n",
      "104/104 [==============================] - 0s 994us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6922 - val_accuracy: 0.5385\n",
      "Epoch 21/1000\n",
      "104/104 [==============================] - 0s 960us/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6922 - val_accuracy: 0.5385\n",
      "Epoch 22/1000\n",
      "104/104 [==============================] - 0s 993us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6921 - val_accuracy: 0.5385\n",
      "Epoch 23/1000\n",
      "104/104 [==============================] - 0s 910us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6921 - val_accuracy: 0.5385\n",
      "Epoch 24/1000\n",
      "104/104 [==============================] - 0s 896us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6921 - val_accuracy: 0.5385\n",
      "Epoch 25/1000\n",
      "104/104 [==============================] - 0s 941us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6921 - val_accuracy: 0.5385\n",
      "Epoch 26/1000\n",
      "104/104 [==============================] - 0s 996us/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6921 - val_accuracy: 0.5385\n",
      "Epoch 27/1000\n",
      "104/104 [==============================] - 0s 948us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6920 - val_accuracy: 0.5385\n",
      "Epoch 28/1000\n",
      "104/104 [==============================] - 0s 991us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6920 - val_accuracy: 0.5385\n",
      "Epoch 29/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6920 - val_accuracy: 0.5385\n",
      "Epoch 30/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6920 - val_accuracy: 0.5385\n",
      "Epoch 31/1000\n",
      "104/104 [==============================] - 0s 979us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6920 - val_accuracy: 0.5385\n",
      "Epoch 32/1000\n",
      "104/104 [==============================] - 0s 998us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6920 - val_accuracy: 0.5385\n",
      "Epoch 33/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6920 - val_accuracy: 0.5385\n",
      "Epoch 34/1000\n",
      "104/104 [==============================] - 0s 946us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 35/1000\n",
      "104/104 [==============================] - 0s 932us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 36/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 37/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 38/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 39/1000\n",
      "104/104 [==============================] - 0s 998us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 40/1000\n",
      "104/104 [==============================] - 0s 944us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 41/1000\n",
      "104/104 [==============================] - 0s 903us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 42/1000\n",
      "104/104 [==============================] - 0s 913us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 43/1000\n",
      "104/104 [==============================] - 0s 963us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 44/1000\n",
      "104/104 [==============================] - 0s 889us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 45/1000\n",
      "104/104 [==============================] - 0s 899us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 46/1000\n",
      "104/104 [==============================] - 0s 834us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 47/1000\n",
      "104/104 [==============================] - 0s 874us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 48/1000\n",
      "104/104 [==============================] - 0s 906us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 49/1000\n",
      "104/104 [==============================] - 0s 951us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 50/1000\n",
      "104/104 [==============================] - 0s 921us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 51/1000\n",
      "104/104 [==============================] - 0s 955us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 52/1000\n",
      "104/104 [==============================] - 0s 972us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 53/1000\n",
      "104/104 [==============================] - 0s 917us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 54/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 55/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 56/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 860us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 57/1000\n",
      "104/104 [==============================] - 0s 992us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 58/1000\n",
      "104/104 [==============================] - 0s 978us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5385\n",
      "Epoch 59/1000\n",
      "104/104 [==============================] - 0s 926us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 60/1000\n",
      "104/104 [==============================] - 0s 925us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 61/1000\n",
      "104/104 [==============================] - 0s 944us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 62/1000\n",
      "104/104 [==============================] - 0s 960us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 63/1000\n",
      "104/104 [==============================] - 0s 968us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 64/1000\n",
      "104/104 [==============================] - 0s 893us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 65/1000\n",
      "104/104 [==============================] - 0s 887us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 66/1000\n",
      "104/104 [==============================] - 0s 968us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 67/1000\n",
      "104/104 [==============================] - 0s 970us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 68/1000\n",
      "104/104 [==============================] - 0s 946us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 69/1000\n",
      "104/104 [==============================] - 0s 954us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 70/1000\n",
      "104/104 [==============================] - 0s 923us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 71/1000\n",
      "104/104 [==============================] - 0s 938us/step - loss: 0.6935 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 72/1000\n",
      "104/104 [==============================] - 0s 966us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 73/1000\n",
      "104/104 [==============================] - 0s 966us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 74/1000\n",
      "104/104 [==============================] - 0s 988us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 75/1000\n",
      "104/104 [==============================] - 0s 990us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 76/1000\n",
      "104/104 [==============================] - 0s 989us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 77/1000\n",
      "104/104 [==============================] - 0s 979us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 78/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6918 - val_accuracy: 0.5385\n",
      "Epoch 79/1000\n",
      "104/104 [==============================] - 0s 958us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 80/1000\n",
      "104/104 [==============================] - 0s 912us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 81/1000\n",
      "104/104 [==============================] - 0s 957us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 82/1000\n",
      "104/104 [==============================] - 0s 883us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 83/1000\n",
      "104/104 [==============================] - 0s 944us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 84/1000\n",
      "104/104 [==============================] - 0s 923us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 85/1000\n",
      "104/104 [==============================] - 0s 911us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 86/1000\n",
      "104/104 [==============================] - 0s 889us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 87/1000\n",
      "104/104 [==============================] - 0s 868us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 88/1000\n",
      "104/104 [==============================] - 0s 886us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 89/1000\n",
      "104/104 [==============================] - 0s 786us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 90/1000\n",
      "104/104 [==============================] - 0s 887us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 91/1000\n",
      "104/104 [==============================] - 0s 979us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 92/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 93/1000\n",
      "104/104 [==============================] - 0s 948us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 94/1000\n",
      "104/104 [==============================] - 0s 974us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 95/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 96/1000\n",
      "104/104 [==============================] - 0s 913us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 97/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 98/1000\n",
      "104/104 [==============================] - 0s 922us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 99/1000\n",
      "104/104 [==============================] - 0s 947us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 100/1000\n",
      "104/104 [==============================] - 0s 1000us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 101/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 102/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 103/1000\n",
      "104/104 [==============================] - 0s 944us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 104/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 105/1000\n",
      "104/104 [==============================] - 0s 963us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 106/1000\n",
      "104/104 [==============================] - 0s 964us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 107/1000\n",
      "104/104 [==============================] - 0s 946us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 108/1000\n",
      "104/104 [==============================] - 0s 671us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 109/1000\n",
      "104/104 [==============================] - 0s 805us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 110/1000\n",
      "104/104 [==============================] - 0s 906us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 111/1000\n",
      "104/104 [==============================] - 0s 912us/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/1000\n",
      "104/104 [==============================] - 0s 924us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 113/1000\n",
      "104/104 [==============================] - 0s 859us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 114/1000\n",
      "104/104 [==============================] - 0s 858us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 115/1000\n",
      "104/104 [==============================] - 0s 769us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 116/1000\n",
      "104/104 [==============================] - 0s 808us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 117/1000\n",
      "104/104 [==============================] - 0s 860us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 118/1000\n",
      "104/104 [==============================] - 0s 831us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 119/1000\n",
      "104/104 [==============================] - 0s 831us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 120/1000\n",
      "104/104 [==============================] - 0s 920us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 121/1000\n",
      "104/104 [==============================] - 0s 912us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 122/1000\n",
      "104/104 [==============================] - 0s 842us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 123/1000\n",
      "104/104 [==============================] - 0s 894us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 124/1000\n",
      "104/104 [==============================] - 0s 921us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 125/1000\n",
      "104/104 [==============================] - 0s 870us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 126/1000\n",
      "104/104 [==============================] - 0s 865us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 127/1000\n",
      "104/104 [==============================] - 0s 874us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 128/1000\n",
      "104/104 [==============================] - 0s 891us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 129/1000\n",
      "104/104 [==============================] - 0s 942us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 130/1000\n",
      "104/104 [==============================] - 0s 931us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 131/1000\n",
      "104/104 [==============================] - 0s 903us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 132/1000\n",
      "104/104 [==============================] - 0s 880us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 133/1000\n",
      "104/104 [==============================] - 0s 902us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6917 - val_accuracy: 0.5385\n",
      "Epoch 134/1000\n",
      "104/104 [==============================] - 0s 922us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 135/1000\n",
      "104/104 [==============================] - 0s 942us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 136/1000\n",
      "104/104 [==============================] - 0s 945us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 137/1000\n",
      "104/104 [==============================] - 0s 973us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 138/1000\n",
      "104/104 [==============================] - 0s 955us/step - loss: 0.6935 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 139/1000\n",
      "104/104 [==============================] - 0s 964us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 140/1000\n",
      "104/104 [==============================] - 0s 952us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 141/1000\n",
      "104/104 [==============================] - 0s 921us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 142/1000\n",
      "104/104 [==============================] - 0s 960us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 143/1000\n",
      "104/104 [==============================] - 0s 966us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 144/1000\n",
      "104/104 [==============================] - 0s 913us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 145/1000\n",
      "104/104 [==============================] - 0s 976us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 146/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 147/1000\n",
      "104/104 [==============================] - 0s 933us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 148/1000\n",
      "104/104 [==============================] - 0s 938us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 149/1000\n",
      "104/104 [==============================] - 0s 981us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 150/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 151/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 152/1000\n",
      "104/104 [==============================] - 0s 996us/step - loss: 0.6926 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 153/1000\n",
      "104/104 [==============================] - 0s 938us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 154/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 155/1000\n",
      "104/104 [==============================] - 0s 991us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 156/1000\n",
      "104/104 [==============================] - 0s 968us/step - loss: 0.6936 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 157/1000\n",
      "104/104 [==============================] - 0s 948us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 158/1000\n",
      "104/104 [==============================] - 0s 967us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 159/1000\n",
      "104/104 [==============================] - 0s 983us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 160/1000\n",
      "104/104 [==============================] - 0s 968us/step - loss: 0.6934 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 161/1000\n",
      "104/104 [==============================] - 0s 982us/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 162/1000\n",
      "104/104 [==============================] - 0s 941us/step - loss: 0.6925 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 163/1000\n",
      "104/104 [==============================] - 0s 903us/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 164/1000\n",
      "104/104 [==============================] - 0s 884us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 165/1000\n",
      "104/104 [==============================] - 0s 972us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 166/1000\n",
      "104/104 [==============================] - 0s 979us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 167/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 168/1000\n",
      "104/104 [==============================] - 0s 865us/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 169/1000\n",
      "104/104 [==============================] - 0s 893us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 170/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 171/1000\n",
      "104/104 [==============================] - 0s 954us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5385\n",
      "Epoch 172/1000\n",
      "104/104 [==============================] - 0s 907us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 173/1000\n",
      "104/104 [==============================] - 0s 808us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 174/1000\n",
      "104/104 [==============================] - 0s 932us/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 175/1000\n",
      "104/104 [==============================] - 0s 908us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 176/1000\n",
      "104/104 [==============================] - 0s 858us/step - loss: 0.6925 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 177/1000\n",
      "104/104 [==============================] - 0s 956us/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 178/1000\n",
      "104/104 [==============================] - 0s 943us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 179/1000\n",
      "104/104 [==============================] - 0s 981us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 180/1000\n",
      "104/104 [==============================] - 0s 957us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 181/1000\n",
      "104/104 [==============================] - 0s 909us/step - loss: 0.6935 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 182/1000\n",
      "104/104 [==============================] - 0s 945us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 183/1000\n",
      "104/104 [==============================] - 0s 896us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 184/1000\n",
      "104/104 [==============================] - 0s 907us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 185/1000\n",
      "104/104 [==============================] - 0s 899us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 186/1000\n",
      "104/104 [==============================] - 0s 906us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 187/1000\n",
      "104/104 [==============================] - 0s 982us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 188/1000\n",
      "104/104 [==============================] - 0s 982us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 189/1000\n",
      "104/104 [==============================] - 0s 969us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 190/1000\n",
      "104/104 [==============================] - 0s 911us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 191/1000\n",
      "104/104 [==============================] - 0s 927us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 192/1000\n",
      "104/104 [==============================] - 0s 819us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 193/1000\n",
      "104/104 [==============================] - 0s 879us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6915 - val_accuracy: 0.5385\n",
      "Epoch 194/1000\n",
      "104/104 [==============================] - 0s 937us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 195/1000\n",
      "104/104 [==============================] - 0s 905us/step - loss: 0.6926 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 196/1000\n",
      "104/104 [==============================] - 0s 988us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 197/1000\n",
      "104/104 [==============================] - 0s 995us/step - loss: 0.6926 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 198/1000\n",
      "104/104 [==============================] - 0s 948us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 199/1000\n",
      "104/104 [==============================] - 0s 962us/step - loss: 0.6923 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 200/1000\n",
      "104/104 [==============================] - 0s 984us/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 201/1000\n",
      "104/104 [==============================] - 0s 934us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 202/1000\n",
      "104/104 [==============================] - 0s 953us/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 203/1000\n",
      "104/104 [==============================] - 0s 876us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 204/1000\n",
      "104/104 [==============================] - 0s 925us/step - loss: 0.6925 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 205/1000\n",
      "104/104 [==============================] - 0s 960us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 206/1000\n",
      "104/104 [==============================] - 0s 941us/step - loss: 0.6924 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 207/1000\n",
      "104/104 [==============================] - 0s 675us/step - loss: 0.6926 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 208/1000\n",
      "104/104 [==============================] - 0s 984us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 209/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 210/1000\n",
      "104/104 [==============================] - 0s 729us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 211/1000\n",
      "104/104 [==============================] - 0s 612us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 212/1000\n",
      "104/104 [==============================] - 0s 620us/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 213/1000\n",
      "104/104 [==============================] - 0s 646us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 214/1000\n",
      "104/104 [==============================] - 0s 711us/step - loss: 0.6925 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 215/1000\n",
      "104/104 [==============================] - 0s 930us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 216/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6926 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 217/1000\n",
      "104/104 [==============================] - 0s 999us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 218/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 219/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6924 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 220/1000\n",
      "104/104 [==============================] - 0s 887us/step - loss: 0.6930 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 221/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 222/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 992us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 223/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6925 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 224/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6914 - val_accuracy: 0.5385\n",
      "Epoch 225/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 226/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6925 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 227/1000\n",
      "104/104 [==============================] - 0s 974us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 228/1000\n",
      "104/104 [==============================] - 0s 994us/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 229/1000\n",
      "104/104 [==============================] - 0s 971us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 230/1000\n",
      "104/104 [==============================] - 0s 935us/step - loss: 0.6926 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 231/1000\n",
      "104/104 [==============================] - 0s 988us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 232/1000\n",
      "104/104 [==============================] - 0s 951us/step - loss: 0.6923 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 233/1000\n",
      "104/104 [==============================] - 0s 977us/step - loss: 0.6924 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 234/1000\n",
      "104/104 [==============================] - 0s 1000us/step - loss: 0.6935 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 235/1000\n",
      "104/104 [==============================] - 0s 961us/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 236/1000\n",
      "104/104 [==============================] - 0s 978us/step - loss: 0.6925 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 237/1000\n",
      "104/104 [==============================] - 0s 955us/step - loss: 0.6926 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 238/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6926 - accuracy: 0.5096 - val_loss: 0.6913 - val_accuracy: 0.5385\n",
      "Epoch 239/1000\n",
      "104/104 [==============================] - 0s 998us/step - loss: 0.6925 - accuracy: 0.5096 - val_loss: 0.6912 - val_accuracy: 0.5385\n",
      "Epoch 240/1000\n",
      "104/104 [==============================] - 0s 997us/step - loss: 0.6914 - accuracy: 0.5096 - val_loss: 0.6912 - val_accuracy: 0.5385\n",
      "Epoch 241/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6912 - val_accuracy: 0.5385\n",
      "Epoch 242/1000\n",
      "104/104 [==============================] - 0s 910us/step - loss: 0.6922 - accuracy: 0.5096 - val_loss: 0.6912 - val_accuracy: 0.5385\n",
      "Epoch 243/1000\n",
      "104/104 [==============================] - 0s 888us/step - loss: 0.6922 - accuracy: 0.5096 - val_loss: 0.6912 - val_accuracy: 0.5385\n",
      "Epoch 244/1000\n",
      "104/104 [==============================] - 0s 819us/step - loss: 0.6925 - accuracy: 0.5096 - val_loss: 0.6912 - val_accuracy: 0.5385\n",
      "Epoch 245/1000\n",
      "104/104 [==============================] - 0s 952us/step - loss: 0.6919 - accuracy: 0.5096 - val_loss: 0.6912 - val_accuracy: 0.5385\n",
      "Epoch 246/1000\n",
      "104/104 [==============================] - 0s 918us/step - loss: 0.6924 - accuracy: 0.5096 - val_loss: 0.6912 - val_accuracy: 0.5385\n",
      "Epoch 247/1000\n",
      "104/104 [==============================] - 0s 889us/step - loss: 0.6916 - accuracy: 0.5096 - val_loss: 0.6912 - val_accuracy: 0.5385\n",
      "Epoch 248/1000\n",
      "104/104 [==============================] - 0s 924us/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6912 - val_accuracy: 0.5385\n",
      "Epoch 249/1000\n",
      "104/104 [==============================] - 0s 919us/step - loss: 0.6922 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 250/1000\n",
      "104/104 [==============================] - 0s 959us/step - loss: 0.6928 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 251/1000\n",
      "104/104 [==============================] - 0s 923us/step - loss: 0.6923 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 252/1000\n",
      "104/104 [==============================] - 0s 840us/step - loss: 0.6920 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 253/1000\n",
      "104/104 [==============================] - 0s 880us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 254/1000\n",
      "104/104 [==============================] - 0s 893us/step - loss: 0.6921 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 255/1000\n",
      "104/104 [==============================] - 0s 851us/step - loss: 0.6920 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 256/1000\n",
      "104/104 [==============================] - 0s 866us/step - loss: 0.6919 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 257/1000\n",
      "104/104 [==============================] - 0s 922us/step - loss: 0.6916 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 258/1000\n",
      "104/104 [==============================] - 0s 877us/step - loss: 0.6927 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 259/1000\n",
      "104/104 [==============================] - 0s 904us/step - loss: 0.6918 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 260/1000\n",
      "104/104 [==============================] - 0s 922us/step - loss: 0.6926 - accuracy: 0.5096 - val_loss: 0.6910 - val_accuracy: 0.5385\n",
      "Epoch 261/1000\n",
      "104/104 [==============================] - 0s 940us/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 262/1000\n",
      "104/104 [==============================] - 0s 945us/step - loss: 0.6926 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 263/1000\n",
      "104/104 [==============================] - 0s 885us/step - loss: 0.6932 - accuracy: 0.5096 - val_loss: 0.6911 - val_accuracy: 0.5385\n",
      "Epoch 264/1000\n",
      "104/104 [==============================] - 0s 966us/step - loss: 0.6918 - accuracy: 0.5096 - val_loss: 0.6910 - val_accuracy: 0.5385\n",
      "Epoch 265/1000\n",
      "104/104 [==============================] - 0s 941us/step - loss: 0.6916 - accuracy: 0.5096 - val_loss: 0.6910 - val_accuracy: 0.5385\n",
      "Epoch 266/1000\n",
      "104/104 [==============================] - 0s 947us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6910 - val_accuracy: 0.5385\n",
      "Epoch 267/1000\n",
      "104/104 [==============================] - 0s 903us/step - loss: 0.6921 - accuracy: 0.5096 - val_loss: 0.6910 - val_accuracy: 0.5385\n",
      "Epoch 268/1000\n",
      "104/104 [==============================] - 0s 818us/step - loss: 0.6922 - accuracy: 0.5096 - val_loss: 0.6910 - val_accuracy: 0.5385\n",
      "Epoch 269/1000\n",
      "104/104 [==============================] - 0s 594us/step - loss: 0.6921 - accuracy: 0.5096 - val_loss: 0.6910 - val_accuracy: 0.5385\n",
      "Epoch 270/1000\n",
      "104/104 [==============================] - 0s 566us/step - loss: 0.6921 - accuracy: 0.5096 - val_loss: 0.6910 - val_accuracy: 0.5385\n",
      "Epoch 271/1000\n",
      "104/104 [==============================] - 0s 689us/step - loss: 0.6917 - accuracy: 0.5096 - val_loss: 0.6910 - val_accuracy: 0.5385\n",
      "Epoch 272/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6919 - accuracy: 0.5096 - val_loss: 0.6910 - val_accuracy: 0.5385\n",
      "Epoch 273/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5096 - val_loss: 0.6910 - val_accuracy: 0.5385\n",
      "Epoch 274/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5096 - val_loss: 0.6909 - val_accuracy: 0.5385\n",
      "Epoch 275/1000\n",
      "104/104 [==============================] - 0s 788us/step - loss: 0.6906 - accuracy: 0.5096 - val_loss: 0.6909 - val_accuracy: 0.5385\n",
      "Epoch 276/1000\n",
      "104/104 [==============================] - 0s 553us/step - loss: 0.6917 - accuracy: 0.5096 - val_loss: 0.6909 - val_accuracy: 0.5385\n",
      "Epoch 277/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 532us/step - loss: 0.6916 - accuracy: 0.5096 - val_loss: 0.6908 - val_accuracy: 0.5385\n",
      "Epoch 278/1000\n",
      "104/104 [==============================] - 0s 517us/step - loss: 0.6923 - accuracy: 0.5096 - val_loss: 0.6908 - val_accuracy: 0.5385\n",
      "Epoch 279/1000\n",
      "104/104 [==============================] - 0s 553us/step - loss: 0.6921 - accuracy: 0.5096 - val_loss: 0.6908 - val_accuracy: 0.5385\n",
      "Epoch 280/1000\n",
      "104/104 [==============================] - 0s 555us/step - loss: 0.6911 - accuracy: 0.5096 - val_loss: 0.6908 - val_accuracy: 0.5385\n",
      "Epoch 281/1000\n",
      "104/104 [==============================] - 0s 555us/step - loss: 0.6902 - accuracy: 0.5096 - val_loss: 0.6908 - val_accuracy: 0.5385\n",
      "Epoch 282/1000\n",
      "104/104 [==============================] - 0s 483us/step - loss: 0.6918 - accuracy: 0.5096 - val_loss: 0.6907 - val_accuracy: 0.5385\n",
      "Epoch 283/1000\n",
      "104/104 [==============================] - 0s 538us/step - loss: 0.6909 - accuracy: 0.5096 - val_loss: 0.6907 - val_accuracy: 0.5385\n",
      "Epoch 284/1000\n",
      "104/104 [==============================] - 0s 532us/step - loss: 0.6915 - accuracy: 0.5096 - val_loss: 0.6907 - val_accuracy: 0.5385\n",
      "Epoch 285/1000\n",
      "104/104 [==============================] - 0s 584us/step - loss: 0.6917 - accuracy: 0.5096 - val_loss: 0.6907 - val_accuracy: 0.5385\n",
      "Epoch 286/1000\n",
      "104/104 [==============================] - 0s 534us/step - loss: 0.6920 - accuracy: 0.5096 - val_loss: 0.6907 - val_accuracy: 0.5385\n",
      "Epoch 287/1000\n",
      "104/104 [==============================] - 0s 540us/step - loss: 0.6914 - accuracy: 0.5096 - val_loss: 0.6907 - val_accuracy: 0.5385\n",
      "Epoch 288/1000\n",
      "104/104 [==============================] - 0s 533us/step - loss: 0.6912 - accuracy: 0.5096 - val_loss: 0.6907 - val_accuracy: 0.5385\n",
      "Epoch 289/1000\n",
      "104/104 [==============================] - 0s 567us/step - loss: 0.6914 - accuracy: 0.5096 - val_loss: 0.6906 - val_accuracy: 0.5385\n",
      "Epoch 290/1000\n",
      "104/104 [==============================] - 0s 593us/step - loss: 0.6897 - accuracy: 0.5096 - val_loss: 0.6906 - val_accuracy: 0.5385\n",
      "Epoch 291/1000\n",
      "104/104 [==============================] - 0s 512us/step - loss: 0.6905 - accuracy: 0.5096 - val_loss: 0.6905 - val_accuracy: 0.5385\n",
      "Epoch 292/1000\n",
      "104/104 [==============================] - 0s 526us/step - loss: 0.6901 - accuracy: 0.5096 - val_loss: 0.6904 - val_accuracy: 0.5385\n",
      "Epoch 293/1000\n",
      "104/104 [==============================] - 0s 546us/step - loss: 0.6923 - accuracy: 0.5096 - val_loss: 0.6904 - val_accuracy: 0.5385\n",
      "Epoch 294/1000\n",
      "104/104 [==============================] - 0s 550us/step - loss: 0.6910 - accuracy: 0.5096 - val_loss: 0.6904 - val_accuracy: 0.5385\n",
      "Epoch 295/1000\n",
      "104/104 [==============================] - 0s 541us/step - loss: 0.6921 - accuracy: 0.5096 - val_loss: 0.6903 - val_accuracy: 0.5385\n",
      "Epoch 296/1000\n",
      "104/104 [==============================] - 0s 553us/step - loss: 0.6913 - accuracy: 0.5096 - val_loss: 0.6903 - val_accuracy: 0.5385\n",
      "Epoch 297/1000\n",
      "104/104 [==============================] - 0s 559us/step - loss: 0.6925 - accuracy: 0.5096 - val_loss: 0.6903 - val_accuracy: 0.5385\n",
      "Epoch 298/1000\n",
      "104/104 [==============================] - 0s 538us/step - loss: 0.6905 - accuracy: 0.5096 - val_loss: 0.6903 - val_accuracy: 0.5385\n",
      "Epoch 299/1000\n",
      "104/104 [==============================] - 0s 516us/step - loss: 0.6907 - accuracy: 0.5096 - val_loss: 0.6902 - val_accuracy: 0.5385\n",
      "Epoch 300/1000\n",
      "104/104 [==============================] - 0s 498us/step - loss: 0.6925 - accuracy: 0.5096 - val_loss: 0.6902 - val_accuracy: 0.5385\n",
      "Epoch 301/1000\n",
      "104/104 [==============================] - 0s 466us/step - loss: 0.6916 - accuracy: 0.5096 - val_loss: 0.6902 - val_accuracy: 0.5385\n",
      "Epoch 302/1000\n",
      "104/104 [==============================] - 0s 473us/step - loss: 0.6904 - accuracy: 0.5096 - val_loss: 0.6902 - val_accuracy: 0.5385\n",
      "Epoch 303/1000\n",
      "104/104 [==============================] - 0s 559us/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6902 - val_accuracy: 0.5385\n",
      "Epoch 304/1000\n",
      "104/104 [==============================] - 0s 530us/step - loss: 0.6915 - accuracy: 0.5096 - val_loss: 0.6901 - val_accuracy: 0.5385\n",
      "Epoch 305/1000\n",
      "104/104 [==============================] - 0s 486us/step - loss: 0.6889 - accuracy: 0.5096 - val_loss: 0.6901 - val_accuracy: 0.5385\n",
      "Epoch 306/1000\n",
      "104/104 [==============================] - 0s 557us/step - loss: 0.6922 - accuracy: 0.5096 - val_loss: 0.6901 - val_accuracy: 0.5385\n",
      "Epoch 307/1000\n",
      "104/104 [==============================] - 0s 568us/step - loss: 0.6919 - accuracy: 0.5096 - val_loss: 0.6901 - val_accuracy: 0.5385\n",
      "Epoch 308/1000\n",
      "104/104 [==============================] - 0s 544us/step - loss: 0.6905 - accuracy: 0.5096 - val_loss: 0.6900 - val_accuracy: 0.5385\n",
      "Epoch 309/1000\n",
      "104/104 [==============================] - 0s 534us/step - loss: 0.6902 - accuracy: 0.5096 - val_loss: 0.6900 - val_accuracy: 0.5385\n",
      "Epoch 310/1000\n",
      "104/104 [==============================] - 0s 488us/step - loss: 0.6917 - accuracy: 0.5096 - val_loss: 0.6899 - val_accuracy: 0.5385\n",
      "Epoch 311/1000\n",
      "104/104 [==============================] - 0s 647us/step - loss: 0.6920 - accuracy: 0.5096 - val_loss: 0.6899 - val_accuracy: 0.5385\n",
      "Epoch 312/1000\n",
      "104/104 [==============================] - 0s 497us/step - loss: 0.6913 - accuracy: 0.5192 - val_loss: 0.6899 - val_accuracy: 0.5385\n",
      "Epoch 313/1000\n",
      "104/104 [==============================] - 0s 536us/step - loss: 0.6904 - accuracy: 0.5096 - val_loss: 0.6899 - val_accuracy: 0.5385\n",
      "Epoch 314/1000\n",
      "104/104 [==============================] - 0s 441us/step - loss: 0.6911 - accuracy: 0.5096 - val_loss: 0.6898 - val_accuracy: 0.5385\n",
      "Epoch 315/1000\n",
      "104/104 [==============================] - 0s 553us/step - loss: 0.6906 - accuracy: 0.5096 - val_loss: 0.6898 - val_accuracy: 0.5385\n",
      "Epoch 316/1000\n",
      "104/104 [==============================] - 0s 502us/step - loss: 0.6911 - accuracy: 0.5096 - val_loss: 0.6897 - val_accuracy: 0.5385\n",
      "Epoch 317/1000\n",
      "104/104 [==============================] - 0s 583us/step - loss: 0.6890 - accuracy: 0.5096 - val_loss: 0.6896 - val_accuracy: 0.5385\n",
      "Epoch 318/1000\n",
      "104/104 [==============================] - 0s 537us/step - loss: 0.6906 - accuracy: 0.5096 - val_loss: 0.6896 - val_accuracy: 0.5385\n",
      "Epoch 319/1000\n",
      "104/104 [==============================] - 0s 512us/step - loss: 0.6898 - accuracy: 0.5096 - val_loss: 0.6895 - val_accuracy: 0.5385\n",
      "Epoch 320/1000\n",
      "104/104 [==============================] - 0s 613us/step - loss: 0.6904 - accuracy: 0.5096 - val_loss: 0.6895 - val_accuracy: 0.5385\n",
      "Epoch 321/1000\n",
      "104/104 [==============================] - 0s 500us/step - loss: 0.6900 - accuracy: 0.5096 - val_loss: 0.6895 - val_accuracy: 0.5385\n",
      "Epoch 322/1000\n",
      "104/104 [==============================] - 0s 511us/step - loss: 0.6896 - accuracy: 0.5096 - val_loss: 0.6894 - val_accuracy: 0.5385\n",
      "Epoch 323/1000\n",
      "104/104 [==============================] - 0s 572us/step - loss: 0.6895 - accuracy: 0.5096 - val_loss: 0.6893 - val_accuracy: 0.5385\n",
      "Epoch 324/1000\n",
      "104/104 [==============================] - 0s 575us/step - loss: 0.6896 - accuracy: 0.5096 - val_loss: 0.6893 - val_accuracy: 0.5385\n",
      "Epoch 325/1000\n",
      "104/104 [==============================] - 0s 617us/step - loss: 0.6898 - accuracy: 0.5096 - val_loss: 0.6892 - val_accuracy: 0.5385\n",
      "Epoch 326/1000\n",
      "104/104 [==============================] - 0s 576us/step - loss: 0.6886 - accuracy: 0.5096 - val_loss: 0.6893 - val_accuracy: 0.5385\n",
      "Epoch 327/1000\n",
      "104/104 [==============================] - 0s 534us/step - loss: 0.6873 - accuracy: 0.5096 - val_loss: 0.6891 - val_accuracy: 0.5385\n",
      "Epoch 328/1000\n",
      "104/104 [==============================] - 0s 565us/step - loss: 0.6897 - accuracy: 0.5096 - val_loss: 0.6890 - val_accuracy: 0.5385\n",
      "Epoch 329/1000\n",
      "104/104 [==============================] - 0s 533us/step - loss: 0.6900 - accuracy: 0.5096 - val_loss: 0.6890 - val_accuracy: 0.5385\n",
      "Epoch 330/1000\n",
      "104/104 [==============================] - 0s 479us/step - loss: 0.6858 - accuracy: 0.5096 - val_loss: 0.6889 - val_accuracy: 0.5385\n",
      "Epoch 331/1000\n",
      "104/104 [==============================] - 0s 541us/step - loss: 0.6887 - accuracy: 0.5096 - val_loss: 0.6889 - val_accuracy: 0.5385\n",
      "Epoch 332/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 576us/step - loss: 0.6853 - accuracy: 0.5192 - val_loss: 0.6887 - val_accuracy: 0.5385\n",
      "Epoch 333/1000\n",
      "104/104 [==============================] - 0s 556us/step - loss: 0.6905 - accuracy: 0.5096 - val_loss: 0.6886 - val_accuracy: 0.5385\n",
      "Epoch 334/1000\n",
      "104/104 [==============================] - 0s 606us/step - loss: 0.6897 - accuracy: 0.5096 - val_loss: 0.6886 - val_accuracy: 0.5385\n",
      "Epoch 335/1000\n",
      "104/104 [==============================] - 0s 588us/step - loss: 0.6867 - accuracy: 0.5096 - val_loss: 0.6886 - val_accuracy: 0.5385\n",
      "Epoch 336/1000\n",
      "104/104 [==============================] - 0s 667us/step - loss: 0.6878 - accuracy: 0.5192 - val_loss: 0.6885 - val_accuracy: 0.5385\n",
      "Epoch 337/1000\n",
      "104/104 [==============================] - 0s 559us/step - loss: 0.6895 - accuracy: 0.5096 - val_loss: 0.6885 - val_accuracy: 0.5385\n",
      "Epoch 338/1000\n",
      "104/104 [==============================] - 0s 545us/step - loss: 0.6876 - accuracy: 0.5096 - val_loss: 0.6884 - val_accuracy: 0.5385\n",
      "Epoch 339/1000\n",
      "104/104 [==============================] - 0s 570us/step - loss: 0.6902 - accuracy: 0.5096 - val_loss: 0.6884 - val_accuracy: 0.5385\n",
      "Epoch 340/1000\n",
      "104/104 [==============================] - 0s 526us/step - loss: 0.6875 - accuracy: 0.5000 - val_loss: 0.6882 - val_accuracy: 0.5385\n",
      "Epoch 341/1000\n",
      "104/104 [==============================] - 0s 577us/step - loss: 0.6850 - accuracy: 0.5000 - val_loss: 0.6880 - val_accuracy: 0.5385\n",
      "Epoch 342/1000\n",
      "104/104 [==============================] - 0s 571us/step - loss: 0.6873 - accuracy: 0.5096 - val_loss: 0.6879 - val_accuracy: 0.5385\n",
      "Epoch 343/1000\n",
      "104/104 [==============================] - 0s 595us/step - loss: 0.6924 - accuracy: 0.5096 - val_loss: 0.6879 - val_accuracy: 0.5385\n",
      "Epoch 344/1000\n",
      "104/104 [==============================] - 0s 528us/step - loss: 0.6908 - accuracy: 0.5096 - val_loss: 0.6880 - val_accuracy: 0.5385\n",
      "Epoch 345/1000\n",
      "104/104 [==============================] - 0s 507us/step - loss: 0.6863 - accuracy: 0.5192 - val_loss: 0.6878 - val_accuracy: 0.5385\n",
      "Epoch 346/1000\n",
      "104/104 [==============================] - 0s 531us/step - loss: 0.6858 - accuracy: 0.5192 - val_loss: 0.6877 - val_accuracy: 0.5385\n",
      "Epoch 347/1000\n",
      "104/104 [==============================] - 0s 551us/step - loss: 0.6885 - accuracy: 0.5096 - val_loss: 0.6876 - val_accuracy: 0.5385\n",
      "Epoch 348/1000\n",
      "104/104 [==============================] - 0s 543us/step - loss: 0.6861 - accuracy: 0.5096 - val_loss: 0.6875 - val_accuracy: 0.5385\n",
      "Epoch 349/1000\n",
      "104/104 [==============================] - 0s 528us/step - loss: 0.6852 - accuracy: 0.5192 - val_loss: 0.6872 - val_accuracy: 0.5385\n",
      "Epoch 350/1000\n",
      "104/104 [==============================] - 0s 502us/step - loss: 0.6872 - accuracy: 0.5192 - val_loss: 0.6872 - val_accuracy: 0.5385\n",
      "Epoch 351/1000\n",
      "104/104 [==============================] - 0s 523us/step - loss: 0.6795 - accuracy: 0.5000 - val_loss: 0.6869 - val_accuracy: 0.5385\n",
      "Epoch 352/1000\n",
      "104/104 [==============================] - 0s 512us/step - loss: 0.6836 - accuracy: 0.5096 - val_loss: 0.6868 - val_accuracy: 0.5385\n",
      "Epoch 353/1000\n",
      "104/104 [==============================] - 0s 525us/step - loss: 0.6856 - accuracy: 0.5096 - val_loss: 0.6869 - val_accuracy: 0.5385\n",
      "Epoch 354/1000\n",
      "104/104 [==============================] - 0s 539us/step - loss: 0.6850 - accuracy: 0.5288 - val_loss: 0.6867 - val_accuracy: 0.5385\n",
      "Epoch 355/1000\n",
      "104/104 [==============================] - 0s 540us/step - loss: 0.6859 - accuracy: 0.5096 - val_loss: 0.6864 - val_accuracy: 0.5385\n",
      "Epoch 356/1000\n",
      "104/104 [==============================] - 0s 564us/step - loss: 0.6853 - accuracy: 0.5096 - val_loss: 0.6863 - val_accuracy: 0.5385\n",
      "Epoch 357/1000\n",
      "104/104 [==============================] - 0s 539us/step - loss: 0.6871 - accuracy: 0.5000 - val_loss: 0.6863 - val_accuracy: 0.5385\n",
      "Epoch 358/1000\n",
      "104/104 [==============================] - 0s 524us/step - loss: 0.6858 - accuracy: 0.5096 - val_loss: 0.6866 - val_accuracy: 0.5385\n",
      "Epoch 359/1000\n",
      "104/104 [==============================] - 0s 536us/step - loss: 0.6875 - accuracy: 0.4808 - val_loss: 0.6866 - val_accuracy: 0.5385\n",
      "Epoch 360/1000\n",
      "104/104 [==============================] - 0s 573us/step - loss: 0.6820 - accuracy: 0.5192 - val_loss: 0.6863 - val_accuracy: 0.5385\n",
      "Epoch 361/1000\n",
      "104/104 [==============================] - 0s 566us/step - loss: 0.6874 - accuracy: 0.5192 - val_loss: 0.6863 - val_accuracy: 0.5385\n",
      "Epoch 362/1000\n",
      "104/104 [==============================] - 0s 541us/step - loss: 0.6877 - accuracy: 0.4808 - val_loss: 0.6863 - val_accuracy: 0.5385\n",
      "Epoch 363/1000\n",
      "104/104 [==============================] - 0s 525us/step - loss: 0.6844 - accuracy: 0.4904 - val_loss: 0.6863 - val_accuracy: 0.5385\n",
      "Epoch 364/1000\n",
      "104/104 [==============================] - 0s 524us/step - loss: 0.6887 - accuracy: 0.5385 - val_loss: 0.6863 - val_accuracy: 0.5000\n",
      "Epoch 365/1000\n",
      "104/104 [==============================] - 0s 519us/step - loss: 0.6828 - accuracy: 0.5385 - val_loss: 0.6861 - val_accuracy: 0.5000\n",
      "Epoch 366/1000\n",
      "104/104 [==============================] - 0s 564us/step - loss: 0.6810 - accuracy: 0.5577 - val_loss: 0.6859 - val_accuracy: 0.5000\n",
      "Epoch 367/1000\n",
      "104/104 [==============================] - 0s 583us/step - loss: 0.6859 - accuracy: 0.5288 - val_loss: 0.6859 - val_accuracy: 0.5000\n",
      "Epoch 368/1000\n",
      "104/104 [==============================] - 0s 504us/step - loss: 0.6877 - accuracy: 0.5096 - val_loss: 0.6860 - val_accuracy: 0.5000\n",
      "Epoch 369/1000\n",
      "104/104 [==============================] - 0s 476us/step - loss: 0.6852 - accuracy: 0.4712 - val_loss: 0.6857 - val_accuracy: 0.4615\n",
      "Epoch 370/1000\n",
      "104/104 [==============================] - 0s 547us/step - loss: 0.6814 - accuracy: 0.4712 - val_loss: 0.6854 - val_accuracy: 0.4615\n",
      "Epoch 371/1000\n",
      "104/104 [==============================] - 0s 618us/step - loss: 0.6764 - accuracy: 0.5577 - val_loss: 0.6850 - val_accuracy: 0.4615\n",
      "Epoch 372/1000\n",
      "104/104 [==============================] - 0s 539us/step - loss: 0.6855 - accuracy: 0.4904 - val_loss: 0.6849 - val_accuracy: 0.4615\n",
      "Epoch 373/1000\n",
      "104/104 [==============================] - 0s 550us/step - loss: 0.6843 - accuracy: 0.5000 - val_loss: 0.6849 - val_accuracy: 0.4615\n",
      "Epoch 374/1000\n",
      "104/104 [==============================] - 0s 597us/step - loss: 0.6877 - accuracy: 0.5192 - val_loss: 0.6849 - val_accuracy: 0.4615\n",
      "Epoch 375/1000\n",
      "104/104 [==============================] - 0s 531us/step - loss: 0.6776 - accuracy: 0.5000 - val_loss: 0.6847 - val_accuracy: 0.3846\n",
      "Epoch 376/1000\n",
      "104/104 [==============================] - 0s 504us/step - loss: 0.6777 - accuracy: 0.5481 - val_loss: 0.6843 - val_accuracy: 0.3462\n",
      "Epoch 377/1000\n",
      "104/104 [==============================] - 0s 473us/step - loss: 0.6834 - accuracy: 0.5000 - val_loss: 0.6841 - val_accuracy: 0.3462\n",
      "Epoch 378/1000\n",
      "104/104 [==============================] - 0s 488us/step - loss: 0.6847 - accuracy: 0.5385 - val_loss: 0.6839 - val_accuracy: 0.3846\n",
      "Epoch 379/1000\n",
      "104/104 [==============================] - 0s 537us/step - loss: 0.6837 - accuracy: 0.5096 - val_loss: 0.6841 - val_accuracy: 0.3462\n",
      "Epoch 380/1000\n",
      "104/104 [==============================] - 0s 534us/step - loss: 0.6892 - accuracy: 0.4904 - val_loss: 0.6841 - val_accuracy: 0.3846\n",
      "Epoch 381/1000\n",
      "104/104 [==============================] - 0s 536us/step - loss: 0.6868 - accuracy: 0.4808 - val_loss: 0.6839 - val_accuracy: 0.3462\n",
      "Epoch 382/1000\n",
      "104/104 [==============================] - 0s 541us/step - loss: 0.6828 - accuracy: 0.5962 - val_loss: 0.6840 - val_accuracy: 0.4231\n",
      "Epoch 383/1000\n",
      "104/104 [==============================] - 0s 535us/step - loss: 0.6755 - accuracy: 0.5769 - val_loss: 0.6839 - val_accuracy: 0.3846\n",
      "Epoch 384/1000\n",
      "104/104 [==============================] - 0s 527us/step - loss: 0.6791 - accuracy: 0.5577 - val_loss: 0.6841 - val_accuracy: 0.4231\n",
      "Epoch 385/1000\n",
      "104/104 [==============================] - 0s 495us/step - loss: 0.6834 - accuracy: 0.5577 - val_loss: 0.6842 - val_accuracy: 0.3462\n",
      "Epoch 386/1000\n",
      "104/104 [==============================] - 0s 471us/step - loss: 0.6755 - accuracy: 0.5481 - val_loss: 0.6838 - val_accuracy: 0.3846\n",
      "Epoch 387/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 545us/step - loss: 0.6827 - accuracy: 0.5481 - val_loss: 0.6839 - val_accuracy: 0.4231\n",
      "Epoch 388/1000\n",
      "104/104 [==============================] - 0s 449us/step - loss: 0.6797 - accuracy: 0.4808 - val_loss: 0.6834 - val_accuracy: 0.3462\n",
      "Epoch 389/1000\n",
      "104/104 [==============================] - 0s 464us/step - loss: 0.6884 - accuracy: 0.4519 - val_loss: 0.6836 - val_accuracy: 0.3846\n",
      "Epoch 390/1000\n",
      "104/104 [==============================] - 0s 563us/step - loss: 0.6832 - accuracy: 0.5288 - val_loss: 0.6835 - val_accuracy: 0.4231\n",
      "Epoch 391/1000\n",
      "104/104 [==============================] - 0s 537us/step - loss: 0.6785 - accuracy: 0.5577 - val_loss: 0.6835 - val_accuracy: 0.4231\n",
      "Epoch 392/1000\n",
      "104/104 [==============================] - 0s 591us/step - loss: 0.6792 - accuracy: 0.5673 - val_loss: 0.6831 - val_accuracy: 0.4231\n",
      "Epoch 393/1000\n",
      "104/104 [==============================] - 0s 559us/step - loss: 0.6768 - accuracy: 0.5865 - val_loss: 0.6829 - val_accuracy: 0.4231\n",
      "Epoch 394/1000\n",
      "104/104 [==============================] - 0s 627us/step - loss: 0.6805 - accuracy: 0.4904 - val_loss: 0.6829 - val_accuracy: 0.4231\n",
      "Epoch 395/1000\n",
      "104/104 [==============================] - 0s 558us/step - loss: 0.6868 - accuracy: 0.5769 - val_loss: 0.6831 - val_accuracy: 0.4231\n",
      "Epoch 396/1000\n",
      "104/104 [==============================] - 0s 543us/step - loss: 0.6747 - accuracy: 0.5385 - val_loss: 0.6830 - val_accuracy: 0.4231\n",
      "Epoch 397/1000\n",
      "104/104 [==============================] - 0s 547us/step - loss: 0.6720 - accuracy: 0.5865 - val_loss: 0.6831 - val_accuracy: 0.4231\n",
      "Epoch 398/1000\n",
      "104/104 [==============================] - 0s 539us/step - loss: 0.6874 - accuracy: 0.5385 - val_loss: 0.6833 - val_accuracy: 0.4231\n",
      "Epoch 399/1000\n",
      "104/104 [==============================] - 0s 559us/step - loss: 0.6761 - accuracy: 0.5962 - val_loss: 0.6828 - val_accuracy: 0.4231\n",
      "Epoch 400/1000\n",
      "104/104 [==============================] - 0s 600us/step - loss: 0.6756 - accuracy: 0.5769 - val_loss: 0.6829 - val_accuracy: 0.4231\n",
      "Epoch 401/1000\n",
      "104/104 [==============================] - 0s 589us/step - loss: 0.6751 - accuracy: 0.5962 - val_loss: 0.6826 - val_accuracy: 0.4231\n",
      "Epoch 402/1000\n",
      "104/104 [==============================] - 0s 545us/step - loss: 0.6661 - accuracy: 0.5577 - val_loss: 0.6824 - val_accuracy: 0.4231\n",
      "Epoch 403/1000\n",
      "104/104 [==============================] - 0s 566us/step - loss: 0.6735 - accuracy: 0.5577 - val_loss: 0.6823 - val_accuracy: 0.4231\n",
      "Epoch 404/1000\n",
      "104/104 [==============================] - 0s 523us/step - loss: 0.6745 - accuracy: 0.5962 - val_loss: 0.6822 - val_accuracy: 0.4231\n",
      "Epoch 405/1000\n",
      "104/104 [==============================] - 0s 546us/step - loss: 0.6740 - accuracy: 0.5481 - val_loss: 0.6823 - val_accuracy: 0.4231\n",
      "Epoch 406/1000\n",
      "104/104 [==============================] - 0s 527us/step - loss: 0.6738 - accuracy: 0.5481 - val_loss: 0.6823 - val_accuracy: 0.4231\n",
      "Epoch 407/1000\n",
      "104/104 [==============================] - 0s 573us/step - loss: 0.6815 - accuracy: 0.4904 - val_loss: 0.6821 - val_accuracy: 0.4231\n",
      "Epoch 408/1000\n",
      "104/104 [==============================] - 0s 549us/step - loss: 0.6757 - accuracy: 0.5288 - val_loss: 0.6819 - val_accuracy: 0.4231\n",
      "Epoch 409/1000\n",
      "104/104 [==============================] - 0s 525us/step - loss: 0.6702 - accuracy: 0.5000 - val_loss: 0.6820 - val_accuracy: 0.4231\n",
      "Epoch 410/1000\n",
      "104/104 [==============================] - 0s 512us/step - loss: 0.6798 - accuracy: 0.5481 - val_loss: 0.6817 - val_accuracy: 0.4231\n",
      "Epoch 411/1000\n",
      "104/104 [==============================] - 0s 548us/step - loss: 0.6693 - accuracy: 0.5962 - val_loss: 0.6814 - val_accuracy: 0.4231\n",
      "Epoch 412/1000\n",
      "104/104 [==============================] - 0s 468us/step - loss: 0.6715 - accuracy: 0.5962 - val_loss: 0.6809 - val_accuracy: 0.4231\n",
      "Epoch 413/1000\n",
      "104/104 [==============================] - 0s 525us/step - loss: 0.6831 - accuracy: 0.5962 - val_loss: 0.6807 - val_accuracy: 0.4231\n",
      "Epoch 414/1000\n",
      "104/104 [==============================] - 0s 506us/step - loss: 0.6714 - accuracy: 0.6250 - val_loss: 0.6808 - val_accuracy: 0.4231\n",
      "Epoch 415/1000\n",
      "104/104 [==============================] - 0s 431us/step - loss: 0.6738 - accuracy: 0.5288 - val_loss: 0.6813 - val_accuracy: 0.4615\n",
      "Epoch 416/1000\n",
      "104/104 [==============================] - 0s 499us/step - loss: 0.6592 - accuracy: 0.6154 - val_loss: 0.6806 - val_accuracy: 0.4231\n",
      "Epoch 417/1000\n",
      "104/104 [==============================] - 0s 450us/step - loss: 0.6702 - accuracy: 0.5192 - val_loss: 0.6808 - val_accuracy: 0.4615\n",
      "Epoch 418/1000\n",
      "104/104 [==============================] - 0s 509us/step - loss: 0.6801 - accuracy: 0.5288 - val_loss: 0.6807 - val_accuracy: 0.4615\n",
      "Epoch 419/1000\n",
      "104/104 [==============================] - 0s 517us/step - loss: 0.6771 - accuracy: 0.5962 - val_loss: 0.6811 - val_accuracy: 0.4615\n",
      "Epoch 420/1000\n",
      "104/104 [==============================] - 0s 486us/step - loss: 0.6718 - accuracy: 0.5769 - val_loss: 0.6814 - val_accuracy: 0.4615\n",
      "Epoch 421/1000\n",
      "104/104 [==============================] - 0s 550us/step - loss: 0.6794 - accuracy: 0.5673 - val_loss: 0.6812 - val_accuracy: 0.4615\n",
      "Epoch 422/1000\n",
      "104/104 [==============================] - 0s 510us/step - loss: 0.6605 - accuracy: 0.5769 - val_loss: 0.6811 - val_accuracy: 0.4615\n",
      "Epoch 423/1000\n",
      "104/104 [==============================] - 0s 505us/step - loss: 0.6621 - accuracy: 0.6058 - val_loss: 0.6807 - val_accuracy: 0.4615\n",
      "Epoch 424/1000\n",
      "104/104 [==============================] - 0s 467us/step - loss: 0.6738 - accuracy: 0.5481 - val_loss: 0.6808 - val_accuracy: 0.4615\n",
      "Epoch 425/1000\n",
      "104/104 [==============================] - 0s 560us/step - loss: 0.6686 - accuracy: 0.5673 - val_loss: 0.6805 - val_accuracy: 0.4615\n",
      "Epoch 426/1000\n",
      "104/104 [==============================] - 0s 564us/step - loss: 0.6654 - accuracy: 0.6538 - val_loss: 0.6803 - val_accuracy: 0.4615\n",
      "Epoch 427/1000\n",
      "104/104 [==============================] - 0s 474us/step - loss: 0.6721 - accuracy: 0.6731 - val_loss: 0.6804 - val_accuracy: 0.4615\n",
      "Epoch 428/1000\n",
      "104/104 [==============================] - 0s 471us/step - loss: 0.6698 - accuracy: 0.5962 - val_loss: 0.6806 - val_accuracy: 0.4615\n",
      "Epoch 429/1000\n",
      "104/104 [==============================] - 0s 545us/step - loss: 0.6552 - accuracy: 0.5962 - val_loss: 0.6802 - val_accuracy: 0.4231\n",
      "Epoch 430/1000\n",
      "104/104 [==============================] - 0s 533us/step - loss: 0.6655 - accuracy: 0.5769 - val_loss: 0.6805 - val_accuracy: 0.4231\n",
      "Epoch 431/1000\n",
      "104/104 [==============================] - 0s 587us/step - loss: 0.6591 - accuracy: 0.5865 - val_loss: 0.6806 - val_accuracy: 0.4231\n",
      "Epoch 432/1000\n",
      "104/104 [==============================] - 0s 969us/step - loss: 0.6670 - accuracy: 0.5769 - val_loss: 0.6801 - val_accuracy: 0.4615\n",
      "Epoch 433/1000\n",
      "104/104 [==============================] - 0s 964us/step - loss: 0.6736 - accuracy: 0.5192 - val_loss: 0.6798 - val_accuracy: 0.4615\n",
      "Epoch 434/1000\n",
      "104/104 [==============================] - 0s 971us/step - loss: 0.6828 - accuracy: 0.5288 - val_loss: 0.6802 - val_accuracy: 0.4615\n",
      "Epoch 435/1000\n",
      "104/104 [==============================] - 0s 911us/step - loss: 0.6621 - accuracy: 0.5865 - val_loss: 0.6804 - val_accuracy: 0.4615\n",
      "Epoch 436/1000\n",
      "104/104 [==============================] - 0s 978us/step - loss: 0.6743 - accuracy: 0.5481 - val_loss: 0.6804 - val_accuracy: 0.4615\n",
      "Epoch 437/1000\n",
      "104/104 [==============================] - 0s 969us/step - loss: 0.6675 - accuracy: 0.6538 - val_loss: 0.6805 - val_accuracy: 0.4615\n",
      "Epoch 438/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6715 - accuracy: 0.5577 - val_loss: 0.6811 - val_accuracy: 0.4615\n",
      "Epoch 439/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6608 - accuracy: 0.6250 - val_loss: 0.6815 - val_accuracy: 0.4615\n",
      "Epoch 440/1000\n",
      "104/104 [==============================] - 0s 996us/step - loss: 0.6483 - accuracy: 0.5865 - val_loss: 0.6818 - val_accuracy: 0.4615\n",
      "Epoch 441/1000\n",
      "104/104 [==============================] - 0s 977us/step - loss: 0.6684 - accuracy: 0.5962 - val_loss: 0.6816 - val_accuracy: 0.4615\n",
      "Epoch 442/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6588 - accuracy: 0.5577 - val_loss: 0.6814 - val_accuracy: 0.4615\n",
      "Epoch 443/1000\n",
      "104/104 [==============================] - 0s 990us/step - loss: 0.6678 - accuracy: 0.5673 - val_loss: 0.6809 - val_accuracy: 0.4615\n",
      "Epoch 444/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6617 - accuracy: 0.6250 - val_loss: 0.6815 - val_accuracy: 0.4615\n",
      "Epoch 445/1000\n",
      "104/104 [==============================] - 0s 964us/step - loss: 0.6593 - accuracy: 0.5673 - val_loss: 0.6815 - val_accuracy: 0.4615\n",
      "Epoch 446/1000\n",
      "104/104 [==============================] - 0s 993us/step - loss: 0.6628 - accuracy: 0.5673 - val_loss: 0.6818 - val_accuracy: 0.4615\n",
      "Epoch 447/1000\n",
      "104/104 [==============================] - 0s 967us/step - loss: 0.6605 - accuracy: 0.6154 - val_loss: 0.6814 - val_accuracy: 0.5000\n",
      "Epoch 448/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6670 - accuracy: 0.5481 - val_loss: 0.6820 - val_accuracy: 0.4615\n",
      "Epoch 449/1000\n",
      "104/104 [==============================] - 0s 986us/step - loss: 0.6541 - accuracy: 0.5865 - val_loss: 0.6820 - val_accuracy: 0.4615\n",
      "Epoch 450/1000\n",
      "104/104 [==============================] - 0s 956us/step - loss: 0.6637 - accuracy: 0.5865 - val_loss: 0.6820 - val_accuracy: 0.4615\n",
      "Epoch 451/1000\n",
      "104/104 [==============================] - 0s 967us/step - loss: 0.6472 - accuracy: 0.5962 - val_loss: 0.6820 - val_accuracy: 0.4615\n",
      "Epoch 452/1000\n",
      "104/104 [==============================] - 0s 966us/step - loss: 0.6528 - accuracy: 0.6346 - val_loss: 0.6813 - val_accuracy: 0.4615\n",
      "Epoch 453/1000\n",
      "104/104 [==============================] - 0s 965us/step - loss: 0.6497 - accuracy: 0.6058 - val_loss: 0.6820 - val_accuracy: 0.4615\n",
      "Epoch 454/1000\n",
      "104/104 [==============================] - 0s 979us/step - loss: 0.6745 - accuracy: 0.5769 - val_loss: 0.6819 - val_accuracy: 0.4615\n",
      "Epoch 455/1000\n",
      "104/104 [==============================] - 0s 995us/step - loss: 0.6666 - accuracy: 0.5481 - val_loss: 0.6822 - val_accuracy: 0.5000\n",
      "Epoch 456/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6587 - accuracy: 0.6154 - val_loss: 0.6830 - val_accuracy: 0.5000\n",
      "Epoch 457/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6517 - accuracy: 0.6058 - val_loss: 0.6842 - val_accuracy: 0.4615\n",
      "Epoch 458/1000\n",
      "104/104 [==============================] - 0s 1000us/step - loss: 0.6806 - accuracy: 0.5769 - val_loss: 0.6836 - val_accuracy: 0.5000\n",
      "Epoch 459/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6592 - accuracy: 0.6058 - val_loss: 0.6837 - val_accuracy: 0.5000\n",
      "Epoch 460/1000\n",
      "104/104 [==============================] - 0s 966us/step - loss: 0.6615 - accuracy: 0.5865 - val_loss: 0.6842 - val_accuracy: 0.5000\n",
      "Epoch 461/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6512 - accuracy: 0.6442 - val_loss: 0.6859 - val_accuracy: 0.4615\n",
      "Epoch 462/1000\n",
      "104/104 [==============================] - 0s 956us/step - loss: 0.6553 - accuracy: 0.5962 - val_loss: 0.6865 - val_accuracy: 0.4615\n",
      "Epoch 463/1000\n",
      "104/104 [==============================] - 0s 990us/step - loss: 0.6667 - accuracy: 0.5577 - val_loss: 0.6870 - val_accuracy: 0.4615\n",
      "Epoch 464/1000\n",
      "104/104 [==============================] - 0s 956us/step - loss: 0.6537 - accuracy: 0.5865 - val_loss: 0.6876 - val_accuracy: 0.4615\n",
      "Epoch 465/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6621 - accuracy: 0.6058 - val_loss: 0.6881 - val_accuracy: 0.4615\n",
      "Epoch 466/1000\n",
      "104/104 [==============================] - 0s 982us/step - loss: 0.6562 - accuracy: 0.6058 - val_loss: 0.6882 - val_accuracy: 0.4615\n",
      "Epoch 467/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6608 - accuracy: 0.5865 - val_loss: 0.6877 - val_accuracy: 0.4615\n",
      "Epoch 468/1000\n",
      "104/104 [==============================] - 0s 767us/step - loss: 0.6649 - accuracy: 0.5962 - val_loss: 0.6874 - val_accuracy: 0.4615\n",
      "Epoch 469/1000\n",
      "104/104 [==============================] - 0s 610us/step - loss: 0.6388 - accuracy: 0.6250 - val_loss: 0.6897 - val_accuracy: 0.4615\n",
      "Epoch 470/1000\n",
      "104/104 [==============================] - 0s 568us/step - loss: 0.6556 - accuracy: 0.6154 - val_loss: 0.6902 - val_accuracy: 0.4615\n",
      "Epoch 471/1000\n",
      "104/104 [==============================] - 0s 752us/step - loss: 0.6622 - accuracy: 0.6538 - val_loss: 0.6906 - val_accuracy: 0.4615\n",
      "Epoch 472/1000\n",
      "104/104 [==============================] - 0s 502us/step - loss: 0.6447 - accuracy: 0.6538 - val_loss: 0.6908 - val_accuracy: 0.4615\n",
      "Epoch 473/1000\n",
      "104/104 [==============================] - 0s 985us/step - loss: 0.6447 - accuracy: 0.6250 - val_loss: 0.6914 - val_accuracy: 0.4615\n",
      "Epoch 474/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6712 - accuracy: 0.5769 - val_loss: 0.6909 - val_accuracy: 0.4615\n",
      "Epoch 475/1000\n",
      "104/104 [==============================] - 0s 987us/step - loss: 0.6662 - accuracy: 0.5192 - val_loss: 0.6908 - val_accuracy: 0.4615\n",
      "Epoch 476/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6688 - accuracy: 0.5385 - val_loss: 0.6902 - val_accuracy: 0.4231\n",
      "Epoch 477/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6549 - accuracy: 0.5769 - val_loss: 0.6914 - val_accuracy: 0.4231\n",
      "Epoch 478/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6456 - accuracy: 0.6250 - val_loss: 0.6926 - val_accuracy: 0.4231\n",
      "Epoch 479/1000\n",
      "104/104 [==============================] - 0s 969us/step - loss: 0.6382 - accuracy: 0.6346 - val_loss: 0.6930 - val_accuracy: 0.4231\n",
      "Epoch 480/1000\n",
      "104/104 [==============================] - 0s 981us/step - loss: 0.6698 - accuracy: 0.5577 - val_loss: 0.6912 - val_accuracy: 0.4615\n",
      "Epoch 481/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6388 - accuracy: 0.5865 - val_loss: 0.6934 - val_accuracy: 0.4615\n",
      "Epoch 482/1000\n",
      "104/104 [==============================] - 0s 992us/step - loss: 0.6537 - accuracy: 0.5962 - val_loss: 0.6932 - val_accuracy: 0.4231\n",
      "Epoch 483/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6289 - accuracy: 0.6923 - val_loss: 0.6938 - val_accuracy: 0.4231\n",
      "Epoch 484/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6357 - accuracy: 0.6538 - val_loss: 0.6946 - val_accuracy: 0.4231\n",
      "Epoch 485/1000\n",
      "104/104 [==============================] - 0s 999us/step - loss: 0.6733 - accuracy: 0.5288 - val_loss: 0.6925 - val_accuracy: 0.4231\n",
      "Epoch 486/1000\n",
      "104/104 [==============================] - 0s 991us/step - loss: 0.6592 - accuracy: 0.6538 - val_loss: 0.6926 - val_accuracy: 0.4231\n",
      "Epoch 487/1000\n",
      "104/104 [==============================] - 0s 991us/step - loss: 0.6775 - accuracy: 0.5481 - val_loss: 0.6930 - val_accuracy: 0.4231\n",
      "Epoch 488/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6523 - accuracy: 0.6250 - val_loss: 0.6946 - val_accuracy: 0.4231\n",
      "Epoch 489/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6487 - accuracy: 0.6058 - val_loss: 0.6958 - val_accuracy: 0.4231\n",
      "Epoch 490/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6379 - accuracy: 0.5962 - val_loss: 0.6968 - val_accuracy: 0.4231\n",
      "Epoch 491/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6620 - accuracy: 0.5865 - val_loss: 0.6965 - val_accuracy: 0.4231\n",
      "Epoch 492/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6386 - accuracy: 0.6538 - val_loss: 0.6971 - val_accuracy: 0.4231\n",
      "Epoch 493/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6506 - accuracy: 0.6635 - val_loss: 0.6970 - val_accuracy: 0.4231\n",
      "Epoch 494/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6468 - accuracy: 0.5962 - val_loss: 0.6949 - val_accuracy: 0.4231\n",
      "Epoch 495/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6374 - accuracy: 0.6635 - val_loss: 0.6961 - val_accuracy: 0.4231\n",
      "Epoch 496/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6666 - accuracy: 0.5481 - val_loss: 0.6969 - val_accuracy: 0.4231\n",
      "Epoch 497/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6657 - accuracy: 0.5577 - val_loss: 0.6956 - val_accuracy: 0.4231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 498/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6323 - accuracy: 0.6442 - val_loss: 0.6984 - val_accuracy: 0.4231\n",
      "Epoch 499/1000\n",
      "104/104 [==============================] - 0s 983us/step - loss: 0.6529 - accuracy: 0.5865 - val_loss: 0.6979 - val_accuracy: 0.4231\n",
      "Epoch 500/1000\n",
      "104/104 [==============================] - 0s 999us/step - loss: 0.6723 - accuracy: 0.6250 - val_loss: 0.6974 - val_accuracy: 0.4231\n",
      "Epoch 501/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6434 - accuracy: 0.6731 - val_loss: 0.6954 - val_accuracy: 0.4231\n",
      "Epoch 502/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6411 - accuracy: 0.6538 - val_loss: 0.6959 - val_accuracy: 0.4231\n",
      "Epoch 503/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6563 - accuracy: 0.5577 - val_loss: 0.6971 - val_accuracy: 0.4231\n",
      "Epoch 504/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6513 - accuracy: 0.5577 - val_loss: 0.6981 - val_accuracy: 0.4231\n",
      "Epoch 505/1000\n",
      "104/104 [==============================] - 0s 977us/step - loss: 0.6725 - accuracy: 0.5385 - val_loss: 0.6965 - val_accuracy: 0.4615\n",
      "Epoch 506/1000\n",
      "104/104 [==============================] - 0s 983us/step - loss: 0.6562 - accuracy: 0.5769 - val_loss: 0.6972 - val_accuracy: 0.4615\n",
      "Epoch 507/1000\n",
      "104/104 [==============================] - 0s 978us/step - loss: 0.6481 - accuracy: 0.6154 - val_loss: 0.6995 - val_accuracy: 0.4231\n",
      "Epoch 508/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6467 - accuracy: 0.5962 - val_loss: 0.7011 - val_accuracy: 0.4231\n",
      "Epoch 509/1000\n",
      "104/104 [==============================] - 0s 948us/step - loss: 0.6436 - accuracy: 0.6058 - val_loss: 0.7009 - val_accuracy: 0.4231\n",
      "Epoch 510/1000\n",
      "104/104 [==============================] - 0s 960us/step - loss: 0.6346 - accuracy: 0.6731 - val_loss: 0.7014 - val_accuracy: 0.4231\n",
      "Epoch 511/1000\n",
      "104/104 [==============================] - 0s 992us/step - loss: 0.6737 - accuracy: 0.5385 - val_loss: 0.7027 - val_accuracy: 0.4231\n",
      "Epoch 512/1000\n",
      "104/104 [==============================] - 0s 913us/step - loss: 0.6683 - accuracy: 0.6058 - val_loss: 0.7010 - val_accuracy: 0.4231\n",
      "Epoch 513/1000\n",
      "104/104 [==============================] - 0s 957us/step - loss: 0.6518 - accuracy: 0.6058 - val_loss: 0.7026 - val_accuracy: 0.4231\n",
      "Epoch 514/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6418 - accuracy: 0.6538 - val_loss: 0.7016 - val_accuracy: 0.4231\n",
      "Epoch 515/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6311 - accuracy: 0.6058 - val_loss: 0.7014 - val_accuracy: 0.4231\n",
      "Epoch 516/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6301 - accuracy: 0.6538 - val_loss: 0.7016 - val_accuracy: 0.4231\n",
      "Epoch 517/1000\n",
      "104/104 [==============================] - 0s 998us/step - loss: 0.6450 - accuracy: 0.6154 - val_loss: 0.7010 - val_accuracy: 0.4231\n",
      "Epoch 518/1000\n",
      "104/104 [==============================] - 0s 997us/step - loss: 0.6307 - accuracy: 0.6154 - val_loss: 0.7026 - val_accuracy: 0.4231\n",
      "Epoch 519/1000\n",
      "104/104 [==============================] - 0s 989us/step - loss: 0.6553 - accuracy: 0.6250 - val_loss: 0.7051 - val_accuracy: 0.4231\n",
      "Epoch 520/1000\n",
      "104/104 [==============================] - 0s 987us/step - loss: 0.6242 - accuracy: 0.6538 - val_loss: 0.7056 - val_accuracy: 0.4231\n",
      "Epoch 521/1000\n",
      "104/104 [==============================] - 0s 894us/step - loss: 0.6537 - accuracy: 0.5673 - val_loss: 0.7068 - val_accuracy: 0.4231\n",
      "Epoch 522/1000\n",
      "104/104 [==============================] - 0s 884us/step - loss: 0.6447 - accuracy: 0.6058 - val_loss: 0.7073 - val_accuracy: 0.3846\n",
      "Epoch 523/1000\n",
      "104/104 [==============================] - 0s 924us/step - loss: 0.6384 - accuracy: 0.6442 - val_loss: 0.7090 - val_accuracy: 0.3846\n",
      "Epoch 524/1000\n",
      "104/104 [==============================] - 0s 887us/step - loss: 0.6376 - accuracy: 0.6442 - val_loss: 0.7111 - val_accuracy: 0.3846\n",
      "Epoch 525/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6363 - accuracy: 0.6442 - val_loss: 0.7112 - val_accuracy: 0.3846\n",
      "Epoch 526/1000\n",
      "104/104 [==============================] - 0s 927us/step - loss: 0.6283 - accuracy: 0.6731 - val_loss: 0.7110 - val_accuracy: 0.3846\n",
      "Epoch 527/1000\n",
      "104/104 [==============================] - 0s 908us/step - loss: 0.6421 - accuracy: 0.5962 - val_loss: 0.7099 - val_accuracy: 0.3846\n",
      "Epoch 528/1000\n",
      "104/104 [==============================] - 0s 976us/step - loss: 0.6559 - accuracy: 0.6154 - val_loss: 0.7082 - val_accuracy: 0.3846\n",
      "Epoch 529/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6444 - accuracy: 0.6250 - val_loss: 0.7104 - val_accuracy: 0.3846\n",
      "Epoch 530/1000\n",
      "104/104 [==============================] - 0s 992us/step - loss: 0.6315 - accuracy: 0.6250 - val_loss: 0.7108 - val_accuracy: 0.3846\n",
      "Epoch 531/1000\n",
      "104/104 [==============================] - 0s 971us/step - loss: 0.6492 - accuracy: 0.5673 - val_loss: 0.7107 - val_accuracy: 0.3846\n",
      "Epoch 532/1000\n",
      "104/104 [==============================] - 0s 972us/step - loss: 0.6424 - accuracy: 0.6250 - val_loss: 0.7123 - val_accuracy: 0.3846\n",
      "Epoch 533/1000\n",
      "104/104 [==============================] - 0s 972us/step - loss: 0.6340 - accuracy: 0.6058 - val_loss: 0.7122 - val_accuracy: 0.3846\n",
      "Epoch 534/1000\n",
      "104/104 [==============================] - 0s 990us/step - loss: 0.6467 - accuracy: 0.5865 - val_loss: 0.7124 - val_accuracy: 0.3846\n",
      "Epoch 535/1000\n",
      "104/104 [==============================] - 0s 991us/step - loss: 0.6339 - accuracy: 0.5962 - val_loss: 0.7125 - val_accuracy: 0.3846\n",
      "Epoch 536/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6370 - accuracy: 0.6250 - val_loss: 0.7135 - val_accuracy: 0.3846\n",
      "Epoch 537/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6591 - accuracy: 0.5673 - val_loss: 0.7130 - val_accuracy: 0.3846\n",
      "Epoch 538/1000\n",
      "104/104 [==============================] - 0s 930us/step - loss: 0.6700 - accuracy: 0.6058 - val_loss: 0.7109 - val_accuracy: 0.3846\n",
      "Epoch 539/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6407 - accuracy: 0.6058 - val_loss: 0.7114 - val_accuracy: 0.4231\n",
      "Epoch 540/1000\n",
      "104/104 [==============================] - 0s 961us/step - loss: 0.6461 - accuracy: 0.6154 - val_loss: 0.7116 - val_accuracy: 0.3846\n",
      "Epoch 541/1000\n",
      "104/104 [==============================] - 0s 987us/step - loss: 0.6242 - accuracy: 0.6346 - val_loss: 0.7116 - val_accuracy: 0.4231\n",
      "Epoch 542/1000\n",
      "104/104 [==============================] - 0s 994us/step - loss: 0.6464 - accuracy: 0.5962 - val_loss: 0.7132 - val_accuracy: 0.3846\n",
      "Epoch 543/1000\n",
      "104/104 [==============================] - 0s 959us/step - loss: 0.6280 - accuracy: 0.6346 - val_loss: 0.7133 - val_accuracy: 0.3846\n",
      "Epoch 544/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6286 - accuracy: 0.6442 - val_loss: 0.7123 - val_accuracy: 0.4231\n",
      "Epoch 545/1000\n",
      "104/104 [==============================] - 0s 940us/step - loss: 0.6279 - accuracy: 0.6538 - val_loss: 0.7156 - val_accuracy: 0.3846\n",
      "Epoch 546/1000\n",
      "104/104 [==============================] - 0s 935us/step - loss: 0.6572 - accuracy: 0.5962 - val_loss: 0.7148 - val_accuracy: 0.3846\n",
      "Epoch 547/1000\n",
      "104/104 [==============================] - 0s 970us/step - loss: 0.6154 - accuracy: 0.6538 - val_loss: 0.7163 - val_accuracy: 0.3846\n",
      "Epoch 548/1000\n",
      "104/104 [==============================] - 0s 983us/step - loss: 0.6308 - accuracy: 0.6635 - val_loss: 0.7198 - val_accuracy: 0.3846\n",
      "Epoch 549/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6643 - accuracy: 0.6346 - val_loss: 0.7158 - val_accuracy: 0.3846\n",
      "Epoch 550/1000\n",
      "104/104 [==============================] - 0s 993us/step - loss: 0.6391 - accuracy: 0.6442 - val_loss: 0.7166 - val_accuracy: 0.3846\n",
      "Epoch 551/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6371 - accuracy: 0.6154 - val_loss: 0.7161 - val_accuracy: 0.3846\n",
      "Epoch 552/1000\n",
      "104/104 [==============================] - 0s 991us/step - loss: 0.6272 - accuracy: 0.6923 - val_loss: 0.7174 - val_accuracy: 0.3846\n",
      "Epoch 553/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 935us/step - loss: 0.6224 - accuracy: 0.6538 - val_loss: 0.7181 - val_accuracy: 0.3846\n",
      "Epoch 554/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6550 - accuracy: 0.6058 - val_loss: 0.7184 - val_accuracy: 0.3846\n",
      "Epoch 555/1000\n",
      "104/104 [==============================] - 0s 999us/step - loss: 0.6001 - accuracy: 0.7115 - val_loss: 0.7199 - val_accuracy: 0.3846\n",
      "Epoch 556/1000\n",
      "104/104 [==============================] - 0s 982us/step - loss: 0.6363 - accuracy: 0.6250 - val_loss: 0.7220 - val_accuracy: 0.3846\n",
      "Epoch 557/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6438 - accuracy: 0.6635 - val_loss: 0.7237 - val_accuracy: 0.3846\n",
      "Epoch 558/1000\n",
      "104/104 [==============================] - 0s 975us/step - loss: 0.6325 - accuracy: 0.6538 - val_loss: 0.7235 - val_accuracy: 0.3846\n",
      "Epoch 559/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6340 - accuracy: 0.6635 - val_loss: 0.7214 - val_accuracy: 0.3846\n",
      "Epoch 560/1000\n",
      "104/104 [==============================] - 0s 983us/step - loss: 0.6420 - accuracy: 0.6250 - val_loss: 0.7224 - val_accuracy: 0.3846\n",
      "Epoch 561/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6248 - accuracy: 0.6442 - val_loss: 0.7233 - val_accuracy: 0.3846\n",
      "Epoch 562/1000\n",
      "104/104 [==============================] - 0s 982us/step - loss: 0.6316 - accuracy: 0.6538 - val_loss: 0.7211 - val_accuracy: 0.4231\n",
      "Epoch 563/1000\n",
      "104/104 [==============================] - 0s 964us/step - loss: 0.6346 - accuracy: 0.6154 - val_loss: 0.7233 - val_accuracy: 0.3846\n",
      "Epoch 564/1000\n",
      "104/104 [==============================] - 0s 937us/step - loss: 0.6871 - accuracy: 0.5865 - val_loss: 0.7202 - val_accuracy: 0.3846\n",
      "Epoch 565/1000\n",
      "104/104 [==============================] - 0s 979us/step - loss: 0.6588 - accuracy: 0.5865 - val_loss: 0.7197 - val_accuracy: 0.3846\n",
      "Epoch 566/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6391 - accuracy: 0.5673 - val_loss: 0.7200 - val_accuracy: 0.3846\n",
      "Epoch 567/1000\n",
      "104/104 [==============================] - 0s 992us/step - loss: 0.6201 - accuracy: 0.6538 - val_loss: 0.7217 - val_accuracy: 0.3846\n",
      "Epoch 568/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6062 - accuracy: 0.6731 - val_loss: 0.7237 - val_accuracy: 0.3846\n",
      "Epoch 569/1000\n",
      "104/104 [==============================] - 0s 939us/step - loss: 0.6488 - accuracy: 0.5962 - val_loss: 0.7233 - val_accuracy: 0.3846\n",
      "Epoch 570/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6102 - accuracy: 0.6250 - val_loss: 0.7224 - val_accuracy: 0.3846\n",
      "Epoch 571/1000\n",
      "104/104 [==============================] - 0s 973us/step - loss: 0.6510 - accuracy: 0.6058 - val_loss: 0.7198 - val_accuracy: 0.3846\n",
      "Epoch 572/1000\n",
      "104/104 [==============================] - 0s 942us/step - loss: 0.6124 - accuracy: 0.6635 - val_loss: 0.7224 - val_accuracy: 0.3846\n",
      "Epoch 573/1000\n",
      "104/104 [==============================] - 0s 883us/step - loss: 0.6379 - accuracy: 0.6250 - val_loss: 0.7203 - val_accuracy: 0.3846\n",
      "Epoch 574/1000\n",
      "104/104 [==============================] - 0s 804us/step - loss: 0.6482 - accuracy: 0.6154 - val_loss: 0.7256 - val_accuracy: 0.3846\n",
      "Epoch 575/1000\n",
      "104/104 [==============================] - 0s 967us/step - loss: 0.6330 - accuracy: 0.6250 - val_loss: 0.7249 - val_accuracy: 0.3846\n",
      "Epoch 576/1000\n",
      "104/104 [==============================] - 0s 917us/step - loss: 0.6051 - accuracy: 0.6442 - val_loss: 0.7269 - val_accuracy: 0.3846\n",
      "Epoch 577/1000\n",
      "104/104 [==============================] - 0s 978us/step - loss: 0.6311 - accuracy: 0.6538 - val_loss: 0.7254 - val_accuracy: 0.3846\n",
      "Epoch 578/1000\n",
      "104/104 [==============================] - 0s 949us/step - loss: 0.6120 - accuracy: 0.6538 - val_loss: 0.7292 - val_accuracy: 0.3846\n",
      "Epoch 579/1000\n",
      "104/104 [==============================] - 0s 946us/step - loss: 0.6488 - accuracy: 0.6058 - val_loss: 0.7274 - val_accuracy: 0.3846\n",
      "Epoch 580/1000\n",
      "104/104 [==============================] - 0s 884us/step - loss: 0.6468 - accuracy: 0.5673 - val_loss: 0.7289 - val_accuracy: 0.3846\n",
      "Epoch 581/1000\n",
      "104/104 [==============================] - 0s 871us/step - loss: 0.6246 - accuracy: 0.6731 - val_loss: 0.7300 - val_accuracy: 0.3846\n",
      "Epoch 582/1000\n",
      "104/104 [==============================] - 0s 932us/step - loss: 0.6478 - accuracy: 0.5962 - val_loss: 0.7287 - val_accuracy: 0.3462\n",
      "Epoch 583/1000\n",
      "104/104 [==============================] - 0s 834us/step - loss: 0.6330 - accuracy: 0.6346 - val_loss: 0.7290 - val_accuracy: 0.3462\n",
      "Epoch 584/1000\n",
      "104/104 [==============================] - 0s 846us/step - loss: 0.6434 - accuracy: 0.6442 - val_loss: 0.7324 - val_accuracy: 0.3462\n",
      "Epoch 585/1000\n",
      "104/104 [==============================] - 0s 956us/step - loss: 0.6419 - accuracy: 0.7019 - val_loss: 0.7322 - val_accuracy: 0.3462\n",
      "Epoch 586/1000\n",
      "104/104 [==============================] - 0s 925us/step - loss: 0.6426 - accuracy: 0.6442 - val_loss: 0.7314 - val_accuracy: 0.3462\n",
      "Epoch 587/1000\n",
      "104/104 [==============================] - 0s 982us/step - loss: 0.6063 - accuracy: 0.6442 - val_loss: 0.7318 - val_accuracy: 0.3846\n",
      "Epoch 588/1000\n",
      "104/104 [==============================] - 0s 917us/step - loss: 0.6177 - accuracy: 0.6635 - val_loss: 0.7337 - val_accuracy: 0.3846\n",
      "Epoch 589/1000\n",
      "104/104 [==============================] - 0s 941us/step - loss: 0.6226 - accuracy: 0.6731 - val_loss: 0.7328 - val_accuracy: 0.3846\n",
      "Epoch 590/1000\n",
      "104/104 [==============================] - 0s 972us/step - loss: 0.6227 - accuracy: 0.6154 - val_loss: 0.7320 - val_accuracy: 0.3846\n",
      "Epoch 591/1000\n",
      "104/104 [==============================] - 0s 953us/step - loss: 0.5923 - accuracy: 0.6923 - val_loss: 0.7349 - val_accuracy: 0.3462\n",
      "Epoch 592/1000\n",
      "104/104 [==============================] - 0s 922us/step - loss: 0.6025 - accuracy: 0.6538 - val_loss: 0.7375 - val_accuracy: 0.3462\n",
      "Epoch 593/1000\n",
      "104/104 [==============================] - 0s 925us/step - loss: 0.6259 - accuracy: 0.6635 - val_loss: 0.7371 - val_accuracy: 0.3462\n",
      "Epoch 594/1000\n",
      "104/104 [==============================] - 0s 914us/step - loss: 0.6218 - accuracy: 0.6731 - val_loss: 0.7371 - val_accuracy: 0.3462\n",
      "Epoch 595/1000\n",
      "104/104 [==============================] - 0s 852us/step - loss: 0.6060 - accuracy: 0.7212 - val_loss: 0.7389 - val_accuracy: 0.3077\n",
      "Epoch 596/1000\n",
      "104/104 [==============================] - 0s 894us/step - loss: 0.6316 - accuracy: 0.6827 - val_loss: 0.7346 - val_accuracy: 0.3846\n",
      "Epoch 597/1000\n",
      "104/104 [==============================] - 0s 976us/step - loss: 0.6215 - accuracy: 0.6538 - val_loss: 0.7405 - val_accuracy: 0.3846\n",
      "Epoch 598/1000\n",
      "104/104 [==============================] - 0s 897us/step - loss: 0.6284 - accuracy: 0.6346 - val_loss: 0.7394 - val_accuracy: 0.3846\n",
      "Epoch 599/1000\n",
      "104/104 [==============================] - 0s 950us/step - loss: 0.6090 - accuracy: 0.6923 - val_loss: 0.7429 - val_accuracy: 0.3077\n",
      "Epoch 600/1000\n",
      "104/104 [==============================] - 0s 920us/step - loss: 0.6138 - accuracy: 0.6538 - val_loss: 0.7438 - val_accuracy: 0.3462\n",
      "Epoch 601/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.5993 - accuracy: 0.6923 - val_loss: 0.7471 - val_accuracy: 0.3846\n",
      "Epoch 602/1000\n",
      "104/104 [==============================] - 0s 912us/step - loss: 0.6415 - accuracy: 0.5673 - val_loss: 0.7483 - val_accuracy: 0.3462\n",
      "Epoch 603/1000\n",
      "104/104 [==============================] - 0s 889us/step - loss: 0.6118 - accuracy: 0.7019 - val_loss: 0.7499 - val_accuracy: 0.3077\n",
      "Epoch 604/1000\n",
      "104/104 [==============================] - 0s 896us/step - loss: 0.6154 - accuracy: 0.6635 - val_loss: 0.7476 - val_accuracy: 0.3077\n",
      "Epoch 605/1000\n",
      "104/104 [==============================] - 0s 922us/step - loss: 0.6330 - accuracy: 0.6635 - val_loss: 0.7450 - val_accuracy: 0.3462\n",
      "Epoch 606/1000\n",
      "104/104 [==============================] - 0s 902us/step - loss: 0.6302 - accuracy: 0.5865 - val_loss: 0.7487 - val_accuracy: 0.3077\n",
      "Epoch 607/1000\n",
      "104/104 [==============================] - 0s 929us/step - loss: 0.6001 - accuracy: 0.7019 - val_loss: 0.7492 - val_accuracy: 0.3077\n",
      "Epoch 608/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 920us/step - loss: 0.6166 - accuracy: 0.6346 - val_loss: 0.7525 - val_accuracy: 0.3462\n",
      "Epoch 609/1000\n",
      "104/104 [==============================] - 0s 941us/step - loss: 0.6422 - accuracy: 0.5962 - val_loss: 0.7519 - val_accuracy: 0.3462\n",
      "Epoch 610/1000\n",
      "104/104 [==============================] - 0s 996us/step - loss: 0.6219 - accuracy: 0.6827 - val_loss: 0.7528 - val_accuracy: 0.3462\n",
      "Epoch 611/1000\n",
      "104/104 [==============================] - 0s 876us/step - loss: 0.6175 - accuracy: 0.6635 - val_loss: 0.7473 - val_accuracy: 0.3077\n",
      "Epoch 612/1000\n",
      "104/104 [==============================] - 0s 953us/step - loss: 0.6251 - accuracy: 0.6635 - val_loss: 0.7479 - val_accuracy: 0.3077\n",
      "Epoch 613/1000\n",
      "104/104 [==============================] - 0s 952us/step - loss: 0.6170 - accuracy: 0.6346 - val_loss: 0.7467 - val_accuracy: 0.3462\n",
      "Epoch 614/1000\n",
      "104/104 [==============================] - 0s 858us/step - loss: 0.6488 - accuracy: 0.6442 - val_loss: 0.7452 - val_accuracy: 0.3077\n",
      "Epoch 615/1000\n",
      "104/104 [==============================] - 0s 919us/step - loss: 0.6539 - accuracy: 0.5865 - val_loss: 0.7459 - val_accuracy: 0.3077\n",
      "Epoch 616/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6025 - accuracy: 0.6635 - val_loss: 0.7435 - val_accuracy: 0.3846\n",
      "Epoch 617/1000\n",
      "104/104 [==============================] - 0s 896us/step - loss: 0.6247 - accuracy: 0.6538 - val_loss: 0.7459 - val_accuracy: 0.3077\n",
      "Epoch 618/1000\n",
      "104/104 [==============================] - 0s 860us/step - loss: 0.6505 - accuracy: 0.6058 - val_loss: 0.7432 - val_accuracy: 0.3846\n",
      "Epoch 619/1000\n",
      "104/104 [==============================] - 0s 940us/step - loss: 0.6263 - accuracy: 0.6346 - val_loss: 0.7409 - val_accuracy: 0.3462\n",
      "Epoch 620/1000\n",
      "104/104 [==============================] - 0s 904us/step - loss: 0.5970 - accuracy: 0.7019 - val_loss: 0.7446 - val_accuracy: 0.3462\n",
      "Epoch 621/1000\n",
      "104/104 [==============================] - 0s 942us/step - loss: 0.6260 - accuracy: 0.6250 - val_loss: 0.7428 - val_accuracy: 0.3846\n",
      "Epoch 622/1000\n",
      "104/104 [==============================] - 0s 952us/step - loss: 0.5994 - accuracy: 0.6442 - val_loss: 0.7485 - val_accuracy: 0.3846\n",
      "Epoch 623/1000\n",
      "104/104 [==============================] - 0s 902us/step - loss: 0.6117 - accuracy: 0.6635 - val_loss: 0.7488 - val_accuracy: 0.3462\n",
      "Epoch 624/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6307 - accuracy: 0.6538 - val_loss: 0.7476 - val_accuracy: 0.3077\n",
      "Epoch 625/1000\n",
      "104/104 [==============================] - 0s 951us/step - loss: 0.6698 - accuracy: 0.6154 - val_loss: 0.7420 - val_accuracy: 0.4231\n",
      "Epoch 626/1000\n",
      "104/104 [==============================] - 0s 974us/step - loss: 0.6148 - accuracy: 0.6058 - val_loss: 0.7414 - val_accuracy: 0.4231\n",
      "Epoch 627/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6311 - accuracy: 0.6250 - val_loss: 0.7416 - val_accuracy: 0.4231\n",
      "Epoch 628/1000\n",
      "104/104 [==============================] - 0s 999us/step - loss: 0.6408 - accuracy: 0.6154 - val_loss: 0.7421 - val_accuracy: 0.4231\n",
      "Epoch 629/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6095 - accuracy: 0.6442 - val_loss: 0.7446 - val_accuracy: 0.3846\n",
      "Epoch 630/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6254 - accuracy: 0.6635 - val_loss: 0.7404 - val_accuracy: 0.3846\n",
      "Epoch 631/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6225 - accuracy: 0.6250 - val_loss: 0.7395 - val_accuracy: 0.3462\n",
      "Epoch 632/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6175 - accuracy: 0.6635 - val_loss: 0.7368 - val_accuracy: 0.3846\n",
      "Epoch 633/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.5971 - accuracy: 0.6731 - val_loss: 0.7410 - val_accuracy: 0.3462\n",
      "Epoch 634/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.5958 - accuracy: 0.7019 - val_loss: 0.7441 - val_accuracy: 0.3462\n",
      "Epoch 635/1000\n",
      "104/104 [==============================] - 0s 994us/step - loss: 0.6104 - accuracy: 0.6731 - val_loss: 0.7452 - val_accuracy: 0.3462\n",
      "Epoch 636/1000\n",
      "104/104 [==============================] - 0s 870us/step - loss: 0.6257 - accuracy: 0.6346 - val_loss: 0.7469 - val_accuracy: 0.3462\n",
      "Epoch 637/1000\n",
      "104/104 [==============================] - 0s 972us/step - loss: 0.5895 - accuracy: 0.6538 - val_loss: 0.7522 - val_accuracy: 0.3077\n",
      "Epoch 638/1000\n",
      "104/104 [==============================] - 0s 822us/step - loss: 0.6172 - accuracy: 0.6250 - val_loss: 0.7497 - val_accuracy: 0.3462\n",
      "Epoch 639/1000\n",
      "104/104 [==============================] - 0s 870us/step - loss: 0.6052 - accuracy: 0.6346 - val_loss: 0.7516 - val_accuracy: 0.3462\n",
      "Epoch 640/1000\n",
      "104/104 [==============================] - 0s 870us/step - loss: 0.5936 - accuracy: 0.6827 - val_loss: 0.7524 - val_accuracy: 0.3462\n",
      "Epoch 641/1000\n",
      "104/104 [==============================] - 0s 987us/step - loss: 0.6006 - accuracy: 0.6827 - val_loss: 0.7535 - val_accuracy: 0.3077\n",
      "Epoch 642/1000\n",
      "104/104 [==============================] - 0s 939us/step - loss: 0.5889 - accuracy: 0.7212 - val_loss: 0.7659 - val_accuracy: 0.3846\n",
      "Epoch 643/1000\n",
      "104/104 [==============================] - 0s 926us/step - loss: 0.5712 - accuracy: 0.7308 - val_loss: 0.7703 - val_accuracy: 0.3077\n",
      "Epoch 644/1000\n",
      "104/104 [==============================] - 0s 888us/step - loss: 0.5787 - accuracy: 0.6635 - val_loss: 0.7686 - val_accuracy: 0.3462\n",
      "Epoch 645/1000\n",
      "104/104 [==============================] - 0s 951us/step - loss: 0.6294 - accuracy: 0.6346 - val_loss: 0.7674 - val_accuracy: 0.3846\n",
      "Epoch 646/1000\n",
      "104/104 [==============================] - 0s 955us/step - loss: 0.6352 - accuracy: 0.6154 - val_loss: 0.7665 - val_accuracy: 0.3462\n",
      "Epoch 647/1000\n",
      "104/104 [==============================] - 0s 949us/step - loss: 0.5961 - accuracy: 0.6923 - val_loss: 0.7675 - val_accuracy: 0.3846\n",
      "Epoch 648/1000\n",
      "104/104 [==============================] - 0s 896us/step - loss: 0.5957 - accuracy: 0.7404 - val_loss: 0.7733 - val_accuracy: 0.2692\n",
      "Epoch 649/1000\n",
      "104/104 [==============================] - 0s 898us/step - loss: 0.6078 - accuracy: 0.6154 - val_loss: 0.7699 - val_accuracy: 0.3462\n",
      "Epoch 650/1000\n",
      "104/104 [==============================] - 0s 983us/step - loss: 0.5808 - accuracy: 0.7212 - val_loss: 0.7699 - val_accuracy: 0.3846\n",
      "Epoch 651/1000\n",
      "104/104 [==============================] - 0s 958us/step - loss: 0.5925 - accuracy: 0.7308 - val_loss: 0.7666 - val_accuracy: 0.3846\n",
      "Epoch 652/1000\n",
      "104/104 [==============================] - 0s 985us/step - loss: 0.6304 - accuracy: 0.6250 - val_loss: 0.7616 - val_accuracy: 0.4231\n",
      "Epoch 653/1000\n",
      "104/104 [==============================] - 0s 971us/step - loss: 0.5922 - accuracy: 0.6635 - val_loss: 0.7665 - val_accuracy: 0.3846\n",
      "Epoch 654/1000\n",
      "104/104 [==============================] - 0s 978us/step - loss: 0.6528 - accuracy: 0.5962 - val_loss: 0.7607 - val_accuracy: 0.3846\n",
      "Epoch 655/1000\n",
      "104/104 [==============================] - 0s 969us/step - loss: 0.6266 - accuracy: 0.6058 - val_loss: 0.7611 - val_accuracy: 0.4231\n",
      "Epoch 656/1000\n",
      "104/104 [==============================] - 0s 983us/step - loss: 0.6116 - accuracy: 0.6346 - val_loss: 0.7646 - val_accuracy: 0.4231\n",
      "Epoch 657/1000\n",
      "104/104 [==============================] - 0s 986us/step - loss: 0.6123 - accuracy: 0.6731 - val_loss: 0.7604 - val_accuracy: 0.4231\n",
      "Epoch 658/1000\n",
      "104/104 [==============================] - 0s 947us/step - loss: 0.6149 - accuracy: 0.5962 - val_loss: 0.7597 - val_accuracy: 0.4231\n",
      "Epoch 659/1000\n",
      "104/104 [==============================] - 0s 967us/step - loss: 0.6470 - accuracy: 0.6250 - val_loss: 0.7540 - val_accuracy: 0.4231\n",
      "Epoch 660/1000\n",
      "104/104 [==============================] - 0s 921us/step - loss: 0.6060 - accuracy: 0.6442 - val_loss: 0.7583 - val_accuracy: 0.3846\n",
      "Epoch 661/1000\n",
      "104/104 [==============================] - 0s 870us/step - loss: 0.6018 - accuracy: 0.6635 - val_loss: 0.7574 - val_accuracy: 0.3846\n",
      "Epoch 662/1000\n",
      "104/104 [==============================] - 0s 939us/step - loss: 0.5766 - accuracy: 0.6538 - val_loss: 0.7578 - val_accuracy: 0.3846\n",
      "Epoch 663/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 944us/step - loss: 0.6203 - accuracy: 0.6250 - val_loss: 0.7626 - val_accuracy: 0.4231\n",
      "Epoch 664/1000\n",
      "104/104 [==============================] - 0s 969us/step - loss: 0.6417 - accuracy: 0.6635 - val_loss: 0.7601 - val_accuracy: 0.4231\n",
      "Epoch 665/1000\n",
      "104/104 [==============================] - 0s 991us/step - loss: 0.6312 - accuracy: 0.5962 - val_loss: 0.7565 - val_accuracy: 0.4231\n",
      "Epoch 666/1000\n",
      "104/104 [==============================] - 0s 994us/step - loss: 0.5776 - accuracy: 0.6827 - val_loss: 0.7647 - val_accuracy: 0.4231\n",
      "Epoch 667/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.5693 - accuracy: 0.7019 - val_loss: 0.7727 - val_accuracy: 0.4231\n",
      "Epoch 668/1000\n",
      "104/104 [==============================] - 0s 545us/step - loss: 0.6188 - accuracy: 0.6731 - val_loss: 0.7726 - val_accuracy: 0.3846\n",
      "Epoch 669/1000\n",
      "104/104 [==============================] - 0s 573us/step - loss: 0.6055 - accuracy: 0.6250 - val_loss: 0.7728 - val_accuracy: 0.3846\n",
      "Epoch 670/1000\n",
      "104/104 [==============================] - 0s 554us/step - loss: 0.5996 - accuracy: 0.6923 - val_loss: 0.7705 - val_accuracy: 0.4231\n",
      "Epoch 671/1000\n",
      "104/104 [==============================] - 0s 775us/step - loss: 0.5811 - accuracy: 0.6538 - val_loss: 0.7764 - val_accuracy: 0.3846\n",
      "Epoch 672/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.5969 - accuracy: 0.6827 - val_loss: 0.7775 - val_accuracy: 0.3846\n",
      "Epoch 673/1000\n",
      "104/104 [==============================] - 0s 691us/step - loss: 0.5827 - accuracy: 0.6827 - val_loss: 0.7814 - val_accuracy: 0.3846\n",
      "Epoch 674/1000\n",
      "104/104 [==============================] - 0s 579us/step - loss: 0.6024 - accuracy: 0.6250 - val_loss: 0.7837 - val_accuracy: 0.3846\n",
      "Epoch 675/1000\n",
      "104/104 [==============================] - 0s 570us/step - loss: 0.5691 - accuracy: 0.7981 - val_loss: 0.7885 - val_accuracy: 0.3462\n",
      "Epoch 676/1000\n",
      "104/104 [==============================] - 0s 597us/step - loss: 0.5866 - accuracy: 0.6442 - val_loss: 0.7907 - val_accuracy: 0.3462\n",
      "Epoch 677/1000\n",
      "104/104 [==============================] - 0s 664us/step - loss: 0.6122 - accuracy: 0.6827 - val_loss: 0.7846 - val_accuracy: 0.3846\n",
      "Epoch 678/1000\n",
      "104/104 [==============================] - 0s 571us/step - loss: 0.5848 - accuracy: 0.6635 - val_loss: 0.7802 - val_accuracy: 0.3846\n",
      "Epoch 679/1000\n",
      "104/104 [==============================] - 0s 568us/step - loss: 0.6032 - accuracy: 0.6731 - val_loss: 0.7806 - val_accuracy: 0.4615\n",
      "Epoch 680/1000\n",
      "104/104 [==============================] - 0s 571us/step - loss: 0.5753 - accuracy: 0.7212 - val_loss: 0.7850 - val_accuracy: 0.4231\n",
      "Epoch 681/1000\n",
      "104/104 [==============================] - 0s 695us/step - loss: 0.6111 - accuracy: 0.6250 - val_loss: 0.7843 - val_accuracy: 0.4615\n",
      "Epoch 682/1000\n",
      "104/104 [==============================] - 0s 570us/step - loss: 0.5969 - accuracy: 0.6442 - val_loss: 0.7852 - val_accuracy: 0.4615\n",
      "Epoch 683/1000\n",
      "104/104 [==============================] - 0s 570us/step - loss: 0.5676 - accuracy: 0.6827 - val_loss: 0.7879 - val_accuracy: 0.4231\n",
      "Epoch 684/1000\n",
      "104/104 [==============================] - 0s 739us/step - loss: 0.5609 - accuracy: 0.7019 - val_loss: 0.7981 - val_accuracy: 0.3462\n",
      "Epoch 685/1000\n",
      "104/104 [==============================] - 0s 638us/step - loss: 0.5624 - accuracy: 0.7019 - val_loss: 0.7987 - val_accuracy: 0.3462\n",
      "Epoch 686/1000\n",
      "104/104 [==============================] - 0s 617us/step - loss: 0.5999 - accuracy: 0.6731 - val_loss: 0.7979 - val_accuracy: 0.3846\n",
      "Epoch 687/1000\n",
      "104/104 [==============================] - 0s 610us/step - loss: 0.5777 - accuracy: 0.6827 - val_loss: 0.8055 - val_accuracy: 0.3462\n",
      "Epoch 688/1000\n",
      "104/104 [==============================] - 0s 616us/step - loss: 0.5732 - accuracy: 0.6538 - val_loss: 0.8108 - val_accuracy: 0.3462\n",
      "Epoch 689/1000\n",
      "104/104 [==============================] - 0s 649us/step - loss: 0.6104 - accuracy: 0.6346 - val_loss: 0.8029 - val_accuracy: 0.3462\n",
      "Epoch 690/1000\n",
      "104/104 [==============================] - 0s 642us/step - loss: 0.6105 - accuracy: 0.6538 - val_loss: 0.7998 - val_accuracy: 0.4231\n",
      "Epoch 691/1000\n",
      "104/104 [==============================] - 0s 754us/step - loss: 0.5932 - accuracy: 0.6731 - val_loss: 0.8044 - val_accuracy: 0.4615\n",
      "Epoch 692/1000\n",
      "104/104 [==============================] - 0s 816us/step - loss: 0.6160 - accuracy: 0.6346 - val_loss: 0.8004 - val_accuracy: 0.4231\n",
      "Epoch 693/1000\n",
      "104/104 [==============================] - 0s 549us/step - loss: 0.6005 - accuracy: 0.6442 - val_loss: 0.7998 - val_accuracy: 0.4231\n",
      "Epoch 694/1000\n",
      "104/104 [==============================] - 0s 673us/step - loss: 0.5829 - accuracy: 0.7115 - val_loss: 0.7981 - val_accuracy: 0.3846\n",
      "Epoch 695/1000\n",
      "104/104 [==============================] - 0s 541us/step - loss: 0.5808 - accuracy: 0.6538 - val_loss: 0.7868 - val_accuracy: 0.3846\n",
      "Epoch 696/1000\n",
      "104/104 [==============================] - 0s 553us/step - loss: 0.5487 - accuracy: 0.6827 - val_loss: 0.8036 - val_accuracy: 0.4231\n",
      "Epoch 697/1000\n",
      "104/104 [==============================] - 0s 537us/step - loss: 0.5881 - accuracy: 0.7500 - val_loss: 0.8074 - val_accuracy: 0.3462\n",
      "Epoch 698/1000\n",
      "104/104 [==============================] - 0s 545us/step - loss: 0.5650 - accuracy: 0.7404 - val_loss: 0.8179 - val_accuracy: 0.3462\n",
      "Epoch 699/1000\n",
      "104/104 [==============================] - 0s 559us/step - loss: 0.5882 - accuracy: 0.6346 - val_loss: 0.8165 - val_accuracy: 0.3077\n",
      "Epoch 700/1000\n",
      "104/104 [==============================] - 0s 579us/step - loss: 0.6185 - accuracy: 0.6250 - val_loss: 0.8139 - val_accuracy: 0.4231\n",
      "Epoch 701/1000\n",
      "104/104 [==============================] - 0s 559us/step - loss: 0.5828 - accuracy: 0.7212 - val_loss: 0.8096 - val_accuracy: 0.3846\n",
      "Epoch 702/1000\n",
      "104/104 [==============================] - 0s 545us/step - loss: 0.5697 - accuracy: 0.7115 - val_loss: 0.8175 - val_accuracy: 0.3462\n",
      "Epoch 703/1000\n",
      "104/104 [==============================] - 0s 569us/step - loss: 0.6099 - accuracy: 0.6442 - val_loss: 0.8003 - val_accuracy: 0.3462\n",
      "Epoch 704/1000\n",
      "104/104 [==============================] - 0s 548us/step - loss: 0.5491 - accuracy: 0.7115 - val_loss: 0.8103 - val_accuracy: 0.3077\n",
      "Epoch 705/1000\n",
      "104/104 [==============================] - 0s 567us/step - loss: 0.5786 - accuracy: 0.7212 - val_loss: 0.8059 - val_accuracy: 0.3077\n",
      "Epoch 706/1000\n",
      "104/104 [==============================] - 0s 552us/step - loss: 0.6112 - accuracy: 0.6731 - val_loss: 0.8047 - val_accuracy: 0.3077\n",
      "Epoch 707/1000\n",
      "104/104 [==============================] - 0s 579us/step - loss: 0.5789 - accuracy: 0.6538 - val_loss: 0.8107 - val_accuracy: 0.3462\n",
      "Epoch 708/1000\n",
      "104/104 [==============================] - 0s 555us/step - loss: 0.5735 - accuracy: 0.6635 - val_loss: 0.8021 - val_accuracy: 0.3462\n",
      "Epoch 709/1000\n",
      "104/104 [==============================] - 0s 592us/step - loss: 0.5814 - accuracy: 0.6923 - val_loss: 0.8057 - val_accuracy: 0.3077\n",
      "Epoch 710/1000\n",
      "104/104 [==============================] - 0s 565us/step - loss: 0.6242 - accuracy: 0.6442 - val_loss: 0.8045 - val_accuracy: 0.3462\n",
      "Epoch 711/1000\n",
      "104/104 [==============================] - 0s 599us/step - loss: 0.6176 - accuracy: 0.6058 - val_loss: 0.8068 - val_accuracy: 0.3077\n",
      "Epoch 712/1000\n",
      "104/104 [==============================] - 0s 573us/step - loss: 0.6067 - accuracy: 0.6923 - val_loss: 0.8020 - val_accuracy: 0.3077\n",
      "Epoch 713/1000\n",
      "104/104 [==============================] - 0s 571us/step - loss: 0.5881 - accuracy: 0.6538 - val_loss: 0.8027 - val_accuracy: 0.3077\n",
      "Epoch 714/1000\n",
      "104/104 [==============================] - 0s 648us/step - loss: 0.6138 - accuracy: 0.6731 - val_loss: 0.7964 - val_accuracy: 0.3846\n",
      "Epoch 715/1000\n",
      "104/104 [==============================] - 0s 580us/step - loss: 0.5786 - accuracy: 0.6731 - val_loss: 0.8058 - val_accuracy: 0.3462\n",
      "Epoch 716/1000\n",
      "104/104 [==============================] - 0s 606us/step - loss: 0.5652 - accuracy: 0.7788 - val_loss: 0.8141 - val_accuracy: 0.3846\n",
      "Epoch 717/1000\n",
      "104/104 [==============================] - 0s 605us/step - loss: 0.5899 - accuracy: 0.6731 - val_loss: 0.8108 - val_accuracy: 0.3462\n",
      "Epoch 718/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 610us/step - loss: 0.5251 - accuracy: 0.7788 - val_loss: 0.8193 - val_accuracy: 0.3462\n",
      "Epoch 719/1000\n",
      "104/104 [==============================] - 0s 603us/step - loss: 0.5670 - accuracy: 0.6827 - val_loss: 0.8179 - val_accuracy: 0.3462\n",
      "Epoch 720/1000\n",
      "104/104 [==============================] - 0s 594us/step - loss: 0.5740 - accuracy: 0.6635 - val_loss: 0.8227 - val_accuracy: 0.3846\n",
      "Epoch 721/1000\n",
      "104/104 [==============================] - 0s 597us/step - loss: 0.5921 - accuracy: 0.6923 - val_loss: 0.8161 - val_accuracy: 0.3462\n",
      "Epoch 722/1000\n",
      "104/104 [==============================] - 0s 619us/step - loss: 0.5520 - accuracy: 0.7019 - val_loss: 0.8270 - val_accuracy: 0.3462\n",
      "Epoch 723/1000\n",
      "104/104 [==============================] - 0s 677us/step - loss: 0.5338 - accuracy: 0.7404 - val_loss: 0.8360 - val_accuracy: 0.3846\n",
      "Epoch 724/1000\n",
      "104/104 [==============================] - 0s 841us/step - loss: 0.5448 - accuracy: 0.6827 - val_loss: 0.8485 - val_accuracy: 0.3846\n",
      "Epoch 725/1000\n",
      "104/104 [==============================] - 0s 629us/step - loss: 0.5507 - accuracy: 0.6538 - val_loss: 0.8450 - val_accuracy: 0.4231\n",
      "Epoch 726/1000\n",
      "104/104 [==============================] - 0s 620us/step - loss: 0.5479 - accuracy: 0.7404 - val_loss: 0.8532 - val_accuracy: 0.3846\n",
      "Epoch 727/1000\n",
      "104/104 [==============================] - 0s 619us/step - loss: 0.5571 - accuracy: 0.6731 - val_loss: 0.8461 - val_accuracy: 0.3846\n",
      "Epoch 728/1000\n",
      "104/104 [==============================] - 0s 612us/step - loss: 0.5967 - accuracy: 0.6827 - val_loss: 0.8323 - val_accuracy: 0.3462\n",
      "Epoch 729/1000\n",
      "104/104 [==============================] - 0s 682us/step - loss: 0.5840 - accuracy: 0.6923 - val_loss: 0.8358 - val_accuracy: 0.3462\n",
      "Epoch 730/1000\n",
      "104/104 [==============================] - 0s 652us/step - loss: 0.5790 - accuracy: 0.6442 - val_loss: 0.8355 - val_accuracy: 0.3462\n",
      "Epoch 731/1000\n",
      "104/104 [==============================] - 0s 666us/step - loss: 0.5770 - accuracy: 0.6731 - val_loss: 0.8354 - val_accuracy: 0.3846\n",
      "Epoch 732/1000\n",
      "104/104 [==============================] - 0s 628us/step - loss: 0.5393 - accuracy: 0.7308 - val_loss: 0.8606 - val_accuracy: 0.3462\n",
      "Epoch 733/1000\n",
      "104/104 [==============================] - 0s 641us/step - loss: 0.5930 - accuracy: 0.7019 - val_loss: 0.8570 - val_accuracy: 0.3462\n",
      "Epoch 734/1000\n",
      "104/104 [==============================] - 0s 925us/step - loss: 0.5454 - accuracy: 0.6827 - val_loss: 0.8628 - val_accuracy: 0.3077\n",
      "Epoch 735/1000\n",
      "104/104 [==============================] - 0s 587us/step - loss: 0.5717 - accuracy: 0.7115 - val_loss: 0.8708 - val_accuracy: 0.3077\n",
      "Epoch 736/1000\n",
      "104/104 [==============================] - 0s 673us/step - loss: 0.5662 - accuracy: 0.7596 - val_loss: 0.8644 - val_accuracy: 0.3077\n",
      "Epoch 737/1000\n",
      "104/104 [==============================] - 0s 615us/step - loss: 0.5797 - accuracy: 0.7596 - val_loss: 0.8613 - val_accuracy: 0.3462\n",
      "Epoch 738/1000\n",
      "104/104 [==============================] - 0s 598us/step - loss: 0.5829 - accuracy: 0.6635 - val_loss: 0.8565 - val_accuracy: 0.3846\n",
      "Epoch 739/1000\n",
      "104/104 [==============================] - 0s 635us/step - loss: 0.5404 - accuracy: 0.7404 - val_loss: 0.8731 - val_accuracy: 0.3846\n",
      "Epoch 740/1000\n",
      "104/104 [==============================] - 0s 636us/step - loss: 0.6074 - accuracy: 0.6442 - val_loss: 0.8140 - val_accuracy: 0.3846\n",
      "Epoch 741/1000\n",
      "104/104 [==============================] - 0s 656us/step - loss: 0.5300 - accuracy: 0.7500 - val_loss: 0.8293 - val_accuracy: 0.3462\n",
      "Epoch 742/1000\n",
      "104/104 [==============================] - 0s 579us/step - loss: 0.5320 - accuracy: 0.7115 - val_loss: 0.8352 - val_accuracy: 0.3462\n",
      "Epoch 743/1000\n",
      "104/104 [==============================] - 0s 596us/step - loss: 0.5793 - accuracy: 0.6635 - val_loss: 0.8349 - val_accuracy: 0.3462\n",
      "Epoch 744/1000\n",
      "104/104 [==============================] - 0s 576us/step - loss: 0.5861 - accuracy: 0.6442 - val_loss: 0.8318 - val_accuracy: 0.3462\n",
      "Epoch 745/1000\n",
      "104/104 [==============================] - 0s 558us/step - loss: 0.5606 - accuracy: 0.6827 - val_loss: 0.8282 - val_accuracy: 0.3462\n",
      "Epoch 746/1000\n",
      "104/104 [==============================] - 0s 550us/step - loss: 0.6148 - accuracy: 0.6731 - val_loss: 0.8219 - val_accuracy: 0.3846\n",
      "Epoch 747/1000\n",
      "104/104 [==============================] - 0s 550us/step - loss: 0.5706 - accuracy: 0.6827 - val_loss: 0.8243 - val_accuracy: 0.4231\n",
      "Epoch 748/1000\n",
      "104/104 [==============================] - 0s 574us/step - loss: 0.5781 - accuracy: 0.6827 - val_loss: 0.8272 - val_accuracy: 0.3846\n",
      "Epoch 749/1000\n",
      "104/104 [==============================] - 0s 582us/step - loss: 0.6088 - accuracy: 0.6442 - val_loss: 0.8295 - val_accuracy: 0.3846\n",
      "Epoch 750/1000\n",
      "104/104 [==============================] - 0s 580us/step - loss: 0.5120 - accuracy: 0.7404 - val_loss: 0.8418 - val_accuracy: 0.3462\n",
      "Epoch 751/1000\n",
      "104/104 [==============================] - 0s 548us/step - loss: 0.5361 - accuracy: 0.7019 - val_loss: 0.8526 - val_accuracy: 0.3846\n",
      "Epoch 752/1000\n",
      "104/104 [==============================] - 0s 661us/step - loss: 0.5459 - accuracy: 0.7404 - val_loss: 0.8606 - val_accuracy: 0.3462\n",
      "Epoch 753/1000\n",
      "104/104 [==============================] - 0s 605us/step - loss: 0.5645 - accuracy: 0.7308 - val_loss: 0.8573 - val_accuracy: 0.3462\n",
      "Epoch 754/1000\n",
      "104/104 [==============================] - 0s 591us/step - loss: 0.5775 - accuracy: 0.6635 - val_loss: 0.8491 - val_accuracy: 0.3462\n",
      "Epoch 755/1000\n",
      "104/104 [==============================] - 0s 578us/step - loss: 0.5318 - accuracy: 0.7596 - val_loss: 0.8521 - val_accuracy: 0.3846\n",
      "Epoch 756/1000\n",
      "104/104 [==============================] - 0s 575us/step - loss: 0.5385 - accuracy: 0.7308 - val_loss: 0.8599 - val_accuracy: 0.3462\n",
      "Epoch 757/1000\n",
      "104/104 [==============================] - 0s 550us/step - loss: 0.5586 - accuracy: 0.7019 - val_loss: 0.8418 - val_accuracy: 0.3462\n",
      "Epoch 758/1000\n",
      "104/104 [==============================] - 0s 560us/step - loss: 0.5582 - accuracy: 0.7115 - val_loss: 0.8298 - val_accuracy: 0.3846\n",
      "Epoch 759/1000\n",
      "104/104 [==============================] - 0s 564us/step - loss: 0.5837 - accuracy: 0.6923 - val_loss: 0.8445 - val_accuracy: 0.3462\n",
      "Epoch 760/1000\n",
      "104/104 [==============================] - 0s 599us/step - loss: 0.5440 - accuracy: 0.7212 - val_loss: 0.8532 - val_accuracy: 0.3462\n",
      "Epoch 761/1000\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.4919 - accuracy: 0.75 - 0s 648us/step - loss: 0.5633 - accuracy: 0.7308 - val_loss: 0.8501 - val_accuracy: 0.3462\n",
      "Epoch 762/1000\n",
      "104/104 [==============================] - 0s 561us/step - loss: 0.5680 - accuracy: 0.6827 - val_loss: 0.8460 - val_accuracy: 0.3846\n",
      "Epoch 763/1000\n",
      "104/104 [==============================] - 0s 566us/step - loss: 0.5552 - accuracy: 0.6827 - val_loss: 0.8514 - val_accuracy: 0.3462\n",
      "Epoch 764/1000\n",
      "104/104 [==============================] - 0s 537us/step - loss: 0.5393 - accuracy: 0.6923 - val_loss: 0.8561 - val_accuracy: 0.3462\n",
      "Epoch 765/1000\n",
      "104/104 [==============================] - 0s 541us/step - loss: 0.5639 - accuracy: 0.7404 - val_loss: 0.8527 - val_accuracy: 0.3462\n",
      "Epoch 766/1000\n",
      "104/104 [==============================] - 0s 551us/step - loss: 0.5839 - accuracy: 0.7500 - val_loss: 0.8568 - val_accuracy: 0.3462\n",
      "Epoch 767/1000\n",
      "104/104 [==============================] - 0s 581us/step - loss: 0.5727 - accuracy: 0.6827 - val_loss: 0.8632 - val_accuracy: 0.3462\n",
      "Epoch 768/1000\n",
      "104/104 [==============================] - 0s 583us/step - loss: 0.5603 - accuracy: 0.7115 - val_loss: 0.8550 - val_accuracy: 0.3462\n",
      "Epoch 769/1000\n",
      "104/104 [==============================] - 0s 582us/step - loss: 0.5582 - accuracy: 0.7212 - val_loss: 0.8672 - val_accuracy: 0.3462\n",
      "Epoch 770/1000\n",
      "104/104 [==============================] - 0s 550us/step - loss: 0.5672 - accuracy: 0.6635 - val_loss: 0.8613 - val_accuracy: 0.3462\n",
      "Epoch 771/1000\n",
      "104/104 [==============================] - 0s 597us/step - loss: 0.5678 - accuracy: 0.7115 - val_loss: 0.8688 - val_accuracy: 0.3462\n",
      "Epoch 772/1000\n",
      "104/104 [==============================] - 0s 595us/step - loss: 0.5301 - accuracy: 0.7308 - val_loss: 0.8728 - val_accuracy: 0.3462\n",
      "Epoch 773/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 593us/step - loss: 0.5512 - accuracy: 0.7212 - val_loss: 0.8698 - val_accuracy: 0.3462\n",
      "Epoch 774/1000\n",
      "104/104 [==============================] - 0s 612us/step - loss: 0.5234 - accuracy: 0.7596 - val_loss: 0.8872 - val_accuracy: 0.3846\n",
      "Epoch 775/1000\n",
      "104/104 [==============================] - 0s 578us/step - loss: 0.5572 - accuracy: 0.7500 - val_loss: 0.8830 - val_accuracy: 0.3846\n",
      "Epoch 776/1000\n",
      "104/104 [==============================] - 0s 575us/step - loss: 0.5129 - accuracy: 0.7500 - val_loss: 0.8888 - val_accuracy: 0.3846\n",
      "Epoch 777/1000\n",
      "104/104 [==============================] - 0s 612us/step - loss: 0.5750 - accuracy: 0.7115 - val_loss: 0.8922 - val_accuracy: 0.3462\n",
      "Epoch 778/1000\n",
      "104/104 [==============================] - 0s 577us/step - loss: 0.5774 - accuracy: 0.6827 - val_loss: 0.8895 - val_accuracy: 0.3462\n",
      "Epoch 779/1000\n",
      "104/104 [==============================] - 0s 580us/step - loss: 0.5537 - accuracy: 0.7500 - val_loss: 0.8708 - val_accuracy: 0.3462\n",
      "Epoch 780/1000\n",
      "104/104 [==============================] - 0s 589us/step - loss: 0.5639 - accuracy: 0.7115 - val_loss: 0.8881 - val_accuracy: 0.3462\n",
      "Epoch 781/1000\n",
      "104/104 [==============================] - 0s 607us/step - loss: 0.5109 - accuracy: 0.7308 - val_loss: 0.8697 - val_accuracy: 0.3077\n",
      "Epoch 782/1000\n",
      "104/104 [==============================] - 0s 556us/step - loss: 0.5171 - accuracy: 0.7500 - val_loss: 0.8753 - val_accuracy: 0.3077\n",
      "Epoch 783/1000\n",
      "104/104 [==============================] - 0s 561us/step - loss: 0.5974 - accuracy: 0.6346 - val_loss: 0.8786 - val_accuracy: 0.3077\n",
      "Epoch 784/1000\n",
      "104/104 [==============================] - 0s 554us/step - loss: 0.5821 - accuracy: 0.6731 - val_loss: 0.8710 - val_accuracy: 0.3462\n",
      "Epoch 785/1000\n",
      "104/104 [==============================] - 0s 710us/step - loss: 0.5472 - accuracy: 0.6731 - val_loss: 0.8806 - val_accuracy: 0.3077\n",
      "Epoch 786/1000\n",
      "104/104 [==============================] - 0s 546us/step - loss: 0.5539 - accuracy: 0.7212 - val_loss: 0.8848 - val_accuracy: 0.3462\n",
      "Epoch 787/1000\n",
      "104/104 [==============================] - 0s 583us/step - loss: 0.5112 - accuracy: 0.7115 - val_loss: 0.9008 - val_accuracy: 0.3077\n",
      "Epoch 788/1000\n",
      "104/104 [==============================] - 0s 920us/step - loss: 0.5533 - accuracy: 0.6923 - val_loss: 0.8968 - val_accuracy: 0.3846\n",
      "Epoch 789/1000\n",
      "104/104 [==============================] - 0s 524us/step - loss: 0.5675 - accuracy: 0.7308 - val_loss: 0.8888 - val_accuracy: 0.3846\n",
      "Epoch 790/1000\n",
      "104/104 [==============================] - 0s 527us/step - loss: 0.5346 - accuracy: 0.7596 - val_loss: 0.9099 - val_accuracy: 0.3462\n",
      "Epoch 791/1000\n",
      "104/104 [==============================] - 0s 551us/step - loss: 0.5739 - accuracy: 0.6923 - val_loss: 0.9000 - val_accuracy: 0.3462\n",
      "Epoch 792/1000\n",
      "104/104 [==============================] - 0s 522us/step - loss: 0.6060 - accuracy: 0.6154 - val_loss: 0.8899 - val_accuracy: 0.3462\n",
      "Epoch 793/1000\n",
      "104/104 [==============================] - 0s 534us/step - loss: 0.5381 - accuracy: 0.7019 - val_loss: 0.9040 - val_accuracy: 0.3846\n",
      "Epoch 794/1000\n",
      "104/104 [==============================] - 0s 539us/step - loss: 0.5497 - accuracy: 0.7019 - val_loss: 0.8849 - val_accuracy: 0.3462\n",
      "Epoch 795/1000\n",
      "104/104 [==============================] - 0s 540us/step - loss: 0.5328 - accuracy: 0.6731 - val_loss: 0.9069 - val_accuracy: 0.3462\n",
      "Epoch 796/1000\n",
      "104/104 [==============================] - 0s 548us/step - loss: 0.5313 - accuracy: 0.7212 - val_loss: 0.9152 - val_accuracy: 0.3462\n",
      "Epoch 797/1000\n",
      "104/104 [==============================] - 0s 567us/step - loss: 0.5404 - accuracy: 0.7500 - val_loss: 0.9037 - val_accuracy: 0.3462\n",
      "Epoch 798/1000\n",
      "104/104 [==============================] - 0s 547us/step - loss: 0.5120 - accuracy: 0.7212 - val_loss: 0.9203 - val_accuracy: 0.3462\n",
      "Epoch 799/1000\n",
      "104/104 [==============================] - 0s 604us/step - loss: 0.5330 - accuracy: 0.7115 - val_loss: 0.9217 - val_accuracy: 0.3462\n",
      "Epoch 800/1000\n",
      "104/104 [==============================] - 0s 601us/step - loss: 0.5251 - accuracy: 0.7019 - val_loss: 0.9201 - val_accuracy: 0.3462\n",
      "Epoch 801/1000\n",
      "104/104 [==============================] - 0s 423us/step - loss: 0.5614 - accuracy: 0.7212 - val_loss: 0.8878 - val_accuracy: 0.3846\n",
      "Epoch 802/1000\n",
      "104/104 [==============================] - 0s 435us/step - loss: 0.5907 - accuracy: 0.6635 - val_loss: 0.8857 - val_accuracy: 0.3846\n",
      "Epoch 803/1000\n",
      "104/104 [==============================] - 0s 536us/step - loss: 0.5533 - accuracy: 0.6923 - val_loss: 0.8976 - val_accuracy: 0.3846\n",
      "Epoch 804/1000\n",
      "104/104 [==============================] - 0s 454us/step - loss: 0.5230 - accuracy: 0.7308 - val_loss: 0.9131 - val_accuracy: 0.3846\n",
      "Epoch 805/1000\n",
      "104/104 [==============================] - 0s 603us/step - loss: 0.5382 - accuracy: 0.7019 - val_loss: 0.9082 - val_accuracy: 0.3846\n",
      "Epoch 806/1000\n",
      "104/104 [==============================] - 0s 625us/step - loss: 0.5201 - accuracy: 0.7115 - val_loss: 0.9058 - val_accuracy: 0.3846\n",
      "Epoch 807/1000\n",
      "104/104 [==============================] - 0s 638us/step - loss: 0.5999 - accuracy: 0.6250 - val_loss: 0.8917 - val_accuracy: 0.4231\n",
      "Epoch 808/1000\n",
      "104/104 [==============================] - 0s 610us/step - loss: 0.5521 - accuracy: 0.7404 - val_loss: 0.8929 - val_accuracy: 0.3846\n",
      "Epoch 809/1000\n",
      "104/104 [==============================] - 0s 605us/step - loss: 0.5789 - accuracy: 0.6538 - val_loss: 0.8949 - val_accuracy: 0.3846\n",
      "Epoch 810/1000\n",
      "104/104 [==============================] - 0s 636us/step - loss: 0.5189 - accuracy: 0.7115 - val_loss: 0.9082 - val_accuracy: 0.4231\n",
      "Epoch 811/1000\n",
      "104/104 [==============================] - 0s 641us/step - loss: 0.5427 - accuracy: 0.7308 - val_loss: 0.8918 - val_accuracy: 0.4231\n",
      "Epoch 812/1000\n",
      "104/104 [==============================] - 0s 593us/step - loss: 0.5314 - accuracy: 0.7596 - val_loss: 0.8888 - val_accuracy: 0.3462\n",
      "Epoch 813/1000\n",
      "104/104 [==============================] - 0s 622us/step - loss: 0.4806 - accuracy: 0.7885 - val_loss: 0.9115 - val_accuracy: 0.3462\n",
      "Epoch 814/1000\n",
      "104/104 [==============================] - 0s 596us/step - loss: 0.5469 - accuracy: 0.7308 - val_loss: 0.9252 - val_accuracy: 0.3846\n",
      "Epoch 815/1000\n",
      "104/104 [==============================] - 0s 574us/step - loss: 0.5492 - accuracy: 0.7019 - val_loss: 0.8931 - val_accuracy: 0.3846\n",
      "Epoch 816/1000\n",
      "104/104 [==============================] - 0s 618us/step - loss: 0.5173 - accuracy: 0.7596 - val_loss: 0.9040 - val_accuracy: 0.3846\n",
      "Epoch 817/1000\n",
      "104/104 [==============================] - 0s 625us/step - loss: 0.5506 - accuracy: 0.7500 - val_loss: 0.9032 - val_accuracy: 0.3846\n",
      "Epoch 818/1000\n",
      "104/104 [==============================] - 0s 594us/step - loss: 0.5168 - accuracy: 0.7596 - val_loss: 0.8962 - val_accuracy: 0.4231\n",
      "Epoch 819/1000\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.5841 - accuracy: 0.62 - 0s 601us/step - loss: 0.5530 - accuracy: 0.6731 - val_loss: 0.8965 - val_accuracy: 0.3846\n",
      "Epoch 820/1000\n",
      "104/104 [==============================] - 0s 607us/step - loss: 0.5579 - accuracy: 0.7019 - val_loss: 0.8972 - val_accuracy: 0.4231\n",
      "Epoch 821/1000\n",
      "104/104 [==============================] - 0s 698us/step - loss: 0.4984 - accuracy: 0.7596 - val_loss: 0.9028 - val_accuracy: 0.3462\n",
      "Epoch 822/1000\n",
      "104/104 [==============================] - 0s 647us/step - loss: 0.5329 - accuracy: 0.7404 - val_loss: 0.8919 - val_accuracy: 0.3846\n",
      "Epoch 823/1000\n",
      "104/104 [==============================] - 0s 616us/step - loss: 0.5088 - accuracy: 0.7212 - val_loss: 0.8957 - val_accuracy: 0.3462\n",
      "Epoch 824/1000\n",
      "104/104 [==============================] - 0s 604us/step - loss: 0.4922 - accuracy: 0.7596 - val_loss: 0.9262 - val_accuracy: 0.3846\n",
      "Epoch 825/1000\n",
      "104/104 [==============================] - 0s 681us/step - loss: 0.5508 - accuracy: 0.7404 - val_loss: 0.9255 - val_accuracy: 0.3462\n",
      "Epoch 826/1000\n",
      "104/104 [==============================] - 0s 639us/step - loss: 0.5193 - accuracy: 0.6827 - val_loss: 0.9310 - val_accuracy: 0.3462\n",
      "Epoch 827/1000\n",
      "104/104 [==============================] - 0s 577us/step - loss: 0.5729 - accuracy: 0.7115 - val_loss: 0.9340 - val_accuracy: 0.3846\n",
      "Epoch 828/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 542us/step - loss: 0.4937 - accuracy: 0.7500 - val_loss: 0.9398 - val_accuracy: 0.3462\n",
      "Epoch 829/1000\n",
      "104/104 [==============================] - 0s 628us/step - loss: 0.5115 - accuracy: 0.7115 - val_loss: 0.9317 - val_accuracy: 0.3846\n",
      "Epoch 830/1000\n",
      "104/104 [==============================] - 0s 557us/step - loss: 0.5389 - accuracy: 0.7308 - val_loss: 0.9361 - val_accuracy: 0.3462\n",
      "Epoch 831/1000\n",
      "104/104 [==============================] - 0s 567us/step - loss: 0.5257 - accuracy: 0.7308 - val_loss: 0.9170 - val_accuracy: 0.4231\n",
      "Epoch 832/1000\n",
      "104/104 [==============================] - 0s 616us/step - loss: 0.5323 - accuracy: 0.7404 - val_loss: 0.9375 - val_accuracy: 0.4231\n",
      "Epoch 833/1000\n",
      "104/104 [==============================] - 0s 577us/step - loss: 0.5185 - accuracy: 0.6923 - val_loss: 0.9365 - val_accuracy: 0.3846\n",
      "Epoch 834/1000\n",
      "104/104 [==============================] - 0s 590us/step - loss: 0.5262 - accuracy: 0.7596 - val_loss: 0.9402 - val_accuracy: 0.3462\n",
      "Epoch 835/1000\n",
      "104/104 [==============================] - 0s 631us/step - loss: 0.5624 - accuracy: 0.6827 - val_loss: 0.9725 - val_accuracy: 0.3462\n",
      "Epoch 836/1000\n",
      "104/104 [==============================] - 0s 620us/step - loss: 0.5426 - accuracy: 0.6538 - val_loss: 0.9458 - val_accuracy: 0.3462\n",
      "Epoch 837/1000\n",
      "104/104 [==============================] - 0s 659us/step - loss: 0.5543 - accuracy: 0.6346 - val_loss: 0.9547 - val_accuracy: 0.3846\n",
      "Epoch 838/1000\n",
      "104/104 [==============================] - 0s 630us/step - loss: 0.5528 - accuracy: 0.6923 - val_loss: 0.9846 - val_accuracy: 0.3077\n",
      "Epoch 839/1000\n",
      "104/104 [==============================] - 0s 595us/step - loss: 0.5469 - accuracy: 0.7115 - val_loss: 0.9932 - val_accuracy: 0.2692\n",
      "Epoch 840/1000\n",
      "104/104 [==============================] - 0s 606us/step - loss: 0.5331 - accuracy: 0.7212 - val_loss: 0.9661 - val_accuracy: 0.3462\n",
      "Epoch 841/1000\n",
      "104/104 [==============================] - 0s 607us/step - loss: 0.5095 - accuracy: 0.7212 - val_loss: 0.9856 - val_accuracy: 0.3077\n",
      "Epoch 842/1000\n",
      "104/104 [==============================] - 0s 614us/step - loss: 0.5745 - accuracy: 0.7308 - val_loss: 0.8932 - val_accuracy: 0.4231\n",
      "Epoch 843/1000\n",
      "104/104 [==============================] - 0s 815us/step - loss: 0.5256 - accuracy: 0.7212 - val_loss: 0.8978 - val_accuracy: 0.4231\n",
      "Epoch 844/1000\n",
      "104/104 [==============================] - 0s 564us/step - loss: 0.5269 - accuracy: 0.7115 - val_loss: 0.9079 - val_accuracy: 0.3846\n",
      "Epoch 845/1000\n",
      "104/104 [==============================] - 0s 606us/step - loss: 0.5369 - accuracy: 0.7115 - val_loss: 0.9220 - val_accuracy: 0.3077\n",
      "Epoch 846/1000\n",
      "104/104 [==============================] - 0s 615us/step - loss: 0.5400 - accuracy: 0.7115 - val_loss: 0.9504 - val_accuracy: 0.3462\n",
      "Epoch 847/1000\n",
      "104/104 [==============================] - 0s 637us/step - loss: 0.5246 - accuracy: 0.7019 - val_loss: 0.9357 - val_accuracy: 0.3077\n",
      "Epoch 848/1000\n",
      "104/104 [==============================] - 0s 615us/step - loss: 0.5281 - accuracy: 0.7019 - val_loss: 0.9354 - val_accuracy: 0.3077\n",
      "Epoch 849/1000\n",
      "104/104 [==============================] - 0s 598us/step - loss: 0.5065 - accuracy: 0.7115 - val_loss: 0.9557 - val_accuracy: 0.3462\n",
      "Epoch 850/1000\n",
      "104/104 [==============================] - 0s 672us/step - loss: 0.5036 - accuracy: 0.7500 - val_loss: 0.9529 - val_accuracy: 0.3462\n",
      "Epoch 851/1000\n",
      "104/104 [==============================] - 0s 648us/step - loss: 0.4887 - accuracy: 0.7404 - val_loss: 0.9662 - val_accuracy: 0.3462\n",
      "Epoch 852/1000\n",
      "104/104 [==============================] - 0s 573us/step - loss: 0.5503 - accuracy: 0.6538 - val_loss: 0.9688 - val_accuracy: 0.3462\n",
      "Epoch 853/1000\n",
      "104/104 [==============================] - 0s 598us/step - loss: 0.4935 - accuracy: 0.7692 - val_loss: 0.9720 - val_accuracy: 0.3462\n",
      "Epoch 854/1000\n",
      "104/104 [==============================] - 0s 636us/step - loss: 0.5128 - accuracy: 0.7404 - val_loss: 0.9909 - val_accuracy: 0.3462\n",
      "Epoch 855/1000\n",
      "104/104 [==============================] - 0s 632us/step - loss: 0.5142 - accuracy: 0.7115 - val_loss: 0.9929 - val_accuracy: 0.3462\n",
      "Epoch 856/1000\n",
      "104/104 [==============================] - 0s 609us/step - loss: 0.5358 - accuracy: 0.6923 - val_loss: 1.0024 - val_accuracy: 0.3846\n",
      "Epoch 857/1000\n",
      "104/104 [==============================] - 0s 645us/step - loss: 0.4913 - accuracy: 0.7692 - val_loss: 1.0065 - val_accuracy: 0.3846\n",
      "Epoch 858/1000\n",
      "104/104 [==============================] - 0s 719us/step - loss: 0.5301 - accuracy: 0.7692 - val_loss: 0.9902 - val_accuracy: 0.3462\n",
      "Epoch 859/1000\n",
      "104/104 [==============================] - 0s 709us/step - loss: 0.5293 - accuracy: 0.7212 - val_loss: 0.9851 - val_accuracy: 0.3462\n",
      "Epoch 860/1000\n",
      "104/104 [==============================] - 0s 806us/step - loss: 0.4887 - accuracy: 0.7596 - val_loss: 1.0031 - val_accuracy: 0.3846\n",
      "Epoch 861/1000\n",
      "104/104 [==============================] - 0s 642us/step - loss: 0.6156 - accuracy: 0.6731 - val_loss: 0.9704 - val_accuracy: 0.3462\n",
      "Epoch 862/1000\n",
      "104/104 [==============================] - 0s 672us/step - loss: 0.5138 - accuracy: 0.7500 - val_loss: 0.9812 - val_accuracy: 0.3462\n",
      "Epoch 863/1000\n",
      "104/104 [==============================] - 0s 603us/step - loss: 0.5182 - accuracy: 0.7500 - val_loss: 0.9922 - val_accuracy: 0.3846\n",
      "Epoch 864/1000\n",
      "104/104 [==============================] - 0s 686us/step - loss: 0.4644 - accuracy: 0.7981 - val_loss: 0.9606 - val_accuracy: 0.3462\n",
      "Epoch 865/1000\n",
      "104/104 [==============================] - 0s 637us/step - loss: 0.5370 - accuracy: 0.7404 - val_loss: 0.9645 - val_accuracy: 0.3462\n",
      "Epoch 866/1000\n",
      "104/104 [==============================] - 0s 618us/step - loss: 0.5127 - accuracy: 0.7404 - val_loss: 0.9820 - val_accuracy: 0.3462\n",
      "Epoch 867/1000\n",
      "104/104 [==============================] - 0s 664us/step - loss: 0.5118 - accuracy: 0.7115 - val_loss: 0.9963 - val_accuracy: 0.3462\n",
      "Epoch 868/1000\n",
      "104/104 [==============================] - 0s 613us/step - loss: 0.4997 - accuracy: 0.7788 - val_loss: 0.9966 - val_accuracy: 0.3846\n",
      "Epoch 869/1000\n",
      "104/104 [==============================] - 0s 631us/step - loss: 0.4771 - accuracy: 0.7596 - val_loss: 1.0033 - val_accuracy: 0.3462\n",
      "Epoch 870/1000\n",
      "104/104 [==============================] - 0s 532us/step - loss: 0.5483 - accuracy: 0.7115 - val_loss: 0.9966 - val_accuracy: 0.3846\n",
      "Epoch 871/1000\n",
      "104/104 [==============================] - 0s 558us/step - loss: 0.5022 - accuracy: 0.7500 - val_loss: 0.9664 - val_accuracy: 0.4231\n",
      "Epoch 872/1000\n",
      "104/104 [==============================] - 0s 591us/step - loss: 0.5128 - accuracy: 0.7404 - val_loss: 0.9739 - val_accuracy: 0.3846\n",
      "Epoch 873/1000\n",
      "104/104 [==============================] - 0s 594us/step - loss: 0.5405 - accuracy: 0.7115 - val_loss: 0.9573 - val_accuracy: 0.3846\n",
      "Epoch 874/1000\n",
      "104/104 [==============================] - 0s 559us/step - loss: 0.5701 - accuracy: 0.6058 - val_loss: 0.9707 - val_accuracy: 0.3846\n",
      "Epoch 875/1000\n",
      "104/104 [==============================] - 0s 570us/step - loss: 0.4961 - accuracy: 0.7500 - val_loss: 0.9867 - val_accuracy: 0.4231\n",
      "Epoch 876/1000\n",
      "104/104 [==============================] - 0s 545us/step - loss: 0.5125 - accuracy: 0.7115 - val_loss: 0.9979 - val_accuracy: 0.3846\n",
      "Epoch 877/1000\n",
      "104/104 [==============================] - 0s 558us/step - loss: 0.5012 - accuracy: 0.7692 - val_loss: 0.9981 - val_accuracy: 0.3846\n",
      "Epoch 878/1000\n",
      "104/104 [==============================] - 0s 492us/step - loss: 0.5372 - accuracy: 0.7115 - val_loss: 1.0072 - val_accuracy: 0.3462\n",
      "Epoch 879/1000\n",
      "104/104 [==============================] - 0s 965us/step - loss: 0.4808 - accuracy: 0.7500 - val_loss: 0.9998 - val_accuracy: 0.3462\n",
      "Epoch 880/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4792 - accuracy: 0.7404 - val_loss: 1.0072 - val_accuracy: 0.3462\n",
      "Epoch 881/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.5095 - accuracy: 0.7596 - val_loss: 1.0369 - val_accuracy: 0.3462\n",
      "Epoch 882/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.5021 - accuracy: 0.6827 - val_loss: 1.0491 - val_accuracy: 0.3462\n",
      "Epoch 883/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4793 - accuracy: 0.7500 - val_loss: 1.0324 - val_accuracy: 0.3462\n",
      "Epoch 884/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.5258 - accuracy: 0.7404 - val_loss: 1.0217 - val_accuracy: 0.3462\n",
      "Epoch 885/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.5408 - accuracy: 0.7308 - val_loss: 1.0154 - val_accuracy: 0.3462\n",
      "Epoch 886/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4961 - accuracy: 0.7212 - val_loss: 1.0189 - val_accuracy: 0.3846\n",
      "Epoch 887/1000\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4865 - accuracy: 0.7212 - val_loss: 1.0301 - val_accuracy: 0.3846\n",
      "Epoch 888/1000\n",
      "104/104 [==============================] - 0s 590us/step - loss: 0.4929 - accuracy: 0.7500 - val_loss: 1.0305 - val_accuracy: 0.3462\n",
      "Epoch 889/1000\n",
      "104/104 [==============================] - 0s 596us/step - loss: 0.5361 - accuracy: 0.7115 - val_loss: 0.9111 - val_accuracy: 0.4231\n",
      "Epoch 890/1000\n",
      "104/104 [==============================] - 0s 501us/step - loss: 0.5117 - accuracy: 0.7115 - val_loss: 0.9214 - val_accuracy: 0.4231\n",
      "Epoch 891/1000\n",
      "104/104 [==============================] - 0s 504us/step - loss: 0.4693 - accuracy: 0.7596 - val_loss: 0.9325 - val_accuracy: 0.3462\n",
      "Epoch 892/1000\n",
      "104/104 [==============================] - 0s 458us/step - loss: 0.5109 - accuracy: 0.7404 - val_loss: 0.9554 - val_accuracy: 0.3846\n",
      "Epoch 893/1000\n",
      "104/104 [==============================] - 0s 496us/step - loss: 0.5187 - accuracy: 0.7115 - val_loss: 0.9566 - val_accuracy: 0.3846\n",
      "Epoch 894/1000\n",
      "104/104 [==============================] - 0s 508us/step - loss: 0.4891 - accuracy: 0.7981 - val_loss: 0.9764 - val_accuracy: 0.3846\n",
      "Epoch 895/1000\n",
      "104/104 [==============================] - 0s 528us/step - loss: 0.5530 - accuracy: 0.7115 - val_loss: 0.9866 - val_accuracy: 0.3462\n",
      "Epoch 896/1000\n",
      "104/104 [==============================] - 0s 519us/step - loss: 0.4844 - accuracy: 0.7692 - val_loss: 0.9875 - val_accuracy: 0.3077\n",
      "Epoch 897/1000\n",
      "104/104 [==============================] - 0s 556us/step - loss: 0.4909 - accuracy: 0.7500 - val_loss: 0.9989 - val_accuracy: 0.3077\n",
      "Epoch 898/1000\n",
      "104/104 [==============================] - 0s 547us/step - loss: 0.5484 - accuracy: 0.7212 - val_loss: 1.0329 - val_accuracy: 0.3462\n",
      "Epoch 899/1000\n",
      "104/104 [==============================] - 0s 510us/step - loss: 0.4922 - accuracy: 0.6923 - val_loss: 1.0260 - val_accuracy: 0.3462\n",
      "Epoch 900/1000\n",
      "104/104 [==============================] - 0s 512us/step - loss: 0.4855 - accuracy: 0.7788 - val_loss: 1.0411 - val_accuracy: 0.3077\n",
      "Epoch 901/1000\n",
      "104/104 [==============================] - 0s 495us/step - loss: 0.5034 - accuracy: 0.7500 - val_loss: 1.0185 - val_accuracy: 0.3462\n",
      "Epoch 902/1000\n",
      "104/104 [==============================] - 0s 491us/step - loss: 0.5097 - accuracy: 0.7115 - val_loss: 1.0252 - val_accuracy: 0.3462\n",
      "Epoch 903/1000\n",
      "104/104 [==============================] - 0s 492us/step - loss: 0.5897 - accuracy: 0.6827 - val_loss: 1.0140 - val_accuracy: 0.3462\n",
      "Epoch 904/1000\n",
      "104/104 [==============================] - 0s 433us/step - loss: 0.4839 - accuracy: 0.7788 - val_loss: 1.0037 - val_accuracy: 0.3462\n",
      "Epoch 905/1000\n",
      "104/104 [==============================] - 0s 559us/step - loss: 0.5423 - accuracy: 0.6923 - val_loss: 1.0116 - val_accuracy: 0.3462\n",
      "Epoch 906/1000\n",
      "104/104 [==============================] - 0s 571us/step - loss: 0.4484 - accuracy: 0.7885 - val_loss: 1.0257 - val_accuracy: 0.3462\n",
      "Epoch 907/1000\n",
      "104/104 [==============================] - 0s 544us/step - loss: 0.5288 - accuracy: 0.6538 - val_loss: 1.0216 - val_accuracy: 0.3462\n",
      "Epoch 908/1000\n",
      "104/104 [==============================] - 0s 511us/step - loss: 0.4810 - accuracy: 0.8173 - val_loss: 1.0344 - val_accuracy: 0.3462\n",
      "Epoch 909/1000\n",
      "104/104 [==============================] - 0s 456us/step - loss: 0.5150 - accuracy: 0.7212 - val_loss: 1.0365 - val_accuracy: 0.3462\n",
      "Epoch 910/1000\n",
      "104/104 [==============================] - 0s 501us/step - loss: 0.5084 - accuracy: 0.7308 - val_loss: 1.0334 - val_accuracy: 0.3462\n",
      "Epoch 911/1000\n",
      "104/104 [==============================] - 0s 545us/step - loss: 0.4606 - accuracy: 0.8269 - val_loss: 1.0305 - val_accuracy: 0.3846\n",
      "Epoch 912/1000\n",
      "104/104 [==============================] - 0s 513us/step - loss: 0.4869 - accuracy: 0.7404 - val_loss: 1.0142 - val_accuracy: 0.3846\n",
      "Epoch 913/1000\n",
      "104/104 [==============================] - 0s 516us/step - loss: 0.4911 - accuracy: 0.7404 - val_loss: 1.0167 - val_accuracy: 0.3846\n",
      "Epoch 914/1000\n",
      "104/104 [==============================] - 0s 539us/step - loss: 0.5206 - accuracy: 0.7500 - val_loss: 1.0558 - val_accuracy: 0.3462\n",
      "Epoch 915/1000\n",
      "104/104 [==============================] - 0s 483us/step - loss: 0.5006 - accuracy: 0.7596 - val_loss: 1.0443 - val_accuracy: 0.3462\n",
      "Epoch 916/1000\n",
      "104/104 [==============================] - 0s 572us/step - loss: 0.5258 - accuracy: 0.7115 - val_loss: 1.0467 - val_accuracy: 0.3462\n",
      "Epoch 917/1000\n",
      "104/104 [==============================] - 0s 455us/step - loss: 0.4660 - accuracy: 0.7500 - val_loss: 1.0548 - val_accuracy: 0.3846\n",
      "Epoch 918/1000\n",
      "104/104 [==============================] - 0s 471us/step - loss: 0.4751 - accuracy: 0.7692 - val_loss: 1.0558 - val_accuracy: 0.3846\n",
      "Epoch 919/1000\n",
      "104/104 [==============================] - 0s 555us/step - loss: 0.5058 - accuracy: 0.7788 - val_loss: 1.0740 - val_accuracy: 0.3846\n",
      "Epoch 920/1000\n",
      "104/104 [==============================] - 0s 522us/step - loss: 0.5113 - accuracy: 0.7115 - val_loss: 1.0805 - val_accuracy: 0.3846\n",
      "Epoch 921/1000\n",
      "104/104 [==============================] - 0s 632us/step - loss: 0.5196 - accuracy: 0.7308 - val_loss: 1.0809 - val_accuracy: 0.3846\n",
      "Epoch 922/1000\n",
      "104/104 [==============================] - 0s 575us/step - loss: 0.5038 - accuracy: 0.6923 - val_loss: 1.0963 - val_accuracy: 0.3462\n",
      "Epoch 923/1000\n",
      "104/104 [==============================] - 0s 585us/step - loss: 0.4902 - accuracy: 0.7596 - val_loss: 1.1022 - val_accuracy: 0.3077\n",
      "Epoch 924/1000\n",
      "104/104 [==============================] - 0s 533us/step - loss: 0.4593 - accuracy: 0.7692 - val_loss: 1.0763 - val_accuracy: 0.3462\n",
      "Epoch 925/1000\n",
      "104/104 [==============================] - 0s 546us/step - loss: 0.4807 - accuracy: 0.7788 - val_loss: 1.0850 - val_accuracy: 0.3462\n",
      "Epoch 926/1000\n",
      "104/104 [==============================] - 0s 462us/step - loss: 0.4863 - accuracy: 0.7404 - val_loss: 1.0888 - val_accuracy: 0.3462\n",
      "Epoch 927/1000\n",
      "104/104 [==============================] - 0s 489us/step - loss: 0.4442 - accuracy: 0.7885 - val_loss: 1.0731 - val_accuracy: 0.3846\n",
      "Epoch 928/1000\n",
      "104/104 [==============================] - 0s 486us/step - loss: 0.5404 - accuracy: 0.7308 - val_loss: 1.0497 - val_accuracy: 0.4231\n",
      "Epoch 929/1000\n",
      "104/104 [==============================] - 0s 497us/step - loss: 0.5079 - accuracy: 0.7115 - val_loss: 1.0195 - val_accuracy: 0.3846\n",
      "Epoch 930/1000\n",
      "104/104 [==============================] - 0s 495us/step - loss: 0.4578 - accuracy: 0.7788 - val_loss: 1.0580 - val_accuracy: 0.3462\n",
      "Epoch 931/1000\n",
      "104/104 [==============================] - 0s 540us/step - loss: 0.4433 - accuracy: 0.8077 - val_loss: 1.0562 - val_accuracy: 0.3462\n",
      "Epoch 932/1000\n",
      "104/104 [==============================] - 0s 552us/step - loss: 0.4600 - accuracy: 0.7692 - val_loss: 1.0566 - val_accuracy: 0.3462\n",
      "Epoch 933/1000\n",
      "104/104 [==============================] - 0s 519us/step - loss: 0.5355 - accuracy: 0.7596 - val_loss: 1.0379 - val_accuracy: 0.3846\n",
      "Epoch 934/1000\n",
      "104/104 [==============================] - 0s 559us/step - loss: 0.4464 - accuracy: 0.7404 - val_loss: 1.0725 - val_accuracy: 0.3462\n",
      "Epoch 935/1000\n",
      "104/104 [==============================] - 0s 509us/step - loss: 0.5308 - accuracy: 0.7212 - val_loss: 1.0912 - val_accuracy: 0.3462\n",
      "Epoch 936/1000\n",
      "104/104 [==============================] - 0s 586us/step - loss: 0.4511 - accuracy: 0.8173 - val_loss: 1.0775 - val_accuracy: 0.3846\n",
      "Epoch 937/1000\n",
      "104/104 [==============================] - 0s 526us/step - loss: 0.4808 - accuracy: 0.7212 - val_loss: 1.0799 - val_accuracy: 0.3462\n",
      "Epoch 938/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 557us/step - loss: 0.4973 - accuracy: 0.7981 - val_loss: 0.9994 - val_accuracy: 0.4231\n",
      "Epoch 939/1000\n",
      "104/104 [==============================] - 0s 581us/step - loss: 0.5160 - accuracy: 0.7596 - val_loss: 1.0345 - val_accuracy: 0.3462\n",
      "Epoch 940/1000\n",
      "104/104 [==============================] - 0s 583us/step - loss: 0.5356 - accuracy: 0.7212 - val_loss: 1.0194 - val_accuracy: 0.4231\n",
      "Epoch 941/1000\n",
      "104/104 [==============================] - 0s 496us/step - loss: 0.5237 - accuracy: 0.7115 - val_loss: 1.0330 - val_accuracy: 0.3462\n",
      "Epoch 942/1000\n",
      "104/104 [==============================] - 0s 561us/step - loss: 0.5004 - accuracy: 0.7308 - val_loss: 1.0134 - val_accuracy: 0.3846\n",
      "Epoch 943/1000\n",
      "104/104 [==============================] - 0s 543us/step - loss: 0.5244 - accuracy: 0.8077 - val_loss: 0.9605 - val_accuracy: 0.3846\n",
      "Epoch 944/1000\n",
      "104/104 [==============================] - 0s 518us/step - loss: 0.4886 - accuracy: 0.7692 - val_loss: 0.9833 - val_accuracy: 0.3462\n",
      "Epoch 945/1000\n",
      "104/104 [==============================] - 0s 523us/step - loss: 0.5190 - accuracy: 0.6827 - val_loss: 1.0021 - val_accuracy: 0.3462\n",
      "Epoch 946/1000\n",
      "104/104 [==============================] - 0s 511us/step - loss: 0.5104 - accuracy: 0.7500 - val_loss: 1.0158 - val_accuracy: 0.3462\n",
      "Epoch 947/1000\n",
      "104/104 [==============================] - 0s 582us/step - loss: 0.5194 - accuracy: 0.6923 - val_loss: 1.0130 - val_accuracy: 0.3462\n",
      "Epoch 948/1000\n",
      "104/104 [==============================] - 0s 581us/step - loss: 0.5094 - accuracy: 0.7308 - val_loss: 1.0197 - val_accuracy: 0.3462\n",
      "Epoch 949/1000\n",
      "104/104 [==============================] - 0s 563us/step - loss: 0.5145 - accuracy: 0.7596 - val_loss: 1.0079 - val_accuracy: 0.3846\n",
      "Epoch 950/1000\n",
      "104/104 [==============================] - 0s 556us/step - loss: 0.4651 - accuracy: 0.8077 - val_loss: 1.0180 - val_accuracy: 0.3846\n",
      "Epoch 951/1000\n",
      "104/104 [==============================] - 0s 536us/step - loss: 0.5210 - accuracy: 0.7308 - val_loss: 1.0295 - val_accuracy: 0.3462\n",
      "Epoch 952/1000\n",
      "104/104 [==============================] - 0s 534us/step - loss: 0.4782 - accuracy: 0.7404 - val_loss: 1.0415 - val_accuracy: 0.3462\n",
      "Epoch 953/1000\n",
      "104/104 [==============================] - 0s 551us/step - loss: 0.4945 - accuracy: 0.7692 - val_loss: 1.0573 - val_accuracy: 0.3462\n",
      "Epoch 954/1000\n",
      "104/104 [==============================] - 0s 602us/step - loss: 0.4729 - accuracy: 0.7500 - val_loss: 1.0647 - val_accuracy: 0.3462\n",
      "Epoch 955/1000\n",
      "104/104 [==============================] - 0s 567us/step - loss: 0.4608 - accuracy: 0.7885 - val_loss: 1.0832 - val_accuracy: 0.3462\n",
      "Epoch 956/1000\n",
      "104/104 [==============================] - 0s 500us/step - loss: 0.4780 - accuracy: 0.7692 - val_loss: 1.1088 - val_accuracy: 0.3462\n",
      "Epoch 957/1000\n",
      "104/104 [==============================] - 0s 507us/step - loss: 0.4872 - accuracy: 0.7308 - val_loss: 1.1068 - val_accuracy: 0.3462\n",
      "Epoch 958/1000\n",
      "104/104 [==============================] - 0s 505us/step - loss: 0.4608 - accuracy: 0.7404 - val_loss: 1.0981 - val_accuracy: 0.3077\n",
      "Epoch 959/1000\n",
      "104/104 [==============================] - 0s 505us/step - loss: 0.4646 - accuracy: 0.7404 - val_loss: 1.0781 - val_accuracy: 0.3077\n",
      "Epoch 960/1000\n",
      "104/104 [==============================] - 0s 523us/step - loss: 0.4885 - accuracy: 0.7692 - val_loss: 1.0845 - val_accuracy: 0.3462\n",
      "Epoch 961/1000\n",
      "104/104 [==============================] - 0s 504us/step - loss: 0.4493 - accuracy: 0.7596 - val_loss: 1.0992 - val_accuracy: 0.3462\n",
      "Epoch 962/1000\n",
      "104/104 [==============================] - 0s 533us/step - loss: 0.4993 - accuracy: 0.7404 - val_loss: 1.0953 - val_accuracy: 0.3462\n",
      "Epoch 963/1000\n",
      "104/104 [==============================] - 0s 539us/step - loss: 0.4633 - accuracy: 0.7500 - val_loss: 1.1032 - val_accuracy: 0.3462\n",
      "Epoch 964/1000\n",
      "104/104 [==============================] - 0s 548us/step - loss: 0.4307 - accuracy: 0.7981 - val_loss: 1.1042 - val_accuracy: 0.3462\n",
      "Epoch 965/1000\n",
      "104/104 [==============================] - 0s 562us/step - loss: 0.5056 - accuracy: 0.7788 - val_loss: 1.1415 - val_accuracy: 0.3846\n",
      "Epoch 966/1000\n",
      "104/104 [==============================] - 0s 528us/step - loss: 0.4470 - accuracy: 0.7500 - val_loss: 1.1419 - val_accuracy: 0.3462\n",
      "Epoch 967/1000\n",
      "104/104 [==============================] - 0s 564us/step - loss: 0.4600 - accuracy: 0.7788 - val_loss: 1.0550 - val_accuracy: 0.3846\n",
      "Epoch 968/1000\n",
      "104/104 [==============================] - 0s 515us/step - loss: 0.4895 - accuracy: 0.6923 - val_loss: 1.0684 - val_accuracy: 0.3846\n",
      "Epoch 969/1000\n",
      "104/104 [==============================] - 0s 554us/step - loss: 0.4719 - accuracy: 0.7404 - val_loss: 1.0783 - val_accuracy: 0.3846\n",
      "Epoch 970/1000\n",
      "104/104 [==============================] - 0s 470us/step - loss: 0.4498 - accuracy: 0.7404 - val_loss: 1.0890 - val_accuracy: 0.3846\n",
      "Epoch 971/1000\n",
      "104/104 [==============================] - 0s 503us/step - loss: 0.4983 - accuracy: 0.7692 - val_loss: 1.0675 - val_accuracy: 0.3846\n",
      "Epoch 972/1000\n",
      "104/104 [==============================] - 0s 486us/step - loss: 0.4726 - accuracy: 0.7692 - val_loss: 1.0722 - val_accuracy: 0.3077\n",
      "Epoch 973/1000\n",
      "104/104 [==============================] - 0s 521us/step - loss: 0.4968 - accuracy: 0.7885 - val_loss: 1.0514 - val_accuracy: 0.3462\n",
      "Epoch 974/1000\n",
      "104/104 [==============================] - 0s 484us/step - loss: 0.5010 - accuracy: 0.7404 - val_loss: 1.0691 - val_accuracy: 0.3462\n",
      "Epoch 975/1000\n",
      "104/104 [==============================] - 0s 538us/step - loss: 0.4411 - accuracy: 0.7885 - val_loss: 1.0792 - val_accuracy: 0.3077\n",
      "Epoch 976/1000\n",
      "104/104 [==============================] - 0s 584us/step - loss: 0.4772 - accuracy: 0.7692 - val_loss: 1.0973 - val_accuracy: 0.3077\n",
      "Epoch 977/1000\n",
      "104/104 [==============================] - 0s 477us/step - loss: 0.4784 - accuracy: 0.7500 - val_loss: 1.0831 - val_accuracy: 0.3077\n",
      "Epoch 978/1000\n",
      "104/104 [==============================] - 0s 504us/step - loss: 0.4919 - accuracy: 0.7115 - val_loss: 1.1233 - val_accuracy: 0.3462\n",
      "Epoch 979/1000\n",
      "104/104 [==============================] - 0s 518us/step - loss: 0.4852 - accuracy: 0.7981 - val_loss: 1.0868 - val_accuracy: 0.3462\n",
      "Epoch 980/1000\n",
      "104/104 [==============================] - 0s 539us/step - loss: 0.4562 - accuracy: 0.8173 - val_loss: 1.1356 - val_accuracy: 0.3462\n",
      "Epoch 981/1000\n",
      "104/104 [==============================] - 0s 444us/step - loss: 0.5005 - accuracy: 0.6923 - val_loss: 1.1086 - val_accuracy: 0.3462\n",
      "Epoch 982/1000\n",
      "104/104 [==============================] - 0s 450us/step - loss: 0.5077 - accuracy: 0.7308 - val_loss: 1.1088 - val_accuracy: 0.3462\n",
      "Epoch 983/1000\n",
      "104/104 [==============================] - 0s 508us/step - loss: 0.4659 - accuracy: 0.7500 - val_loss: 1.1318 - val_accuracy: 0.3077\n",
      "Epoch 984/1000\n",
      "104/104 [==============================] - 0s 534us/step - loss: 0.4950 - accuracy: 0.7212 - val_loss: 1.1326 - val_accuracy: 0.3462\n",
      "Epoch 985/1000\n",
      "104/104 [==============================] - 0s 478us/step - loss: 0.4880 - accuracy: 0.7212 - val_loss: 1.1478 - val_accuracy: 0.3077\n",
      "Epoch 986/1000\n",
      "104/104 [==============================] - 0s 514us/step - loss: 0.4610 - accuracy: 0.7788 - val_loss: 1.1086 - val_accuracy: 0.3077\n",
      "Epoch 987/1000\n",
      "104/104 [==============================] - 0s 517us/step - loss: 0.4874 - accuracy: 0.7692 - val_loss: 1.1090 - val_accuracy: 0.3462\n",
      "Epoch 988/1000\n",
      "104/104 [==============================] - 0s 545us/step - loss: 0.4737 - accuracy: 0.7404 - val_loss: 1.1876 - val_accuracy: 0.3077\n",
      "Epoch 989/1000\n",
      "104/104 [==============================] - 0s 512us/step - loss: 0.4594 - accuracy: 0.7885 - val_loss: 1.1755 - val_accuracy: 0.3077\n",
      "Epoch 990/1000\n",
      "104/104 [==============================] - 0s 515us/step - loss: 0.5070 - accuracy: 0.7596 - val_loss: 1.1831 - val_accuracy: 0.3077\n",
      "Epoch 991/1000\n",
      "104/104 [==============================] - 0s 577us/step - loss: 0.4754 - accuracy: 0.7404 - val_loss: 1.1934 - val_accuracy: 0.3077\n",
      "Epoch 992/1000\n",
      "104/104 [==============================] - 0s 554us/step - loss: 0.4698 - accuracy: 0.7981 - val_loss: 1.2151 - val_accuracy: 0.3462\n",
      "Epoch 993/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 0s 509us/step - loss: 0.4271 - accuracy: 0.7885 - val_loss: 1.2050 - val_accuracy: 0.3077\n",
      "Epoch 994/1000\n",
      "104/104 [==============================] - 0s 513us/step - loss: 0.3965 - accuracy: 0.8558 - val_loss: 1.2237 - val_accuracy: 0.3462\n",
      "Epoch 995/1000\n",
      "104/104 [==============================] - 0s 507us/step - loss: 0.4922 - accuracy: 0.7500 - val_loss: 1.1288 - val_accuracy: 0.3462\n",
      "Epoch 996/1000\n",
      "104/104 [==============================] - 0s 518us/step - loss: 0.4372 - accuracy: 0.7692 - val_loss: 1.1382 - val_accuracy: 0.4231\n",
      "Epoch 997/1000\n",
      "104/104 [==============================] - 0s 522us/step - loss: 0.4738 - accuracy: 0.7212 - val_loss: 1.1009 - val_accuracy: 0.4231\n",
      "Epoch 998/1000\n",
      "104/104 [==============================] - 0s 513us/step - loss: 0.4793 - accuracy: 0.7885 - val_loss: 1.1110 - val_accuracy: 0.3846\n",
      "Epoch 999/1000\n",
      "104/104 [==============================] - 0s 469us/step - loss: 0.4172 - accuracy: 0.7885 - val_loss: 1.1334 - val_accuracy: 0.3846\n",
      "Epoch 1000/1000\n",
      "104/104 [==============================] - 0s 490us/step - loss: 0.4730 - accuracy: 0.7115 - val_loss: 1.1423 - val_accuracy: 0.3462\n"
     ]
    }
   ],
   "source": [
    "#Dependencies\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=30, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation=\"softmax\", kernel_initializer=\"uniform\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_name = \"loss\"\n",
    "accuracy_name = \"accuracy\"\n",
    "\n",
    "acc = model.history.history[accuracy_name]\n",
    "val_acc = model.history.history[f'val_{accuracy_name}']\n",
    "\n",
    "loss = model.history.history[loss_name]\n",
    "val_loss = model.history.history[f\"val_{loss_name}\"]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "# plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict([X_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def sensitivity_specificity(model, X, y):\n",
    "    \n",
    "    cm1 = confusion_matrix(y, np.round(model.predict(X)))\n",
    "    \n",
    "    # total1 = sum(sum(cm1))\n",
    "    \n",
    "    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    \n",
    "    return sensitivity1, specificity1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42857142857142855 0.25\n"
     ]
    }
   ],
   "source": [
    "cm1 = confusion_matrix(y_test[:,0], np.round(model.predict([X_test]))[:,0])\n",
    "\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "\n",
    "print(sensitivity1, specificity1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 8],\n",
       "       [9, 3]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_names = []\n",
    "for subject, label in zip(subjects, labels):\n",
    "    if subject == \"RE_ATTILIO\":\n",
    "        continue\n",
    "    if not sequence_repo.has(subject, \"FLAIR\"):\n",
    "        continue\n",
    "    if not sequence_repo.has(subject, \"MPRAGE\"):\n",
    "        continue\n",
    "    if not sequence_repo.has(subject, \"T1\"):\n",
    "        continue\n",
    "    tasks.append((sequence_repo, subject, sequence_names, label))\n",
    "    real_names.append(subject)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALESSANDRINI_GLAUCO 0.0 [1. 0.]\n",
      "ANGELONI_GIUSEPPINA 1.0 [0. 1.]\n",
      "ASSANTO_MARIA 0.0 [1. 0.]\n",
      "BAGNOLI_VINCENZO 1.0 [0. 1.]\n",
      "BARONTINI_MARIA_GIOVANNA 0.0 [1. 0.]\n",
      "BATTISTA_DOMENICA 1.0 [0. 1.]\n",
      "BERGNACH_SILVANO 1.0 [0. 1.]\n",
      "BERNOLA_TERESA 0.0 [1. 0.]\n",
      "BERTUZZI_LUISA 0.0 [1. 0.]\n",
      "BEVILACQUA_RITA 0.0 [1. 0.]\n",
      "BIANCHI_GIOVANNI 0.0 [1. 0.]\n",
      "BIANCHI_ORAZIO 0.0 [1. 0.]\n",
      "BIAVATI_S 0.0 [1. 0.]\n",
      "BOEZI_MARIO 1.0 [0. 1.]\n",
      "BOVE_A 0.0 [1. 0.]\n",
      "CACACE_PAOLO 1.0 [0. 1.]\n",
      "CALDARONI_ANNA 1.0 [0. 1.]\n",
      "CAMACCI_FILIBERTO 0.0 [1. 0.]\n",
      "CAMPLESE_CANDEROLA 1.0 [0. 1.]\n",
      "CAPEZZONE 0.0 [1. 0.]\n",
      "CARULLI_L 1.0 [0. 1.]\n",
      "CARZEDDA_PAOLO 0.0 [1. 0.]\n",
      "CATALANI_F 0.0 [1. 0.]\n",
      "CIMPUREANU_N 1.0 [0. 1.]\n",
      "COLAFRANCESCO_ROCCO 0.0 [1. 0.]\n",
      "COLAZZO_LUIGI_GIUSEPPE 0.0 [1. 0.]\n",
      "COSTANZI_P 1.0 [0. 1.]\n",
      "CRESCENZI_ARMANDO 0.0 [1. 0.]\n",
      "DARIDA 0.0 [1. 0.]\n",
      "DEL_BOVE_PIERINA 1.0 [0. 1.]\n",
      "DE_PAOLI_R 1.0 [0. 1.]\n",
      "DE_SANTIS_GIORGO 0.0 [1. 0.]\n",
      "DIASPRO_G 1.0 [0. 1.]\n",
      "DI_CARLATONIO_MAURIZIO 0.0 [1. 0.]\n",
      "DI_LORENZO_TOMMASO 0.0 [1. 0.]\n",
      "DI_MARCO_L 0.0 [1. 0.]\n",
      "DI_MASO_SIMONE 0.0 [1. 0.]\n",
      "DI_MASSA_SERGIO 0.0 [1. 0.]\n",
      "DOBRISAN_DORINA 0.0 [1. 0.]\n",
      "D_ANGELI_ANNUNZIATA 0.0 [1. 0.]\n",
      "D_ANGELO_RENATO 1.0 [0. 1.]\n",
      "EMERY_R_C 1.0 [0. 1.]\n",
      "FABIANI_ANNA 1.0 [0. 1.]\n",
      "FEDERICO_FRANCESCO 1.0 [0. 1.]\n",
      "FERRAZZA_RITA 0.0 [1. 0.]\n",
      "FERRI_M_B 1.0 [0. 1.]\n",
      "FILIPPONI_QUINTINO 0.0 [1. 0.]\n",
      "FIUCCI_A 0.0 [1. 0.]\n",
      "FLORIO_FRANCESCO_PAOLO 1.0 [0. 1.]\n",
      "FRATINI_RITA 0.0 [1. 0.]\n",
      "GATTAMORTA_NATALINA 0.0 [1. 0.]\n",
      "GEGGI_GIULIO 0.0 [1. 0.]\n",
      "GENNARI_CRISTIANO 1.0 [0. 1.]\n",
      "GIANFELICI_LUISA 0.0 [1. 0.]\n",
      "GIOIA_COSMO_DAMIANO 1.0 [0. 1.]\n",
      "GIORDANO_STEFANIA 1.0 [0. 1.]\n",
      "INCITI_DONATA 0.0 [1. 0.]\n",
      "IONTA_LUCIANA 0.0 [1. 0.]\n",
      "ISONI_FRANCESCO 0.0 [1. 0.]\n",
      "LABELLA_ADRIANA 0.0 [1. 0.]\n",
      "LANDONE_ANNUNZIATA 1.0 [0. 1.]\n",
      "LIBERATI_G_L 1.0 [0. 1.]\n",
      "LIOCE_CARMELA 1.0 [0. 1.]\n",
      "LONGO_ROSALIA 1.0 [0. 1.]\n",
      "LO_BELLO_MARIO 0.0 [1. 0.]\n",
      "LUPI_GIANCARLO 1.0 [0. 1.]\n",
      "MAIOLINI_SANTA 1.0 [0. 1.]\n",
      "MARAGNO_CLARA 1.0 [0. 1.]\n",
      "MARCOLINI 1.0 [0. 1.]\n",
      "MARIANI_BERNARDO 0.0 [1. 0.]\n",
      "MAROCCHI_CORRADO 0.0 [1. 0.]\n",
      "MARTINEZ 1.0 [0. 1.]\n",
      "MASCI_ADA 0.0 [1. 0.]\n",
      "MEDICI_GIOVANNA 1.0 [0. 1.]\n",
      "MICHELI_MICHELE 1.0 [0. 1.]\n",
      "MONACELLI_LAURA 1.0 [0. 1.]\n",
      "MOSCARDINI_GIACINTO 0.0 [1. 0.]\n",
      "MOVIA_A 0.0 [1. 0.]\n",
      "MUSAT_DORINA 1.0 [0. 1.]\n",
      "NERONE_GIANLUCA 1.0 [0. 1.]\n",
      "NERVEGNA_G 1.0 [0. 1.]\n",
      "ORLANDI_PAOLO 1.0 [0. 1.]\n",
      "PAGANNONE_GIANNI 0.0 [1. 0.]\n",
      "PAGLIAROLI_LUCIA 0.0 [1. 0.]\n",
      "PAGNOTTA 1.0 [0. 1.]\n",
      "PALMA 0.0 [1. 0.]\n",
      "PALMIERI 1.0 [0. 1.]\n",
      "PANETTI 1.0 [0. 1.]\n",
      "PASSARI 1.0 [0. 1.]\n",
      "PIERI 1.0 [0. 1.]\n",
      "PIERINI_CATERINA 1.0 [0. 1.]\n",
      "PINEDA_MARIA_ASSUNTA 0.0 [1. 0.]\n",
      "PISTOIA_CARLO 1.0 [0. 1.]\n",
      "PODAGROSI_TERESA 1.0 [0. 1.]\n",
      "PODDA_ANTONINO 0.0 [1. 0.]\n",
      "POMPEI_F 1.0 [0. 1.]\n",
      "PRINCIPI_ANNA_MARIA 1.0 [0. 1.]\n",
      "PROIETTI_GIOVANNI 1.0 [0. 1.]\n",
      "PROIETTI_MARIA 0.0 [1. 0.]\n",
      "QUACQUARELLI_A 1.0 [0. 1.]\n",
      "QUATTROCIOCCHI_EVELINA 0.0 [1. 0.]\n",
      "RICCI_ALESSANDRO 1.0 [0. 1.]\n",
      "ROMITO_ORAZIO 0.0 [1. 0.]\n",
      "ROSARI_NANDO 1.0 [0. 1.]\n",
      "RUSCITO_ELISABETTA 1.0 [0. 1.]\n",
      "RUSNAC_NINA 1.0 [0. 1.]\n",
      "RUSSO_IDA 1.0 [0. 1.]\n",
      "SALA_CLARA 0.0 [1. 0.]\n",
      "SALTARELLI_DOMENICO 0.0 [1. 0.]\n",
      "SANTINI_ERMANNO 1.0 [0. 1.]\n",
      "SCARAMUZZA_F 0.0 [1. 0.]\n",
      "SOLOVIY_VOLODYMYR 0.0 [1. 0.]\n",
      "STAN_FLORENTINA 0.0 [1. 0.]\n",
      "STEFANINI_CLORINDA 0.0 [1. 0.]\n",
      "STERPA_GIUSEPPE 1.0 [0. 1.]\n",
      "SYKULA_GRAZYNA_BARBARA 0.0 [1. 0.]\n",
      "TAVERNESE_G 0.0 [1. 0.]\n",
      "TAVOLUCCI_MARIA_RITA 0.0 [1. 0.]\n",
      "TEMPESTINI_MARISA 0.0 [1. 0.]\n",
      "TEMPORIN_PATRIZIA 1.0 [0. 1.]\n",
      "TEOFILI_STEFANO 1.0 [0. 1.]\n",
      "TIBERI_GIUSEPPE 1.0 [0. 1.]\n",
      "TOMAO_ANGELO 1.0 [0. 1.]\n",
      "TOMEO_VINCENZO 1.0 [0. 1.]\n",
      "TROSCIA_M 1.0 [0. 1.]\n",
      "VERONESI_ROCCO 1.0 [0. 1.]\n",
      "VITULANO_RITA 1.0 [0. 1.]\n",
      "ZANATTA_CARLO 1.0 [0. 1.]\n",
      "ZANGARI_ALDO 0.0 [1. 0.]\n",
      "ZEPPA_ONORIO 0.0 [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "for s, l, c in zip(real_names, y, to_categorical(np.array(y))):\n",
    "    print(s, l, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
