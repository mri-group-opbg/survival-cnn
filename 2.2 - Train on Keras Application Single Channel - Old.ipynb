{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "Questo notebook effettua training e validation su Keras application per capire quale \n",
    "combinazione di sequenza e modello sia pi√π performante.\n",
    "I modelli sono presi senza pesi e modificati per diventare un classificatore binario.\n",
    "Le sequenze sono prese singolarmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliomi import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "optimizers = {\n",
    "    'rmsprop': (lambda : tf.keras.optimizers.RMSprop(learning_rate=0.0001)),\n",
    "    'sgd': (lambda : tf.keras.optimizers.SGD(lr=0.001, momentum=0.9, decay=0.001/10, nesterov=False)),\n",
    "    'adam' : (lambda : tf.keras.optimizers.Adam(lr=0.0001))\n",
    "}\n",
    "\n",
    "# sgd = optimizers.SGD(lr=0.01, clipvalue=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# from tensorflow.python.keras.applications import *\n",
    "from tensorflow.keras.applications import *\n",
    "\n",
    "# from tensorflow.python.keras.models import Model\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# from tensorflow.python.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
    "\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from tensorflow.python.keras.optimizers import SGD, Adam\n",
    "# from keras.optimizers import SGD, Adam\n",
    "\n",
    "# import tensorflow.python.keras.backend as K\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.python.keras.metrics import BinaryAccuracy\n",
    "\n",
    "def get_model(model_name, optimizer, include_dropout=False):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    K.set_image_data_format('channels_last')\n",
    "\n",
    "    IMG_SHAPE = (224, 224, 1)\n",
    "\n",
    "    # Base model is a Keras Application\n",
    "    base_model = eval(model_name)(weights=None, include_top=True, input_shape=IMG_SHAPE)\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    # global_spatial_avg_pool_layer = GlobalMaxPooling2D()(base_model.output) # GlobalAveragePooling2D()(base_model.output)\n",
    "\n",
    "    # flatten_layer = Flatten()(global_spatial_avg_pool_layer)\n",
    "\n",
    "    # Feature leayer\n",
    "    #if include_dropout:\n",
    "        # Drop-out\n",
    "        # dropout_layer = Dropout(0.5)(flatten_layer)\n",
    "        # feature_layer = Dense(512, activation='relu', kernel_initializer='glorot_uniform')(dropout_layer)\n",
    "    #else:\n",
    "        # feature_layer = Dense(512, activation='relu', kernel_initializer='glorot_uniform')(flatten_layer)\n",
    "\n",
    "    # and a logistic layer -- let's say we have 200 classes\n",
    "    # prediction_layer = Dense(2, activation='softmax')(feature_layer)\n",
    "    # prediction_layer = Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')(feature_layer)\n",
    "    prediction_layer = Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')(base_model.output)\n",
    "\n",
    "    # Final model\n",
    "    model = Model(inputs=base_model.input, outputs=prediction_layer)\n",
    "    \n",
    "    # loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, name='categorical_crossentropy')\n",
    "    \n",
    "    # sigmoid with binary_crossentropy and single dense neuron\n",
    "    \n",
    "    # Compile model\n",
    "    # model.compile(optimizer=optimizer, loss=\"mse\", metrics=['accuracy'])\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=[BinaryAccuracy()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "    \n",
    "def train_and_test(model_name, model, X_train, y_train, X_test, y_test, random_state=42, epochs=100, batch_size=16):\n",
    "    \n",
    "    # early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "    # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"logs-2.2-test/{model_name}\")\n",
    "\n",
    "    aug = ImageDataGenerator(\n",
    "        rotation_range=90,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        # zoom_range=0.15,\n",
    "        # width_shift_range=0.2,\n",
    "        # height_shift_range=0.2,\n",
    "        # shear_range=0.15,\n",
    "        #fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    fit = model.fit(\n",
    "            aug.flow(X_train, y_train, batch_size=batch_size),\n",
    "            # X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            # callbacks=[early_stopping_callback],\n",
    "            validation_data=(X_test, y_test), \n",
    "            # batch_size=batch_size,\n",
    "            shuffle=False)\n",
    "    \n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def make_name(slice_dataset, dataset, model_name, optimizer_name, sequence, percentile):\n",
    "    slice_dataset = os.path.basename(slice_dataset).replace(\"datasets-\", \"\")\n",
    "    return \"-\".join([slice_dataset, dataset, model_name, optimizer_name, sequence, percentile])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DatasetLoader():\n",
    "    \n",
    "    def __init__(self, dataset_path, classification_path, single=False):\n",
    "        \n",
    "        self.single=single\n",
    "        \n",
    "        self.X, self.y = get_dataset_for_classification(dataset_path, classification_path)\n",
    "            \n",
    "    def load(self):\n",
    "        subjects = np.array(list(self.X.keys()))\n",
    "        \n",
    "        if self.single:\n",
    "            self.slices = np.concatenate([[self.X[subject][0]] for subject in subjects])\n",
    "            self.labels = np.concatenate([np.repeat((self.y)[subject], 1) for subject in subjects])\n",
    "            self.subjects = np.array(subjects)\n",
    "        else:\n",
    "            self.slices = np.concatenate([self.X[subject] for subject in subjects])\n",
    "            self.labels = np.concatenate([np.repeat((self.y)[subject], (self.X)[subject].shape[0]) for subject in subjects])\n",
    "            self.subjects = np.concatenate([np.repeat(subject, (self.X)[subject].shape[0]) for subject in subjects])\n",
    "        \n",
    "        # Categorical\n",
    "        dictionary = np.array([[0, 1], [1, 0]])\n",
    "        int_labels = self.labels.astype(int)\n",
    "        self.categorical_labels = dictionary[int_labels]\n",
    "\n",
    "    def get_subjects(self):\n",
    "        return self.subjects\n",
    "        # return list(self.X.keys())\n",
    "\n",
    "    def get_split(self, test_size=0.2, random_state=42):\n",
    "        \n",
    "        subjects = np.array(self.get_subjects())\n",
    "        \n",
    "        indexes = list(range(len(subjects)))\n",
    "        \n",
    "        train_index, test_index, _, _ = train_test_split(\n",
    "            indexes, \n",
    "            self.labels,\n",
    "            # stratify=self.labels,\n",
    "            test_size=test_size, \n",
    "            random_state=random_state)\n",
    "        \n",
    "        train_subjects = subjects[train_index]\n",
    "        test_subjects = subjects[test_index]\n",
    "        \n",
    "        X_train = self.slices[np.isin(self.subjects, train_subjects)]\n",
    "        X_test = self.slices[np.isin(self.subjects, test_subjects)]\n",
    "        y_train = self.labels[np.isin(self.subjects, train_subjects)]\n",
    "        y_test = self.labels[np.isin(self.subjects, test_subjects)]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def get_split_categorical(self, test_size=0.2, random_state=42):\n",
    "\n",
    "        subjects = np.array(self.get_subjects())\n",
    "\n",
    "        indexes = list(range(len(subjects)))\n",
    "\n",
    "        train_index, test_index, _, _ = train_test_split(\n",
    "            indexes,\n",
    "            self.categorical_labels, \n",
    "            # stratify=self.labels,\n",
    "            test_size=test_size, \n",
    "            random_state=random_state)\n",
    "        \n",
    "        train_subjects = subjects[train_index]\n",
    "        test_subjects = subjects[test_index]\n",
    "        \n",
    "        X_train = self.slices[np.isin(self.subjects, train_subjects)]\n",
    "        X_test = self.slices[np.isin(self.subjects, test_subjects)]\n",
    "        y_train = self.categorical_labels[np.isin(self.subjects, train_subjects)]\n",
    "        y_test = self.categorical_labels[np.isin(self.subjects, test_subjects)]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/RMN/dataset-gliomi-cnn/datasets-tumor-crop/t1-224-100-perc.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bb247b1f1591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_loader = DatasetLoader(\"/data/RMN/dataset-gliomi-cnn/datasets-tumor-crop/t1-224-100-perc.pickle\", \n\u001b[1;32m      2\u001b[0m                                \u001b[0;34m\"/data/RMN/dataset-gliomi-cnn/dataset-survivor.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                single=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-6b16cc33df7a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_path, classification_path, single)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset_for_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~tf/local/notebooks/gliomi/loader.py\u001b[0m in \u001b[0;36mget_dataset_for_classification\u001b[0;34m(dataset_path, classification_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubjects_to_exclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubjects_to_exclude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[1;32m     16\u001b[0m     \u001b[0mEstablish\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mintersection\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtest\u001b[0m \u001b[0msubjects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/RMN/dataset-gliomi-cnn/datasets-tumor-crop/t1-224-100-perc.pickle'"
     ]
    }
   ],
   "source": [
    "dataset_loader = DatasetLoader(\"/data/RMN/dataset-gliomi-cnn/datasets-tumor-crop/t1-224-100-perc.pickle\", \n",
    "                               \"/data/RMN/dataset-gliomi-cnn/dataset-survivor.csv\", \n",
    "                               single=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader.slices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(\"MobileNetV2\", optimizer=optimizers[\"adam\"](), include_dropout=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = dataset_loader.get_split(test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(X_train), np.max(X_train), np.mean(X_train), np.std(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(X_test), np.max(X_test), np.mean(X_test), np.std(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaler.fit(X_test)\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    print(model.predict(X_train))\n",
    "\n",
    "    fit = model.fit(\n",
    "            scaler.transform(X_train), y_train,\n",
    "            epochs=1,\n",
    "            validation_data=(scaler.transform(X_test), y_test), \n",
    "            batch_size=batch_size,\n",
    "            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = train_and_test(\"MobileNetV2\", \n",
    "                         model, \n",
    "                         X_train, \n",
    "                         y_train, \n",
    "                         X_test, \n",
    "                         y_test, \n",
    "                         random_state=42, \n",
    "                         epochs=1000, \n",
    "                         batch_size=32)\n",
    "\n",
    "train_score = model.evaluate(X_train, y_train)\n",
    "\n",
    "test_score = model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.BinaryAccuracy()\n",
    "m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])\n",
    "m.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, categorical_accuracy, categorical_crossentropy\n",
    "train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_normalized =  normalize(X_test, max_value=1., axis=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_test_normalized), np.std(X_test_normalized), np.min(X_test_normalized), np.max(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_test), np.std(X_test), np.min(X_test), np.max(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(X_test, axis=0), np.max(X_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_train), np.std(X_train), np.min(X_train), np.max(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "slice_datasets = [\n",
    "    # \"/data/RMN/dataset-gliomi-cnn/datasets-full-brain\",\n",
    "    \"/data/RMN/dataset-gliomi-cnn/2-datasets-tumor-crop\"\n",
    "]\n",
    "\n",
    "augmentation = \"aug-100-\"\n",
    "\n",
    "datasets = [\n",
    "    \"survivor\", \n",
    "    \"idh\", \n",
    "    \"ki67\", \n",
    "    \"egfr\", \n",
    "    \"mgmt\"\n",
    "]\n",
    "\n",
    "percentiles = [\n",
    "    100 \n",
    "#    70\n",
    "]\n",
    "\n",
    "sequences = [\n",
    "    \"t1\", \n",
    "    \"t2\", \n",
    "    \"flair\", \n",
    "    \"rcbv\", \n",
    "    \"mprage\"\n",
    "    \"adc\"\n",
    "]\n",
    "\n",
    "keras_models = [\n",
    "    \"MobileNetV2\",\n",
    "    \"NASNetMobile\",\n",
    "    \"VGG19\",\n",
    "    \"ResNet50\",\n",
    "    \"ResNet101\",\n",
    "    \"DenseNet169\",\n",
    "]\n",
    "\n",
    "optimizer_names = [\n",
    "    'rmsprop',\n",
    "    'sgd',\n",
    "    'adam'\n",
    "]\n",
    "\n",
    "result_file = \"aug-new-results-2.2.csv\"\n",
    "\n",
    "side = 224\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "columns = [\"slice-dataset\", \"dataset\", \"model\", \"optimizer\", \"sequence\", \"percentile\", \"accuracy\", \"loss\", \"val. accuracy\", \"val. loss\"]\n",
    "rows_list = []\n",
    "recover_count = 0\n",
    "recover = False\n",
    "\n",
    "if recover:\n",
    "    df = pd.read_csv(result_file)\n",
    "    rows_list = df.iloc[:, 1:]\n",
    "    rows_list = np.array(rows_list).tolist()\n",
    "    \n",
    "for slice_dataset in slice_datasets:\n",
    "    for dataset in datasets:\n",
    "        for sequence in sequences:\n",
    "            for percentile in percentiles:\n",
    "                \n",
    "                if recover and recover_count < len(rows_list):\n",
    "                    pass\n",
    "                else:\n",
    "                    dataset_loader = DatasetLoader(f\"{slice_dataset}/{augmentation}dataset-{dataset}-{sequence}-{side}-{percentile}-perc.pickle\")\n",
    "    \n",
    "                for model_name in keras_models:\n",
    "                    for optimizer_name in optimizer_names:\n",
    "\n",
    "                        if recover and recover_count < len(rows_list):\n",
    "                            recover_count = recover_count + 1\n",
    "                            print(\"Skip row:\", recover_count)\n",
    "                            continue\n",
    "\n",
    "                        recover = False\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            K.clear_session()\n",
    "\n",
    "                            plot_file_name = make_name(slice_dataset, dataset, model_name, optimizer_name, sequence, str(percentile))\n",
    "\n",
    "                            print(\"Loading\", model_name, \"[\", plot_file_name, \"]\")\n",
    "\n",
    "                            model = get_model(model_name, optimizer=optimizers[optimizer_name]())\n",
    "\n",
    "                            print(\"Training\", model_name)\n",
    "\n",
    "                            X_train, X_test, y_train, y_test = dataset_loader.get_split_categorical(test_size=0.2, random_state=42)\n",
    "\n",
    "                            print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "                            history = train_and_test(model_name, model, X_train, y_train, X_test, y_test, random_state=42, epochs=epochs, batch_size=16)\n",
    "\n",
    "                            save_plot(history, f\"new-2.2-{plot_file_name}\")\n",
    "\n",
    "                            train_score = model.evaluate(X_train, y_train)\n",
    "\n",
    "                            test_score = model.evaluate(X_test, y_test)\n",
    "\n",
    "                            rows_list.append([\n",
    "                                slice_dataset,\n",
    "                                dataset,\n",
    "                                model_name,\n",
    "                                optimizer_name,\n",
    "                                sequence,\n",
    "                                percentile,\n",
    "                                str(train_score[1]),\n",
    "                                str(train_score[0]),\n",
    "                                str(test_score[1]),\n",
    "                                str(test_score[0])\n",
    "                            ])\n",
    "\n",
    "                            df = pd.DataFrame(rows_list, columns=columns)\n",
    "                            df.to_csv(result_file)\n",
    "\n",
    "                        except:\n",
    "                            \n",
    "                            e = sys.exc_info()[0]\n",
    "                            print(e)\n",
    "\n",
    "                            rows_list.append([\n",
    "                                slice_dataset,\n",
    "                                dataset,\n",
    "                                model_name,\n",
    "                                optimizer_name,\n",
    "                                sequence,\n",
    "                                percentile,\n",
    "                                \"Error\",\n",
    "                                \"Error\",\n",
    "                                \"Error\",\n",
    "                                \"Error\"\n",
    "                            ])\n",
    "\n",
    "                            df = pd.DataFrame(rows_list, columns=columns)\n",
    "                            df.to_csv(result_file)                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
