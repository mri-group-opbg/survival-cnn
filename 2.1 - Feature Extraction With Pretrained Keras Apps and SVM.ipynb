{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "This notebook load dataset prepared by `1 - Load Data.ipynb` and run a series of test on pretrained Keras Application.\n",
    "\n",
    "In particular the procedure is:\n",
    "* Download a pretrained Keras Application\n",
    "* Split Subject dataset in train and test\n",
    "* Use train slices to in Keras Appliation to classify the slices\n",
    "* Use classification prediction as SVM features to be trained with the corresponding label\n",
    "* Test the classification of test slices against the trained SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliomi import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_models = [\n",
    "    \"MobileNetV2\",\n",
    "    \"NASNetMobile\",\n",
    "    \"VGG19\",\n",
    "    \"ResNet50\",\n",
    "    \"ResNet101\",\n",
    "    \"DenseNet169\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_3_channels(images):\n",
    "    return np.concatenate([\n",
    "        images,\n",
    "        images,\n",
    "        images\n",
    "    ], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, times):\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for random_state in range(times):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = dataset.get_split(test_size=0.2, random_state=random_state)\n",
    "            \n",
    "        features_train = model.predict([make_3_channels(X_train)])\n",
    "\n",
    "        features_test = model.predict([make_3_channels(X_test)])\n",
    "            \n",
    "        clf = svm.SVC()\n",
    "\n",
    "        clf.fit(features_train, y_train)\n",
    "\n",
    "        score = clf.score(features_test, y_test)\n",
    "\n",
    "        print(random_state, \":\", score)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DatasetLoader():\n",
    "    \n",
    "    def __init__(self, dataset_path, classification_path, single=False):\n",
    "        \n",
    "        self.single=single\n",
    "        \n",
    "        with open(dataset_path, \"rb\") as file:\n",
    "            X = pickle.load(file)\n",
    "\n",
    "        df = pd.read_csv(classification_path)\n",
    "        subjects = np.array(df.iloc[:,1])\n",
    "        labels = np.array(df.iloc[:,2])\n",
    "\n",
    "        X_new = {}\n",
    "        y_new = {}\n",
    "\n",
    "        for i, subject in enumerate(subjects):\n",
    "            if subject in X:\n",
    "                X_new[subject] = X[subject]\n",
    "                y_new[subject] = labels[i]\n",
    "\n",
    "        self.X = X_new\n",
    "        self.y = y_new\n",
    "            \n",
    "    def load(self):\n",
    "        subjects = np.array(list(self.X.keys()))\n",
    "        \n",
    "        if self.single:\n",
    "            self.slices = np.concatenate([[self.X[subject][0]] for subject in subjects])\n",
    "            self.labels = np.concatenate([np.repeat((self.y)[subject], 1) for subject in subjects])\n",
    "            self.subjects = np.array(subjects)\n",
    "        else:\n",
    "            self.slices = np.concatenate([self.X[subject] for subject in subjects])\n",
    "            self.labels = np.concatenate([np.repeat((self.y)[subject], (self.X)[subject].shape[0]) for subject in subjects])\n",
    "            self.subjects = np.concatenate([np.repeat(subject, (self.X)[subject].shape[0]) for subject in subjects])\n",
    "        \n",
    "        # Categorical\n",
    "        dictionary = np.array([[0, 1], [1, 0]])\n",
    "        int_labels = self.labels.astype(int)\n",
    "        self.categorical_labels = dictionary[int_labels]\n",
    "\n",
    "    def get_subjects(self):\n",
    "        return list(self.X.keys())\n",
    "\n",
    "    def get_split(self, test_size=0.2, random_state=42):\n",
    "        \n",
    "        subjects = np.array(self.get_subjects())\n",
    "        \n",
    "        indexes = list(range(len(subjects)))\n",
    "        \n",
    "        train_index, test_index, _, _ = train_test_split(\n",
    "            indexes, \n",
    "            self.labels,\n",
    "            # stratify=self.labels,\n",
    "            test_size=test_size, \n",
    "            random_state=random_state)\n",
    "        \n",
    "        train_subjects = subjects[train_index]\n",
    "        test_subjects = subjects[test_index]\n",
    "        \n",
    "        X_train = self.slices[np.isin(self.subjects, train_subjects)]\n",
    "        X_test = self.slices[np.isin(self.subjects, test_subjects)]\n",
    "        y_train = self.labels[np.isin(self.subjects, train_subjects)]\n",
    "        y_test = self.labels[np.isin(self.subjects, test_subjects)]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def get_split_categorical(self, test_size=0.2, random_state=42):\n",
    "\n",
    "        subjects = np.array(self.get_subjects())\n",
    "\n",
    "        indexes = list(range(len(subjects)))\n",
    "\n",
    "        train_index, test_index, _, _ = train_test_split(\n",
    "            indexes,\n",
    "            self.categorical_labels, \n",
    "            # stratify=self.labels,\n",
    "            test_size=test_size, \n",
    "            random_state=random_state)\n",
    "        \n",
    "        train_subjects = subjects[train_index]\n",
    "        test_subjects = subjects[test_index]\n",
    "        \n",
    "        X_train = self.slices[np.isin(self.subjects, train_subjects)]\n",
    "        X_test = self.slices[np.isin(self.subjects, test_subjects)]\n",
    "        y_train = self.categorical_labels[np.isin(self.subjects, train_subjects)]\n",
    "        y_test = self.categorical_labels[np.isin(self.subjects, test_subjects)]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MobileNetV2\n",
      "Evaluating MobileNetV2\n",
      "0 : 0.48148148148148145\n",
      "1 : 0.48148148148148145\n",
      "2 : 0.4074074074074074\n",
      "3 : 0.4444444444444444\n",
      "4 : 0.4074074074074074\n",
      "5 : 0.2962962962962963\n",
      "6 : 0.4444444444444444\n",
      "7 : 0.37037037037037035\n",
      "8 : 0.3333333333333333\n",
      "9 : 0.48148148148148145\n",
      "Loading NASNetMobile\n",
      "Evaluating NASNetMobile\n",
      "0 : 0.48148148148148145\n",
      "1 : 0.5925925925925926\n",
      "2 : 0.4074074074074074\n",
      "3 : 0.5185185185185185\n",
      "4 : 0.4444444444444444\n",
      "5 : 0.4444444444444444\n",
      "6 : 0.48148148148148145\n",
      "7 : 0.4074074074074074\n",
      "8 : 0.4444444444444444\n",
      "9 : 0.4444444444444444\n",
      "Loading VGG19\n",
      "Evaluating VGG19\n",
      "0 : 0.5555555555555556\n",
      "1 : 0.48148148148148145\n",
      "2 : 0.4074074074074074\n",
      "3 : 0.4074074074074074\n",
      "4 : 0.6296296296296297\n",
      "5 : 0.4444444444444444\n",
      "6 : 0.4444444444444444\n",
      "7 : 0.6666666666666666\n",
      "8 : 0.4444444444444444\n",
      "9 : 0.4444444444444444\n",
      "Loading ResNet50\n",
      "Evaluating ResNet50\n",
      "0 : 0.5185185185185185\n",
      "1 : 0.48148148148148145\n",
      "2 : 0.4074074074074074\n",
      "3 : 0.4444444444444444\n",
      "4 : 0.5555555555555556\n",
      "5 : 0.4444444444444444\n",
      "6 : 0.4444444444444444\n",
      "7 : 0.5185185185185185\n",
      "8 : 0.4444444444444444\n",
      "9 : 0.4444444444444444\n",
      "Loading ResNet101\n",
      "Evaluating ResNet101\n",
      "0 : 0.5185185185185185\n",
      "1 : 0.48148148148148145\n",
      "2 : 0.4074074074074074\n",
      "3 : 0.4444444444444444\n",
      "4 : 0.5555555555555556\n",
      "5 : 0.4444444444444444\n",
      "6 : 0.4444444444444444\n",
      "7 : 0.6296296296296297\n",
      "8 : 0.4444444444444444\n",
      "9 : 0.4074074074074074\n",
      "Loading DenseNet169\n",
      "Evaluating DenseNet169\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8bf581d46493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mscores_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0d54e615d578>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataset, times)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    146\u001b[0m         X, y = check_X_y(X, y, dtype=np.float64,\n\u001b[1;32m    147\u001b[0m                          \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                          accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [\"slice-dataset\", \"dataset\", \"model\", \"sequence\", \"percentile\", \"scores\", \"avg. scores\", \"std. scores\"]\n",
    "\n",
    "times = 10\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "side = 224\n",
    "\n",
    "recover_count = 0\n",
    "\n",
    "recover = False\n",
    "\n",
    "output_file = \"06072020-results-2.1.csv\"\n",
    "\n",
    "if recover:\n",
    "    df = pd.read_csv(output_file)\n",
    "    rows_list = df.iloc[:, 1:]\n",
    "    rows_list = np.array(rows_list).tolist()\n",
    "\n",
    "for dataset in [\"survivor\", \"idh\", \"ki67\", \"egfr\", \"mgmt\"]:\n",
    "    for slice_dataset in [\"/data/RMN/dataset-gliomi-cnn/datasets-full-brain\", \"/data/RMN/dataset-gliomi-cnn/datasets-tumor-crop\"]:\n",
    "        for sequence in [\"t1\", \"t2\", \"flair\", \"rcbv\", \"mprage\", \"adc\"]:\n",
    "            for percentile in [100]:\n",
    "                \n",
    "                if recover and recover_count < len(rows_list):\n",
    "                    pass\n",
    "                else:\n",
    "                    dataset_loader = DatasetLoader(\n",
    "                        f\"{slice_dataset}/{sequence}-{side}-{percentile}-perc.pickle\",\n",
    "                        f\"/data/RMN/dataset-gliomi-cnn/dataset-{dataset}.csv\",\n",
    "                        single=True)\n",
    "                    dataset_loader.load()\n",
    "    \n",
    "                for model_name in keras_models:\n",
    "\n",
    "                    if recover and recover_count < len(rows_list):\n",
    "                        recover_count = recover_count + 1\n",
    "                        print(\"Skip row:\", recover_count)\n",
    "                        continue\n",
    "                \n",
    "                    recover = False\n",
    "\n",
    "#                    try:\n",
    "\n",
    "                    K.clear_session()\n",
    "\n",
    "                    print(\"Loading\", model_name)\n",
    "\n",
    "                    base_model = eval(model_name)(weights='imagenet', include_top=True, input_shape=IMG_SHAPE)\n",
    "\n",
    "                    print(\"Evaluating\", model_name)\n",
    "\n",
    "                    scores = evaluate_model(base_model, dataset_loader, times)\n",
    "\n",
    "                    scores_str = \", \".join(str(x) for x in (scores * 100))\n",
    "\n",
    "                    rows_list.append([\n",
    "                        slice_dataset,\n",
    "                        dataset,\n",
    "                        model_name,\n",
    "                        sequence,\n",
    "                        percentile,\n",
    "                        scores_str,\n",
    "                        np.average(scores * 100),\n",
    "                        np.std(scores * 100)\n",
    "                    ])\n",
    "\n",
    "                    df = pd.DataFrame(rows_list, columns=columns)\n",
    "                    df.to_csv(output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliomi import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3, 40, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40,  7,  6,  5])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[ordered_index_percentile_of_sizes(np.array(a), 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = 50\n",
    "\n",
    "roi_sizes = np.array(a)\n",
    "\n",
    "non_empty_sizes = roi_sizes[np.where(roi_sizes > 0)]\n",
    "\n",
    "percentile_val = np.percentile(non_empty_sizes, percentile)\n",
    "\n",
    "b = np.where(roi_sizes >= percentile_val)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40,  5,  6,  7])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    except:\n",
    "                        \n",
    "                        rows_list.append([\n",
    "                            slice_dataset,\n",
    "                            dataset,\n",
    "                            model_name,\n",
    "                            sequence,\n",
    "                            percentile,\n",
    "                            \"Error\",\n",
    "                            0,\n",
    "                            0\n",
    "                        ])\n",
    "\n",
    "                        df = pd.DataFrame(rows_list, columns=columns)\n",
    "                        df.to_csv(output_file)\n",
    "                        \n",
    "                        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results-2.1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df.iloc[:,7] > 85]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[df1.iloc[:, 4] == 't2']\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.bar(x='Unnamed: 0', y='avg. scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def max_score(df):\n",
    "    return df[df['avg. scores'] == np.max(df['avg. scores'])]\n",
    "\n",
    "def the_winner_is(df, dataset):\n",
    "    return max_score(df[(df['dataset'] == dataset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "winners = pd.concat([\n",
    "    the_winner_is(df, 'survivor'),\n",
    "    the_winner_is(df, 'idh'),\n",
    "    the_winner_is(df, 'mgmt'),\n",
    "    the_winner_is(df, 'ki67'),\n",
    "    the_winner_is(df, 'egfr'),    \n",
    "])\n",
    "\n",
    "winners['slice-dataset'] = winners['slice-dataset'].transform(lambda x: os.path.basename(x).replace(\"datasets-\", \"\"))\n",
    "\n",
    "winners = winners[['dataset', 'slice-dataset' , 'model', 'sequence', 'percentile', 'avg. scores', 'std. scores']]\n",
    "\n",
    "winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "def boruta(X, y):\n",
    "    \n",
    "    ###initialize Boruta\n",
    "    forest = RandomForestRegressor(\n",
    "       n_jobs = -1, \n",
    "       max_depth = 5\n",
    "    )\n",
    "    \n",
    "    boruta = BorutaPy(\n",
    "       estimator = forest, \n",
    "       n_estimators = 'auto',\n",
    "       max_iter = 100 # number of trials to perform\n",
    "    )\n",
    "    \n",
    "    ### fit Boruta (it accepts np.array, not pd.DataFrame)\n",
    "    boruta.fit(X, y)\n",
    "    \n",
    "    return boruta.support_, boruta.support_weak_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_boruta(X, columns):\n",
    "    flat_list = [item for sublist in columns for item in sublist]\n",
    "    return X[:,flat_list]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
